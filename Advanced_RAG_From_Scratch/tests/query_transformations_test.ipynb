{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from query_transformations import multi_query_generate_queries, multi_query_retrieve, multi_query_generate\n",
    "from query_transformations import rag_fusion_generate_queries, rag_fusion_retrieve, rag_fusion_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"什么是深度强化学习？\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. multi_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深度强化学习的基本概念是什么？', '深度强化学习的应用领域有哪些？', '深度强化学习与传统强化学习有什么区别？', '深度强化学习的主要算法和模型是什么？', '学习深度强化学习需要掌握哪些理论基础？']\n"
     ]
    }
   ],
   "source": [
    "generated_questions = multi_query_generate_queries(user_prompt)\n",
    "print(generated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19.9.笔记 351\\n深度增强学习的里程碑 ：增强学习的重大成就主要在视频游戏或现实世界游戏中实\\n现，这些游戏提供了具有有限动作和固定规则的约束性环境。深度 Q学习（由 Mnih等\\n人在2015年提出）在 ATARI游戏的基准测试中达到了人类水平的表现。 AlphaGo （由\\nSilver等人在2016年提出）战胜了围棋的世界冠军，这是一个之前被认为计算机难以掌\\n握的游戏。 Berner等人在2019年构建的系统在 Dota 2（五对五玩家游戏）中击败了世\\n界冠军队，显示出了玩家间合作的必要性。 Ye等人在2021年开发的系统能够在有限的\\n数据下超越人类玩家在 Atari游戏中的表现，这与之前需要大量经验的系统形成鲜明对\\n比。最近， FAIR在2022年推出的 Cicero系统在需要自然语言协商和玩家协调的《外\\n交》游戏中展示了人类级别的表现。\\nRL还成功应用于组合优化问题， Mazyavkina 等人在2021年的研究中有所涉及。\\n例如，Kool等人在2019年开发的模型在解决旅行商问题上与最佳启发式方法表现相当。\\n最近，Fawzi等人在2022年提出的 AlphaTensor 将矩阵乘法视作一种游戏，学会了用\\n更少的乘法操作更快地进行矩阵乘法，这一发现对深度学习领域，尤其是因其重度依赖\\n矩阵乘法的特性，意义重大，标志着 AI领域自我进化的重要里程碑。\\n经典增强学习方法 ： 关于马尔科夫决策过程 (MDPs)理论的早期贡献分别由 Thomp-\\nson在1933年和1935年作出。 Bellman 在1966年引入了 Bellman 递归，Howard在\\n1960年引入了策略迭代方法。 Sutton和Barto在2018年的研究中指出， Andreae 在\\n1969年的工作首次使用 MDP形式主义来描述增强学习。\\n现代增强学习的发展起源于 Sutton (1984) 和Watkins (1989) 的博士论文。 Sutton\\n在1988年引入时序差分学习， Watkins (1989) 和Watkins & Dayan (1992) 提出了Q学\\n习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &', '350 CHAPTER 19. 深度强化学习\\n图 19.17:决策变换器。决策变换器把离线强化学习处理为序列预测问题。输入是状态、动作和\\n剩余回报的序列，每个元素都被转换为固定大小的嵌入。每一步，网络预测下一动作。在测试阶\\n段，剩余回报未知；实际中，通常从一个初始估计出发，逐渐扣除观测到的奖励。\\n能够处理大量数据，并在广阔的时间范围内整合信息，使时间信用分配问题变得更易于\\n处理。这为强化学习开辟了一条新的、令人兴奋的道路。\\n19.8总结\\n增强学习 (Reinforcement Learning) 是针对马尔科夫决策过程 (Markov Decision\\nProcesses) 及其类似系统的序贯决策框架。本章介绍了增强学习的表格方法，包括动态\\n规划（环境模型已知） 、蒙特卡罗方法（通过运行多个回合并根据获得的奖励调整动作\\n值和策略）和时差分方法（在回合进行中更新这些值） 。\\n深度Q学习(Deep Q-Learning) 是一种时差分方法，使用深度神经网络预测每个状\\n态的动作价值，能够训练智能体在 Atari 2600 游戏中达到类似人类的水平。策略梯度方\\n法直接对策略进行优化，而非对动作进行价值赋值。这些方法生成的是随机策略，在部\\n分可观测的环境中尤其重要。这些更新过程含有噪声，为减少其方差已经引入了多种改\\n进措施。\\n当无法直接与环境互动而必须依赖历史数据学习时，就会使用离线增强学习。决策\\n变换器(Decision Transformer) 利用深度学习的最新进展构建状态 -动作-奖励序列模型，\\n并预测能够最大化奖励的动作。\\n19.9笔记\\nSutton和Barto在2018年的作品中详细介绍了表格型增强学习方法。 Li (2017) 、\\nArulkumaran 等人(2017)、FranCois-Lavet 等人(2018)和Wang等人(2022c)分别提供\\n了深度增强学习领域的综述。 Graesser 和Keng的2019年作品是一本优秀的入门资源，\\n其中包含了 Python代码示例。', '10 CHAPTER 1. 介绍\\n图 1.13:在强化学习中使用策略网络是一种创新。通过深度神经网络，我们可以定义从状态（例\\n如棋盘上的位置）到动作（可能的移动）的映射。这种映射即为所谓的“策略” 。\\n这种改变不一定是确定性的。执行行动还可能产生奖励，强化学习的目标是让代理学会\\n选择能够平均获得高奖励的行动。\\n一个复杂点是奖励可能在行动后一段时间才出现，因此把奖励与特定行动关联起来\\n并不直接。这被称为时间性信用分配问题。在学习过程中，代理必须在探索（寻找新的\\n可能性）和利用（使用已知的策略）之间做出平衡；也许代理已经学会了如何获得适度\\n的奖励，它应该继续遵循这个策略（利用现有知识） ，还是尝试不同的行动以寻找改进\\n的机会（探索新的可能性） ？\\n1.2.5两个例子\\n考虑教一个类人机器人如何行走。机器人在特定时间可以执行有限的行动（如移动\\n各种关节） ，这些行动会改变世界的状态（即它的姿态） 。我们可以通过设立障碍赛道上\\n的检查点来奖励机器人。为了到达每个检查点，它必须执行许多行动，但当收到奖励时，\\n很难确定哪些行动对奖励有贡献，哪些是无关紧要的。这就是时间性信用分配问题的一\\n个实例。\\n第二个例子是学习下棋。同样，代理在任何时刻都有一组有效的行动（棋子移动） 。\\n然而，这些行动以非确定性的方式改变系统状态；对于任何行动选择，对手可能以多种\\n不同的方式回应。这里，我们可以根据捕获棋子来设定奖励结构，或者在游戏结束时赢\\n得比赛来获得单一奖励。在后者情况下，时间性信用分配问题非常严重；系统必须学习\\n在众多走法中哪些是成功或失败的关键。\\n探索与利用的权衡在这两个例子中也很明显。机器人可能已经发现，通过侧躺并用\\n一条腿推动可以前进。这种策略虽然能让机器人移动并获得奖励，但比最优解——站立\\n行走——要慢得多。因此，它面临一个选择，是利用已知的策略（沿地面滑行）还是探\\n索其他可能的行动（可能实现更快的移动） 。在下棋例子中也是如此，代理可能学到了\\n一系列合理的开局走法。它应该利用这些知识，还是探索不同的开局序列？\\n深度学习如何融入强化学习框架可能不那么明显。有几种可能的方法，其中一种是\\n使用深度网络构建从观察到的世界状态到行动的映射。这被称为策略网络。在机器人的\\n例子中，策略网络会学习从传感器测量到关节运动的映射。在下棋的例子中，网络将学', '习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出', '356 CHAPTER 19. 深度强化学习', 'Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331']\n"
     ]
    }
   ],
   "source": [
    "info_retrieved = multi_query_retrieve(generated_questions)\n",
    "print(info_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "深度强化学习（Deep Reinforcement Learning, DRL）是一种结合了深度学习与强化学习的技术，旨在通过与环境的交互，使智能体学习到最佳的决策策略，以最大化其获得的奖励。\n",
      "\n",
      "强化学习（Reinforcement Learning, RL）是一个序贯决策框架，其中智能体通过在特定环境中执行动作来学习，目标是获得尽可能高的累积奖励。在强化学习中，智能体的行为策略是通过对环境状态的观察和影响而逐步改进的。智能体在环境中采取行动并根据获得的奖励进行学习，这个过程涉及一些核心概念，如马尔科夫决策过程（Markov Decision Process, MDP）、策略、价值函数和奖励信号。\n",
      "\n",
      "深度强化学习则引入了深度神经网络，以应对高维度复杂输入数据（如图像、传感器数据等）。通过深度学习，智能体能够从原始输入中自动提取特征，从而更有效地进行策略学习。例如，在Deep Q-Learning中，智能体利用深度神经网络来估算每个状态下的动作价值（Q值），并通过这种方式做出智能决策。2015年，Mnih等人提出的深度Q学习成功在ATARI游戏中达到了人类水平的表现，标志着深度强化学习在实际应用中的重大突破。\n",
      "\n",
      "深度强化学习的应用并不仅限于游戏。它在机器人控制、自动驾驶、金融交易、资源管理等多个领域展现了巨大的潜力。例如，AlphaGo通过深度强化学习在围棋比赛中击败了世界冠军，Dota 2游戏中建设的系统也通过强化学习展示了团队协作的重要性。\n",
      "\n",
      "近年来，深度强化学习还发展出了许多重要的技术进展，例如决策变换器（Decision Transformer），通过序列预测的方式处理离线强化学习问题，极大地提高了时间信用分配问题的处理能力。\n",
      "\n",
      "总而言之，深度强化学习是一个多学科交叉的前沿领域，通过深度学习技术提升了传统强化学习的方法，使得智能体在复杂环境中自我学习与优化的能力得以显著增强。\n"
     ]
    }
   ],
   "source": [
    "model_answer = multi_query_generate(user_prompt, info_retrieved)\n",
    "print(model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深度强化学习的基本概念是什么？', '深度强化学习与传统强化学习的区别是什么？', '深度强化学习的应用实例有哪些？']\n"
     ]
    }
   ],
   "source": [
    "generated_questions = rag_fusion_generate_queries(user_prompt, num_to_generate=\"three\")\n",
    "print(generated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331', '习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出', '356 CHAPTER 19. 深度强化学习'], ['10 CHAPTER 1. 介绍\\n图 1.13:在强化学习中使用策略网络是一种创新。通过深度神经网络，我们可以定义从状态（例\\n如棋盘上的位置）到动作（可能的移动）的映射。这种映射即为所谓的“策略” 。\\n这种改变不一定是确定性的。执行行动还可能产生奖励，强化学习的目标是让代理学会\\n选择能够平均获得高奖励的行动。\\n一个复杂点是奖励可能在行动后一段时间才出现，因此把奖励与特定行动关联起来\\n并不直接。这被称为时间性信用分配问题。在学习过程中，代理必须在探索（寻找新的\\n可能性）和利用（使用已知的策略）之间做出平衡；也许代理已经学会了如何获得适度\\n的奖励，它应该继续遵循这个策略（利用现有知识） ，还是尝试不同的行动以寻找改进\\n的机会（探索新的可能性） ？\\n1.2.5两个例子\\n考虑教一个类人机器人如何行走。机器人在特定时间可以执行有限的行动（如移动\\n各种关节） ，这些行动会改变世界的状态（即它的姿态） 。我们可以通过设立障碍赛道上\\n的检查点来奖励机器人。为了到达每个检查点，它必须执行许多行动，但当收到奖励时，\\n很难确定哪些行动对奖励有贡献，哪些是无关紧要的。这就是时间性信用分配问题的一\\n个实例。\\n第二个例子是学习下棋。同样，代理在任何时刻都有一组有效的行动（棋子移动） 。\\n然而，这些行动以非确定性的方式改变系统状态；对于任何行动选择，对手可能以多种\\n不同的方式回应。这里，我们可以根据捕获棋子来设定奖励结构，或者在游戏结束时赢\\n得比赛来获得单一奖励。在后者情况下，时间性信用分配问题非常严重；系统必须学习\\n在众多走法中哪些是成功或失败的关键。\\n探索与利用的权衡在这两个例子中也很明显。机器人可能已经发现，通过侧躺并用\\n一条腿推动可以前进。这种策略虽然能让机器人移动并获得奖励，但比最优解——站立\\n行走——要慢得多。因此，它面临一个选择，是利用已知的策略（沿地面滑行）还是探\\n索其他可能的行动（可能实现更快的移动） 。在下棋例子中也是如此，代理可能学到了\\n一系列合理的开局走法。它应该利用这些知识，还是探索不同的开局序列？\\n深度学习如何融入强化学习框架可能不那么明显。有几种可能的方法，其中一种是\\n使用深度网络构建从观察到的世界状态到行动的映射。这被称为策略网络。在机器人的\\n例子中，策略网络会学习从传感器测量到关节运动的映射。在下棋的例子中，网络将学', '19.9.笔记 351\\n深度增强学习的里程碑 ：增强学习的重大成就主要在视频游戏或现实世界游戏中实\\n现，这些游戏提供了具有有限动作和固定规则的约束性环境。深度 Q学习（由 Mnih等\\n人在2015年提出）在 ATARI游戏的基准测试中达到了人类水平的表现。 AlphaGo （由\\nSilver等人在2016年提出）战胜了围棋的世界冠军，这是一个之前被认为计算机难以掌\\n握的游戏。 Berner等人在2019年构建的系统在 Dota 2（五对五玩家游戏）中击败了世\\n界冠军队，显示出了玩家间合作的必要性。 Ye等人在2021年开发的系统能够在有限的\\n数据下超越人类玩家在 Atari游戏中的表现，这与之前需要大量经验的系统形成鲜明对\\n比。最近， FAIR在2022年推出的 Cicero系统在需要自然语言协商和玩家协调的《外\\n交》游戏中展示了人类级别的表现。\\nRL还成功应用于组合优化问题， Mazyavkina 等人在2021年的研究中有所涉及。\\n例如，Kool等人在2019年开发的模型在解决旅行商问题上与最佳启发式方法表现相当。\\n最近，Fawzi等人在2022年提出的 AlphaTensor 将矩阵乘法视作一种游戏，学会了用\\n更少的乘法操作更快地进行矩阵乘法，这一发现对深度学习领域，尤其是因其重度依赖\\n矩阵乘法的特性，意义重大，标志着 AI领域自我进化的重要里程碑。\\n经典增强学习方法 ： 关于马尔科夫决策过程 (MDPs)理论的早期贡献分别由 Thomp-\\nson在1933年和1935年作出。 Bellman 在1966年引入了 Bellman 递归，Howard在\\n1960年引入了策略迭代方法。 Sutton和Barto在2018年的研究中指出， Andreae 在\\n1969年的工作首次使用 MDP形式主义来描述增强学习。\\n现代增强学习的发展起源于 Sutton (1984) 和Watkins (1989) 的博士论文。 Sutton\\n在1988年引入时序差分学习， Watkins (1989) 和Watkins & Dayan (1992) 提出了Q学\\n习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &', 'Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331'], ['10 CHAPTER 1. 介绍\\n图 1.13:在强化学习中使用策略网络是一种创新。通过深度神经网络，我们可以定义从状态（例\\n如棋盘上的位置）到动作（可能的移动）的映射。这种映射即为所谓的“策略” 。\\n这种改变不一定是确定性的。执行行动还可能产生奖励，强化学习的目标是让代理学会\\n选择能够平均获得高奖励的行动。\\n一个复杂点是奖励可能在行动后一段时间才出现，因此把奖励与特定行动关联起来\\n并不直接。这被称为时间性信用分配问题。在学习过程中，代理必须在探索（寻找新的\\n可能性）和利用（使用已知的策略）之间做出平衡；也许代理已经学会了如何获得适度\\n的奖励，它应该继续遵循这个策略（利用现有知识） ，还是尝试不同的行动以寻找改进\\n的机会（探索新的可能性） ？\\n1.2.5两个例子\\n考虑教一个类人机器人如何行走。机器人在特定时间可以执行有限的行动（如移动\\n各种关节） ，这些行动会改变世界的状态（即它的姿态） 。我们可以通过设立障碍赛道上\\n的检查点来奖励机器人。为了到达每个检查点，它必须执行许多行动，但当收到奖励时，\\n很难确定哪些行动对奖励有贡献，哪些是无关紧要的。这就是时间性信用分配问题的一\\n个实例。\\n第二个例子是学习下棋。同样，代理在任何时刻都有一组有效的行动（棋子移动） 。\\n然而，这些行动以非确定性的方式改变系统状态；对于任何行动选择，对手可能以多种\\n不同的方式回应。这里，我们可以根据捕获棋子来设定奖励结构，或者在游戏结束时赢\\n得比赛来获得单一奖励。在后者情况下，时间性信用分配问题非常严重；系统必须学习\\n在众多走法中哪些是成功或失败的关键。\\n探索与利用的权衡在这两个例子中也很明显。机器人可能已经发现，通过侧躺并用\\n一条腿推动可以前进。这种策略虽然能让机器人移动并获得奖励，但比最优解——站立\\n行走——要慢得多。因此，它面临一个选择，是利用已知的策略（沿地面滑行）还是探\\n索其他可能的行动（可能实现更快的移动） 。在下棋例子中也是如此，代理可能学到了\\n一系列合理的开局走法。它应该利用这些知识，还是探索不同的开局序列？\\n深度学习如何融入强化学习框架可能不那么明显。有几种可能的方法，其中一种是\\n使用深度网络构建从观察到的世界状态到行动的映射。这被称为策略网络。在机器人的\\n例子中，策略网络会学习从传感器测量到关节运动的映射。在下棋的例子中，网络将学', '19.9.笔记 351\\n深度增强学习的里程碑 ：增强学习的重大成就主要在视频游戏或现实世界游戏中实\\n现，这些游戏提供了具有有限动作和固定规则的约束性环境。深度 Q学习（由 Mnih等\\n人在2015年提出）在 ATARI游戏的基准测试中达到了人类水平的表现。 AlphaGo （由\\nSilver等人在2016年提出）战胜了围棋的世界冠军，这是一个之前被认为计算机难以掌\\n握的游戏。 Berner等人在2019年构建的系统在 Dota 2（五对五玩家游戏）中击败了世\\n界冠军队，显示出了玩家间合作的必要性。 Ye等人在2021年开发的系统能够在有限的\\n数据下超越人类玩家在 Atari游戏中的表现，这与之前需要大量经验的系统形成鲜明对\\n比。最近， FAIR在2022年推出的 Cicero系统在需要自然语言协商和玩家协调的《外\\n交》游戏中展示了人类级别的表现。\\nRL还成功应用于组合优化问题， Mazyavkina 等人在2021年的研究中有所涉及。\\n例如，Kool等人在2019年开发的模型在解决旅行商问题上与最佳启发式方法表现相当。\\n最近，Fawzi等人在2022年提出的 AlphaTensor 将矩阵乘法视作一种游戏，学会了用\\n更少的乘法操作更快地进行矩阵乘法，这一发现对深度学习领域，尤其是因其重度依赖\\n矩阵乘法的特性，意义重大，标志着 AI领域自我进化的重要里程碑。\\n经典增强学习方法 ： 关于马尔科夫决策过程 (MDPs)理论的早期贡献分别由 Thomp-\\nson在1933年和1935年作出。 Bellman 在1966年引入了 Bellman 递归，Howard在\\n1960年引入了策略迭代方法。 Sutton和Barto在2018年的研究中指出， Andreae 在\\n1969年的工作首次使用 MDP形式主义来描述增强学习。\\n现代增强学习的发展起源于 Sutton (1984) 和Watkins (1989) 的博士论文。 Sutton\\n在1988年引入时序差分学习， Watkins (1989) 和Watkins & Dayan (1992) 提出了Q学\\n习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &', '习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出']]\n"
     ]
    }
   ],
   "source": [
    "info_retrieved = rag_fusion_retrieve(generated_questions)\n",
    "print(info_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VSCode\\VSCode仓库\\RAG\\Advanced_RAG_From_Scratch\\tests\\..\\query_transformations.py:148: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "深度强化学习（Deep Reinforcement Learning，DRL）是结合了深度学习与强化学习的一个领域。在强化学习中，智能体（agent）通过与环境互动来学习如何选择行动，以最大化所获得的奖励。这一过程涉及到一系列的决策，智能体不断评估其选择的行动所带来的结果，并相应调整其策略。\n",
      "\n",
      "在深度强化学习中，深度神经网络被用于近似复杂的策略或价值函数，从而能够处理高维的输入数据，如图像、音频或其他感测数据。这使得深度强化学习在处理那些具有复杂结构和状态空间的任务时，非常有效，例如视频游戏、机器人控制和金融交易等。\n",
      "\n",
      "### 关键概念\n",
      "\n",
      "1. **策略（Policy）**：智能体的策略是从状态到动作的映射。策略可以是确定性的或随机性的。通过深度神经网络，可以学习到一个复杂的策略，将观察到的状态映射到最佳的行动。\n",
      "\n",
      "2. **奖励（Reward）**：智能体在执行行动后会获得奖励，强化学习的目标是让智能体学会选择那些能带来高平均奖励的行动。奖励有时是稀疏或延迟的，即智能体可能在执行多步行动后才能获得反馈，这造成了奖励与行动之间的时间性信用分配问题。\n",
      "\n",
      "3. **探索与利用（Exploration vs. Exploitation）**：智能体在学习过程中需要在“探索新行动”的可能性与“利用已知成功策略”之间进行权衡。例如，在教机器人行走的实例中，机器人可能会发现某种有效但不优化的行走方式（探索），而不是直接学习到最优的行走策略（利用）。\n",
      "\n",
      "### 深度强化学习的应用与成就\n",
      "\n",
      "深度强化学习近年来在多个领域取得了显著成绩。例如：\n",
      "- **AlphaGo**：DeepMind开发的AlphaGo在围棋比赛中战胜了世界冠军，这展示了DRL在处理复杂决策问题中的能力。\n",
      "- **Dota 2**：OpenAI的系统在游戏《Dota 2》中击败了世界冠军队，展示了DRL在复杂团队合作环境下的应用。\n",
      "- **ATARI游戏**：使用深度Q学习（DQN），智能体在ATARI游戏中达到了人类水平的表现。\n",
      "\n",
      "深度强化学习不仅在游戏和机器人技术中表现出色，也已开始应用于金融交易、资源优化等多个实际场景，并展示出其广泛的潜力。因此，深度强化学习正在成为人工智能领域的重要研究方向和应用基础。\n"
     ]
    }
   ],
   "source": [
    "model_answer = rag_fusion_generate(user_prompt, info_retrieved)\n",
    "print(model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
