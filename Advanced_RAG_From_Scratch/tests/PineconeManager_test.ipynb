{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\VSCode\\VSCode仓库\\RAG\\Advanced_RAG_From_Scratch\\tests\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 会跳出tests文件夹到Advanced_RAG_From_Scratch文件夹里面找PineconeManager\n",
    "from PineconeManager import PineconeManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_manager = PineconeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Successfully uploaded 550 chunks from PDF file.\n"
     ]
    }
   ],
   "source": [
    "# 要上传再上传\n",
    "pinecone_manager.upload_pdf_file(\"../files/UnderstandingDeepLearning-ZH-CN-240721.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"什么是深度强化学习？\"\n",
    "results_with_metadata, results_only_str = pinecone_manager.retrieval(query, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='bf086bc4-1995-4d3c-9655-56ea53527972', metadata={'page': 346.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331'),\n",
       "  0.584413826),\n",
       " (Document(id='c0abb283-3d34-449b-8eb5-3e40a09c26af', metadata={'page': 365.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='350 CHAPTER 19. 深度强化学习\\n图 19.17:决策变换器。决策变换器把离线强化学习处理为序列预测问题。输入是状态、动作和\\n剩余回报的序列，每个元素都被转换为固定大小的嵌入。每一步，网络预测下一动作。在测试阶\\n段，剩余回报未知；实际中，通常从一个初始估计出发，逐渐扣除观测到的奖励。\\n能够处理大量数据，并在广阔的时间范围内整合信息，使时间信用分配问题变得更易于\\n处理。这为强化学习开辟了一条新的、令人兴奋的道路。\\n19.8总结\\n增强学习 (Reinforcement Learning) 是针对马尔科夫决策过程 (Markov Decision\\nProcesses) 及其类似系统的序贯决策框架。本章介绍了增强学习的表格方法，包括动态\\n规划（环境模型已知） 、蒙特卡罗方法（通过运行多个回合并根据获得的奖励调整动作\\n值和策略）和时差分方法（在回合进行中更新这些值） 。\\n深度Q学习(Deep Q-Learning) 是一种时差分方法，使用深度神经网络预测每个状\\n态的动作价值，能够训练智能体在 Atari 2600 游戏中达到类似人类的水平。策略梯度方\\n法直接对策略进行优化，而非对动作进行价值赋值。这些方法生成的是随机策略，在部\\n分可观测的环境中尤其重要。这些更新过程含有噪声，为减少其方差已经引入了多种改\\n进措施。\\n当无法直接与环境互动而必须依赖历史数据学习时，就会使用离线增强学习。决策\\n变换器(Decision Transformer) 利用深度学习的最新进展构建状态 -动作-奖励序列模型，\\n并预测能够最大化奖励的动作。\\n19.9笔记\\nSutton和Barto在2018年的作品中详细介绍了表格型增强学习方法。 Li (2017) 、\\nArulkumaran 等人(2017)、FranCois-Lavet 等人(2018)和Wang等人(2022c)分别提供\\n了深度增强学习领域的综述。 Graesser 和Keng的2019年作品是一本优秀的入门资源，\\n其中包含了 Python代码示例。'),\n",
       "  0.547480643),\n",
       " (Document(id='fceac8a3-f89c-47cb-8a4d-a4667f71af7c', metadata={'page': 366.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出'),\n",
       "  0.544806719),\n",
       " (Document(id='8f9ac024-0432-449b-b807-818957b5625d', metadata={'page': 371.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='356 CHAPTER 19. 深度强化学习'),\n",
       "  0.541619599),\n",
       " (Document(id='c29da15c-ccfe-4cfe-b9b4-db7127b7017b', metadata={'page': 366.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='19.9.笔记 351\\n深度增强学习的里程碑 ：增强学习的重大成就主要在视频游戏或现实世界游戏中实\\n现，这些游戏提供了具有有限动作和固定规则的约束性环境。深度 Q学习（由 Mnih等\\n人在2015年提出）在 ATARI游戏的基准测试中达到了人类水平的表现。 AlphaGo （由\\nSilver等人在2016年提出）战胜了围棋的世界冠军，这是一个之前被认为计算机难以掌\\n握的游戏。 Berner等人在2019年构建的系统在 Dota 2（五对五玩家游戏）中击败了世\\n界冠军队，显示出了玩家间合作的必要性。 Ye等人在2021年开发的系统能够在有限的\\n数据下超越人类玩家在 Atari游戏中的表现，这与之前需要大量经验的系统形成鲜明对\\n比。最近， FAIR在2022年推出的 Cicero系统在需要自然语言协商和玩家协调的《外\\n交》游戏中展示了人类级别的表现。\\nRL还成功应用于组合优化问题， Mazyavkina 等人在2021年的研究中有所涉及。\\n例如，Kool等人在2019年开发的模型在解决旅行商问题上与最佳启发式方法表现相当。\\n最近，Fawzi等人在2022年提出的 AlphaTensor 将矩阵乘法视作一种游戏，学会了用\\n更少的乘法操作更快地进行矩阵乘法，这一发现对深度学习领域，尤其是因其重度依赖\\n矩阵乘法的特性，意义重大，标志着 AI领域自我进化的重要里程碑。\\n经典增强学习方法 ： 关于马尔科夫决策过程 (MDPs)理论的早期贡献分别由 Thomp-\\nson在1933年和1935年作出。 Bellman 在1966年引入了 Bellman 递归，Howard在\\n1960年引入了策略迭代方法。 Sutton和Barto在2018年的研究中指出， Andreae 在\\n1969年的工作首次使用 MDP形式主义来描述增强学习。\\n现代增强学习的发展起源于 Sutton (1984) 和Watkins (1989) 的博士论文。 Sutton\\n在1988年引入时序差分学习， Watkins (1989) 和Watkins & Dayan (1992) 提出了Q学\\n习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &'),\n",
       "  0.528779328),\n",
       " (Document(id='a9486f4a-ca51-4749-85e8-235352dc5c76', metadata={'page': 367.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='352 CHAPTER 19. 深度强化学习\\n由于最大化操作导致的状态价值系统性过高估计，并提出了双 Q学习，通过同时训练两\\n个模型来解决这个问题。这种方法后来被应用于深度 Q学习（Van Hasselt 等人，2016） ，\\n虽然其有效性后来受到了质疑（ Hessel等人，2018） 。Wang等人(2016)引入了深度对\\n决网络，其中网络的两个部分分别预测状态的价值和每个动作的相对优势，这种解耦可\\n以提高稳定性，因为有时状态的价值更为重要，而具体采取哪种动作的影响不大。\\nFortunato 等人（2018）提出了噪声深度 Q-网络，其特点是 Q-网络中的部分权重乘\\n以噪声以增加预测的随机性并促进探索行为。该网络能够在逐步收敛到合理策略的过程\\n中学会减少噪声的强度。分布式 DQN（Bellemare 等人，2017a；Dabney等人，2018，继\\nMorimura 等人，2010）的目标是估计回报分布的更全面信息，而不仅仅是期望值。这使\\n得网络有可能减轻最坏情况下的影响，并且还可以通过预测更高阶矩来提高性能，因为\\n这为训练提供了更丰富的信号。 Rainbow（Hessel等人，2018）结合了六项对原始深度\\nQ-学习算法的改进，包括决策网络、分布式 DQN和噪声DQN，从而提高了在 ATARI\\n基准测试上的训练速度和最终性能。\\n策略梯度 ：Williams （1992）首次提出了 REINFORCE 算法。”策略梯度方法 ”一\\n词最早见于 Sutton等人（1999） 。Konda和Tsitsiklis （1999）引入了演员 -评论家算法。\\n使用不同的基准来降低方差在 Greensmith 等人（2004）和Peters & Schaal （2008）的\\n工作中被探讨。 Mei等人（2022）后来指出，价值基准主要是减少更新的激进性而非其\\n方差。\\n策略梯度已经被改进以产生确定性策略（ Silver等人，2014；Lillicrap 等人，2016；\\nFujimoto 等人，2018） 。最直接的方法是对所有可能的行动进行最大化，但如果行动空\\n间是连续的，则每一步都需要一个优化过程。深度确定性策略梯度算法（ Lillicrap 等人，\\n2016）按照行为价值梯度的方向调整策略，这表明使用了演员 -评论家方法。'),\n",
       "  0.515974581),\n",
       " (Document(id='da165ef0-a999-4343-ac63-832a7b2db0fb', metadata={'page': 25.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='10 CHAPTER 1. 介绍\\n图 1.13:在强化学习中使用策略网络是一种创新。通过深度神经网络，我们可以定义从状态（例\\n如棋盘上的位置）到动作（可能的移动）的映射。这种映射即为所谓的“策略” 。\\n这种改变不一定是确定性的。执行行动还可能产生奖励，强化学习的目标是让代理学会\\n选择能够平均获得高奖励的行动。\\n一个复杂点是奖励可能在行动后一段时间才出现，因此把奖励与特定行动关联起来\\n并不直接。这被称为时间性信用分配问题。在学习过程中，代理必须在探索（寻找新的\\n可能性）和利用（使用已知的策略）之间做出平衡；也许代理已经学会了如何获得适度\\n的奖励，它应该继续遵循这个策略（利用现有知识） ，还是尝试不同的行动以寻找改进\\n的机会（探索新的可能性） ？\\n1.2.5两个例子\\n考虑教一个类人机器人如何行走。机器人在特定时间可以执行有限的行动（如移动\\n各种关节） ，这些行动会改变世界的状态（即它的姿态） 。我们可以通过设立障碍赛道上\\n的检查点来奖励机器人。为了到达每个检查点，它必须执行许多行动，但当收到奖励时，\\n很难确定哪些行动对奖励有贡献，哪些是无关紧要的。这就是时间性信用分配问题的一\\n个实例。\\n第二个例子是学习下棋。同样，代理在任何时刻都有一组有效的行动（棋子移动） 。\\n然而，这些行动以非确定性的方式改变系统状态；对于任何行动选择，对手可能以多种\\n不同的方式回应。这里，我们可以根据捕获棋子来设定奖励结构，或者在游戏结束时赢\\n得比赛来获得单一奖励。在后者情况下，时间性信用分配问题非常严重；系统必须学习\\n在众多走法中哪些是成功或失败的关键。\\n探索与利用的权衡在这两个例子中也很明显。机器人可能已经发现，通过侧躺并用\\n一条腿推动可以前进。这种策略虽然能让机器人移动并获得奖励，但比最优解——站立\\n行走——要慢得多。因此，它面临一个选择，是利用已知的策略（沿地面滑行）还是探\\n索其他可能的行动（可能实现更快的移动） 。在下棋例子中也是如此，代理可能学到了\\n一系列合理的开局走法。它应该利用这些知识，还是探索不同的开局序列？\\n深度学习如何融入强化学习框架可能不那么明显。有几种可能的方法，其中一种是\\n使用深度网络构建从观察到的世界状态到行动的映射。这被称为策略网络。在机器人的\\n例子中，策略网络会学习从传感器测量到关节运动的映射。在下棋的例子中，网络将学'),\n",
       "  0.50567317),\n",
       " (Document(id='cb5c3f45-06ca-49af-99b1-3adb540ac812', metadata={'page': 11.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='18.5.2网络的重参数化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\\n18.6实现. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\\n18.6.1应用于图像 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\\n18.6.2提高生成速度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n18.6.3条件生成 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n18.6.4提高生成质量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\\n18.7总结. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\\n18.8笔记. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\\n18.9习题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n19深度强化学习 331\\n19.1马尔可夫决策过程、回报与策略 . . . . . . . . . . . . . . . . . . . . . . . 331\\n19.1.1马尔科夫过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\\n19.1.2马尔可夫奖励过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . 332'),\n",
       "  0.503851116),\n",
       " (Document(id='b4fd6165-ce64-4213-8e29-ebff2c1ae279', metadata={'page': 24.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='1.2.无监督学习 9\\n图 1.11:图像插值。图像插值是一种有趣的应用。在每一行的图像中，左右两侧为真实图像，中\\n间三张则是生成模型创造的插值序列。这些生成模型学习到了所有图像均可通过一组潜在变量\\n来生成的原理。通过确定这两张真实图像的潜变量，对它们的值进行插值，然后用这些中间变量\\n生成新图像，我们能创造出既视觉上合理，又融合了两张原始图像特征的中间图像。上排图片改\\n编自 Sauer等人 (2022)，下排图片改编自 Ramesh等人 (2022)。\\n图 1.12:从“时代广场上的滑板泰迪熊”这个标题出发， DALL·E-2（Ramesh等人， 2022）生\\n成了多张图片。\\n变量。我们可以通过在它们的潜在表示之间插值，并将中间位置映射回数据空间，从而\\n在这些实例之间进行插值（见图 1.11） 。\\n1.2.3结合监督学习与无监督学习\\n具有潜变量的生成式模型也可以促进输出具有结构的监督学习模型的发展（见图\\n1.4） 。例如，考虑学习如何预测与描述相对应的图像。我们可以学习文本的潜变量与图\\n像的潜变量之间的关系，而不是直接将文本输入映射到图像上。\\n这种方法有三个优点。首先，由于输入和输出维度较低，我们可能需要更少的文\\n本/图像对来学习这种映射。其次，我们更有可能生成看起来合理的图像；潜变量的任\\n何合理值都应该产生像是一个可信的示例。第三，如果我们在两组潜变量之间的映射或\\n潜变量到图像的映射中引入随机性，那么我们可以生成多个都与描述相匹配的图像（见\\n图1.12） 。\\n1.2.4强化学习\\n机器学习的最后一个领域是强化学习。这个范畴引入了代理（ agent）的概念，代理\\n生活在一个世界中，在每个时间步骤中可以执行特定行动。行动会改变系统的状态，但'),\n",
       "  0.49486658),\n",
       " (Document(id='da382dd0-2167-4efd-b3fe-b455e9c1fb1d', metadata={'page': 357.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='342 CHAPTER 19. 深度强化学习\\n拟合Q-学习与Q-学习的区别在于它不再保证收敛性。参数的改变可能会同时影响\\n目标r(st,at) +γmax aq(st+1,at+1;ϕ)（最大值可能发生变化）和预测 q(st,at;ϕ)，这在\\n理论和实践上都被证明会对收敛性产生不利影响。\\n19.4.1 深度 Q-网络在 A T ARI 游戏中的应用\\n深度网络非常适合于从高维状态空间进行预测，因此在拟合 Q-学习中成为模型的\\n自然选择。理论上，它们能够同时处理状态和动作作为输入以预测值，但实践中，网络\\n只接收状态作为输入，并同时预测各个动作的值。\\n图 19.13:阿塔里基准。阿塔里基准包含了 49款阿塔里 2600游戏，如突破、乒乓及多种射击、\\n平台等类型的游戏。 a-d)单屏幕游戏的状态无法仅通过一个画面完全确定，因为物体的速度信\\n息不可见。因此，通常用几个连续帧（此处为四帧）来表达状态。 e)通过操纵杆模拟用户的动\\n作输入。 f)共有十八种动作，对应八个方向的移动或停止，并且对这九种状态，按钮可按下或不\\n按。\\nDeep Q-Network 是一个突破性的强化学习架构，它利用深度网络学会了玩 ATARI\\n2600游戏。观测数据包括 220×160 像素的图像，每个像素拥有 128种可能的颜色（见\\n图19.13） 。这些图像被重塑为 84×84像素，并且只保留了亮度值。不幸的是，单一帧\\n图像无法展现完整的状态。例如，游戏中物体的速度未知。为解决这个问题，网络在每\\n个时间步骤摄取最后四帧图像，组成 st。网络通过三个卷积层及一个全连接层处理这些\\n帧，以预测每个动作的价值（见图 19.14） 。\\n对标准训练过程进行了数项修改。首先，根据游戏得分驱动的奖励被限制在 −1到\\n+1之间，这种做法旨在平衡不同游戏间得分的巨大差异，并允许使用统一的学习率。其\\n次，系统采用了经验回放机制。不是仅基于当前步骤的 <s t,at,rt+1,st+1>元组或最近\\nI个元组的批处理来更新网络，而是将所有近期的元组都保存在一个缓冲区内。这个缓\\n冲区被随机采样以在每一步生成一个批量，这种方法使得数据样本被多次重用，并降低\\n了批次中样本间因相邻帧的相似性而产生的相关性。\\n最终，通过固定目标参数到 ϕ−值并定期更新，解决了拟合 Q-网络中的收敛问题，\\n给出了以下更新公式：'),\n",
       "  0.491595984)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331',\n",
       " '350 CHAPTER 19. 深度强化学习\\n图 19.17:决策变换器。决策变换器把离线强化学习处理为序列预测问题。输入是状态、动作和\\n剩余回报的序列，每个元素都被转换为固定大小的嵌入。每一步，网络预测下一动作。在测试阶\\n段，剩余回报未知；实际中，通常从一个初始估计出发，逐渐扣除观测到的奖励。\\n能够处理大量数据，并在广阔的时间范围内整合信息，使时间信用分配问题变得更易于\\n处理。这为强化学习开辟了一条新的、令人兴奋的道路。\\n19.8总结\\n增强学习 (Reinforcement Learning) 是针对马尔科夫决策过程 (Markov Decision\\nProcesses) 及其类似系统的序贯决策框架。本章介绍了增强学习的表格方法，包括动态\\n规划（环境模型已知） 、蒙特卡罗方法（通过运行多个回合并根据获得的奖励调整动作\\n值和策略）和时差分方法（在回合进行中更新这些值） 。\\n深度Q学习(Deep Q-Learning) 是一种时差分方法，使用深度神经网络预测每个状\\n态的动作价值，能够训练智能体在 Atari 2600 游戏中达到类似人类的水平。策略梯度方\\n法直接对策略进行优化，而非对动作进行价值赋值。这些方法生成的是随机策略，在部\\n分可观测的环境中尤其重要。这些更新过程含有噪声，为减少其方差已经引入了多种改\\n进措施。\\n当无法直接与环境互动而必须依赖历史数据学习时，就会使用离线增强学习。决策\\n变换器(Decision Transformer) 利用深度学习的最新进展构建状态 -动作-奖励序列模型，\\n并预测能够最大化奖励的动作。\\n19.9笔记\\nSutton和Barto在2018年的作品中详细介绍了表格型增强学习方法。 Li (2017) 、\\nArulkumaran 等人(2017)、FranCois-Lavet 等人(2018)和Wang等人(2022c)分别提供\\n了深度增强学习领域的综述。 Graesser 和Keng的2019年作品是一本优秀的入门资源，\\n其中包含了 Python代码示例。',\n",
       " '习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_only_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
