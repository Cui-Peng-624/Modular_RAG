{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\VSCode\\VSCode仓库\\RAG\\Advanced_RAG_From_Scratch\\tests\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 会跳出tests文件夹到Advanced_RAG_From_Scratch文件夹里面找PineconeManager\n",
    "from PineconeManager import PineconeManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_manager = PineconeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Successfully uploaded 550 chunks from PDF file.\n"
     ]
    }
   ],
   "source": [
    "# 要上传再上传\n",
    "pinecone_manager.upload_pdf_file(\"../files/UnderstandingDeepLearning-ZH-CN-240721.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"什么是深度强化学习？\"\n",
    "results_with_metadata, results_only_str = pinecone_manager.retrieval(query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='bf086bc4-1995-4d3c-9655-56ea53527972', metadata={'page': 346.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331'),\n",
       "  0.584622264),\n",
       " (Document(id='c0abb283-3d34-449b-8eb5-3e40a09c26af', metadata={'page': 365.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='350 CHAPTER 19. 深度强化学习\\n图 19.17:决策变换器。决策变换器把离线强化学习处理为序列预测问题。输入是状态、动作和\\n剩余回报的序列，每个元素都被转换为固定大小的嵌入。每一步，网络预测下一动作。在测试阶\\n段，剩余回报未知；实际中，通常从一个初始估计出发，逐渐扣除观测到的奖励。\\n能够处理大量数据，并在广阔的时间范围内整合信息，使时间信用分配问题变得更易于\\n处理。这为强化学习开辟了一条新的、令人兴奋的道路。\\n19.8总结\\n增强学习 (Reinforcement Learning) 是针对马尔科夫决策过程 (Markov Decision\\nProcesses) 及其类似系统的序贯决策框架。本章介绍了增强学习的表格方法，包括动态\\n规划（环境模型已知） 、蒙特卡罗方法（通过运行多个回合并根据获得的奖励调整动作\\n值和策略）和时差分方法（在回合进行中更新这些值） 。\\n深度Q学习(Deep Q-Learning) 是一种时差分方法，使用深度神经网络预测每个状\\n态的动作价值，能够训练智能体在 Atari 2600 游戏中达到类似人类的水平。策略梯度方\\n法直接对策略进行优化，而非对动作进行价值赋值。这些方法生成的是随机策略，在部\\n分可观测的环境中尤其重要。这些更新过程含有噪声，为减少其方差已经引入了多种改\\n进措施。\\n当无法直接与环境互动而必须依赖历史数据学习时，就会使用离线增强学习。决策\\n变换器(Decision Transformer) 利用深度学习的最新进展构建状态 -动作-奖励序列模型，\\n并预测能够最大化奖励的动作。\\n19.9笔记\\nSutton和Barto在2018年的作品中详细介绍了表格型增强学习方法。 Li (2017) 、\\nArulkumaran 等人(2017)、FranCois-Lavet 等人(2018)和Wang等人(2022c)分别提供\\n了深度增强学习领域的综述。 Graesser 和Keng的2019年作品是一本优秀的入门资源，\\n其中包含了 Python代码示例。'),\n",
       "  0.547534645),\n",
       " (Document(id='fceac8a3-f89c-47cb-8a4d-a4667f71af7c', metadata={'page': 366.0, 'source': '../files/UnderstandingDeepLearning-ZH-CN-240721.pdf'}, page_content='习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出'),\n",
       "  0.544770598)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter 19\\n深度强化学习\\n强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来\\n学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的\\n移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体）\\n在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许\\n会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最\\n大化利润（奖励） 。\\n以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或\\n0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的，\\n即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间\\n上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键\\n动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次\\n移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后，\\n智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前\\n成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。\\n强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的\\n系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时\\n间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。\\n19.1马尔可夫决策过程、回报与策略\\n强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常，\\n我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术\\n语。\\n331',\n",
       " '350 CHAPTER 19. 深度强化学习\\n图 19.17:决策变换器。决策变换器把离线强化学习处理为序列预测问题。输入是状态、动作和\\n剩余回报的序列，每个元素都被转换为固定大小的嵌入。每一步，网络预测下一动作。在测试阶\\n段，剩余回报未知；实际中，通常从一个初始估计出发，逐渐扣除观测到的奖励。\\n能够处理大量数据，并在广阔的时间范围内整合信息，使时间信用分配问题变得更易于\\n处理。这为强化学习开辟了一条新的、令人兴奋的道路。\\n19.8总结\\n增强学习 (Reinforcement Learning) 是针对马尔科夫决策过程 (Markov Decision\\nProcesses) 及其类似系统的序贯决策框架。本章介绍了增强学习的表格方法，包括动态\\n规划（环境模型已知） 、蒙特卡罗方法（通过运行多个回合并根据获得的奖励调整动作\\n值和策略）和时差分方法（在回合进行中更新这些值） 。\\n深度Q学习(Deep Q-Learning) 是一种时差分方法，使用深度神经网络预测每个状\\n态的动作价值，能够训练智能体在 Atari 2600 游戏中达到类似人类的水平。策略梯度方\\n法直接对策略进行优化，而非对动作进行价值赋值。这些方法生成的是随机策略，在部\\n分可观测的环境中尤其重要。这些更新过程含有噪声，为减少其方差已经引入了多种改\\n进措施。\\n当无法直接与环境互动而必须依赖历史数据学习时，就会使用离线增强学习。决策\\n变换器(Decision Transformer) 利用深度学习的最新进展构建状态 -动作-奖励序列模型，\\n并预测能够最大化奖励的动作。\\n19.9笔记\\nSutton和Barto在2018年的作品中详细介绍了表格型增强学习方法。 Li (2017) 、\\nArulkumaran 等人(2017)、FranCois-Lavet 等人(2018)和Wang等人(2022c)分别提供\\n了深度增强学习领域的综述。 Graesser 和Keng的2019年作品是一本优秀的入门资源，\\n其中包含了 Python代码示例。',\n",
       " '习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。\\nWatkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery &\\nNiranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个\\n状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个\\n状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研\\n究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结，\\nSutton & Barto (2018) 的著作中有更为全面的论述。\\n深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论\\n衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基\\n准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、\\n离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致\\n力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul\\n等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的\\n优先经验回放。\\n原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更\\n接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络\\n架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_only_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
