{
    "raptor": [
        "The document discusses a Graph RAG approach for summarizing texts and answering queries more comprehensively and diversely than naïve RAG and global map-reduce techniques. The method involves converting source documents into text chunks, constructing a graph index with entity nodes, and partitioning it into communities using algorithms like Leiden. Community summaries are derived at varying hierarchical levels to generate final answers, demonstrating better performance at lower token costs compared to source text summarization. The study explores the effectiveness of graphs with LLMs in RAG systems, highlighting advancements in knowledge graph creation, causal graph extraction, and narrative output generation.",
        "4.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-",
        "• Directness. How specifically and clearly does the answer address the question?\nFor our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and\nasked to assess which answer is better according to the metric, as well as why. It returns the winner\nif one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.\nTo account for the stochasticity of LLMs, we run each comparison five times and use mean scores.\nTable 2 shows an example of LLM-generated assessment.\n7",
        "本文讨论了在全文语料库上进行以查询为中心的抽象总结的挑战，特别是上下文窗口中信息可能因文本过长而丢失。研究介绍了一种新的基于知识图谱的RAG方法，旨在实现全球总结。当前的方法在处理大规模文本块时表现可能不理想，但通过预先索引的方法，可以为这种任务提供支持。此外，研究测试了不同上下文窗口大小对任务的影响，发现尽管较小的上下文窗口能更全面地捕捉信息，在综合性指标上表现最佳。",
        "Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704.\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in",
        "• TS. The same method as in subsection 2.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n• SS. An implementation of na¨ıve RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\nThe graph index supporting conditions C0-C3 was created using our generic prompts for entity and\nrelationship extraction only, with entity types and few-shot examples tailored to the domain of the\ndata. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\nPodcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics",
        "This text discusses advanced methods for extracting named entities from various domains using language models (LLMs). It highlights the importance of customizing extraction techniques for domains like science or law and introduces a secondary prompt to extract additional related data. The text outlines a multi-step extraction process that ensures comprehensive coverage of entities and indicates a structured community approach for answering queries. The document compares several retrieval-augmented generation (RAG) methods across different community levels (C0-C3) and text summarization techniques, emphasizing improvements in comprehensiveness and diversity. This research is situated within the context of enhancing knowledge graphs and leveraging large language models for open-domain question answering and intelligence report generation.",
        "本文讨论了如何利用上下文学习生成摘要，但在面对大量文本时，依然面临挑战，特别是针对特定查询的抽象性摘要。这是因为大规模语言模型（LLM）的上下文窗口有限，且文本中的信息在长上下文中可能丢失。虽然直接检索文本片段的方法不足以用于查询聚焦的摘要任务，但通过预索引的另类方法，RAG（检索增强生成）可能促进整体摘要的新模式。本文提出了一种基于LLM派生知识图的图形RAG方法，用于全局总结。相比现有探索图索引的结构化检索且遍历工作，该方法强调了未被探索的方向。在某些情况下，图形RAG能更高效地进行问题回答，与源文本总结相比，性能稍有下降但保持了全面性和多样性优势。",
        "shaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa¨ıve\nRAG\nPublic figures who are repeatedly mentioned across various entertainment articles include Taylor\nSwift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\nTaylor Swift [...] Travis Kelce [...] Britney Spears [...] Justin Timberlake [...]\nThese figures are frequently covered due to their high-profile status and the public’s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\nLLM\nDecision",
        "erated answer is in answering the target question. Answers with score 0 are filtered out.\n• Reduce to global answer. Intermediate community answers are sorted in descending order\nof helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n5",
        "3.3 Conditions\nWe compare six different conditions in our analysis, including Graph RAG using four levels of graph\ncommunities (C0, C1, C2, C3), a text summarization method applying our map-reduce approach\ndirectly to source texts (TS), and a na¨ıve “semantic search” RAG approach (SS):\n• CO. Uses root-level community summaries (fewest in number) to answer user queries.\n• C1. Uses high-level community summaries to answer queries. These are sub-communities\nof C0, if present, otherwise C0 communities projected down.\n• C2. Uses intermediate-level community summaries to answer queries. These are sub-\ncommunities of C1, if present, otherwise C1 communities projected down.\n• C3. Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected down.\n• TS. The same method as in subsection 2.6, except source texts (rather than community",
        "responds that entities were missed, then a continuation indicating that “MANY entities were missed\nin the last extraction” encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances → Element Summaries\nThe use of an LLM to “extract” descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.",
        "在文本中，作者探讨了将源文档提取文本分割为文本块进行处理的设计决策。这些块被传递给用于从中提取图形索引元素的LLM提示集合。较长的文本块需要较少的LLM调用，但可能导致召回率下降的问题。通过分析样例数据发现，较小的600个标记文本块提取到的实体引用比2400个标记块多出一倍，因此需要在召回和精确度之间保持平衡。另外，比较了六种不同条件下的答案生成方法，包括不同层次的图表社群方法(例如C0到C3)，文本总结方法以及语义搜索方法。在娱乐行业中，某些公众人物由于其显著贡献和影响力，在各种娱乐文章中反复被提及，显示出他们在该行业中的持续影响力。",
        "The text outlines the Graph RAG method for optimizing question answering in large text databases. Utilizing a two-step process—forming a graph-based text index and summarizing data into communities—it demonstrates improved performance over previous methods in metrics of comprehensiveness and diversity at reduced computational costs. Techniques and algorithmic choices, such as using the Leiden algorithm for community detection, are described for efficient implementation.",
        "to understand how performance varies across different ranges of question types, data types, and\ndataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index. We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).",
        "the generic community summaries and the use of other graph-related RAG approaches).\nFuture work. The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This “roll-up” operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory “drill down” mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,",
        "Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294.\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. arXiv preprint arXiv:2005.03975.\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.",
        "in the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization.arXiv\npreprint arXiv:1905.13164.\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex.ai/en/stable/\nexamples/index structs/knowledge graph/KnowledgeGraphDemo.html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896.\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA).\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study",
        "powerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n3.6 Results\nThe indexing process resulted in a graph consisting of 8564 nodes and 20691 edges for the Podcast\ndataset, and a larger graph of 15754 nodes and 19520 edges for the News dataset. Table 3 shows the\nnumber of community summaries at different levels of each graph community hierarchy.\nGlobal approaches vs. na ¨ıve RAG. As shown in Figure 4, global approaches consistently out-\nperformed the na ¨ıve RAG (SS) approach in both comprehensiveness and diversity metrics across\ndatasets. Specifically, global approaches achieved comprehensiveness win rates between 72-83%\nfor Podcast transcripts and 72-80% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,",
        "Each level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities → Community Summaries\nThe next step is to create report-like summaries of each community in the Leiden hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful in\ntheir own right as a way to understand the global structure and semantics of the dataset, and may\nthemselves be used to make sense of a corpus in the absence of a question. For example, a user\nmay scan through community summaries at one level looking for general themes of interest, then\nfollow links to the reports at the lower level that provide more details for each of the subtopics. Here,\nhowever, we focus on their utility as part of a graph-based index used for answering global queries.\nCommunity summaries are generated in the following way:\n4",
        "该文本对一种改进大规模文本数据库问答能力的方法——图形检索增强生成（Graph RAG）进行了介绍。通过创建基于图形的文本索引与总结社群数据，该方法显著提高了理解的全面性和多样性，同时降低了计算成本。特别是使用了Leiden算法进行社群检测，解释了图形索引的模块化特性，并使用大语言模型（LLMs）实现实体、关系、与源文本的推导。文本也探讨了如何在层级化的图形社区中应对查询和总结，通过多阶段处理生成覆盖性摘要。该方法的关键特性包括在应对复杂查询时的综合性、多样性、指导性和明确性之间的平衡。此外，文档还提供了多个算法与模型的相关引用及其改进。",
        "sports, gaming, and digital media. It offers insights into the contributions and influence of these\nfigures, as well as controversies and their impact on public discourse. The answer also cites specific\ndata sources for each mentioned figure, indicating a diverse range of evidence to support the claims.\nIn contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry\nand sports, and relies heavily on a single source for data, which makes it less diverse in perspectives\nand insights.\nEmpowerment: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a comprehensive and structured overview of public figures\nacross various sectors of the entertainment industry, including film, television, music, sports, and\ndigital media. It lists multiple individuals, providing specific examples of their contributions and the\ncontext in which they are mentioned in entertainment articles, along with references to data reports",
        "Acknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern´andez, Amber Hoak, Andr ´es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M´onica Carva-\njal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for",
        "Source Documents\nText Chunks\ntext extraction\nand chunking\nElement Instances\ndomain-tailored\nsummarization\nElement Summaries\ndomain-tailored\nsummarization\nGraph Communities\ncommunity\ndetection\nCommunity Summaries\ndomain-tailored\nsummarization\nCommunity Answers\nquery-focused\nsummarization\nGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query TimePipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The “global answer” to a given query is produced using a final round of",
        "A fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks → Element Instances",
        "3.5 Configuration\nThe effect of context window size on any particular task is unclear, especially for models like\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\nbe “lost in the middle” of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\nplore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse",
        "Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288.\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. Scientific Reports, 9(1).\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. ArXiv, abs/2305.04676.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509.\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.",
        "These qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries → Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).",
        "Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\n9(6): e98679. https://doi.org/10.1371/journal.pone.0098679.\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\non Knowledge and Data Engineering, 35(2):1149–1170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024.\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-",
        "in the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.\nWang, Y ., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\nXu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint\narXiv:2106.00104.\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nYao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\nand Information Systems, 53:297–336.\n14",
        "This document explores Graph RAG, a technique merging retrieval-augmented generation and graph-based approaches. It emphasizes its use in fields like scientific research, aiding in automating sensemaking by large language models (LLMs). Future improvements consider embedding-based queries and hybrid models. Discussed are challenges of abstraction in long texts, necessitating efficient context handling. A new knowledge graph-based method seeks better global summaries by adjusting index methods and testing context window sizes. Smaller 600-token blocks improved recall over 2400-token ones. The study compares answer generation across varied community and summarization methods, noting influences of hierarchical community summaries on query handling. Additionally, it covers integrating LLMs with RAG for complex queries, reviewing tools like LangChain. Future research should encompass diverse questions and data types to refine graph-based systems' effectiveness.",
        "for each claim. This approach helps the reader understand the breadth of the topic and make informed\njudgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures\nand primarily discusses their personal lives and relationships, which may not provide as broad an\nunderstanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety\nof Answer 1.\nDirectness: Winner=2 (Na¨ıve RAG)\nAnswer 2 is better because it directly lists specific public figures who are repeatedly mentioned\nacross various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin\nTimberlake, and provides concise explanations for their frequent mentions. Answer 1, while\ncomprehensive, includes a lot of detailed information about various figures in different sectors of\nentertainment, which, while informative, does not directly answer the question with the same level of\nconciseness and specificity as Answer 2.",
        "system supports both creation and traversal of text-relationship graphs for multi-hop question an-\nswering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are\nsupported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization.\n5 Discussion\nLimitations of evaluation approach . Our evaluation to date has only examined a certain class of\nsensemaking questions for two corpora in the region of 1 million tokens. More work is needed\nto understand how performance varies across different ranges of question types, data types, and",
        "本文探讨了通过结合检索增强生成（RAG）和基于图形的方法来改进针对查询的内容总结能力，特别是用于大规模的文本数据集。传统的RAG在处理全面性问题时存在不足，针对这一点，本文引入知识图谱以提高总结的全面性和多样性，优化了总结的效果并降低了计算成本。文中详细介绍了一种新的Graph RAG流程，使用图形索引将数据组织成可管理的部分，并展示了在播客和新闻文章中的优越表现。研究还归纳了该方法在回答复杂查询及总结全球性观点方面的能力，强调其可扩展性和处理多种实体的能力。新方法还利用大语言模型推动内容总结任务的多样化，体现出在科学研究等领域的应用潜力，同时探讨了未来在嵌入式查询及混合模型上的发展可能性。通过更小的语境窗口对比，指出改进的回顾能力，并综述了结合LLMs与RAG处理复杂查询的工具，强调未来研究需涵盖多样化的问题和数据类型以提高系统效能。",
        "P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\nneural information processing systems, 33:1877–1901.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\ntems, 36.\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\nlarge language models. arXiv preprint arXiv:2310.05149.\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.",
        "71 56 59 50 50 51\n69 60 59 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 47 57 49 50 50\n53 50 58 50 50 48\n43 42 50 42 45 44\n51 50 58 50 52 51\n50 50 55 48 50 50\n50 52 56 49 50 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 54 59 55 55 54\n46 50 55 53 52 52\n41 45 50 48 48 47\n45 47 52 50 49 49\n45 48 52 51 50 49\n46 48 53 51 51 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDirectness\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na¨ıve RAG\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n3.5 Configuration",
        "This document examines enhancing query-focused summarization (QFS) by combining retrieval-augmented generation (RAG) with graph-based methods for handling comprehensive queries over large text datasets. Traditional RAG faces challenges in answering comprehensive questions across entire datasets, and the proposed method addresses this by incorporating knowledge graphs to improve summary comprehensiveness and diversity. The approach provides significant improvements over existing RAG and summarization techniques, allowing for effective global querying with reduced computational costs. The paper elaborates on a Graph RAG pipeline that uses an LLM-directed graph index to organize data into manageable sections for QFS via a map-reduce method, showcasing superior performance with podcasts and news articles. Additionally, the system's scalability and capacity for handling varied entities are highlighted, promoting iterative question answering and global sensemaking. The study also notes advancements in large language models (LLMs) that blur distinctions in summarization tasks, allowing more versatile content summarization.",
        "From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1† Ha Trinh1† Newman Cheng2 Joshua Bradley2 Alex Chao3\nApurva Mody3 Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n@microsoft.com\n†These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas “What are the main themes in the dataset?”, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical",
        "all of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be “lost in the middle” of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na¨ıve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present aGraph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored",
        "Fortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., and Wang, H. (2023). Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.\nGoodwin, T. R., Savery, M. E., and Demner-Fushman, D. (2020). Flight of the pegasus? comparing\ntransformers on few-shot and zero-shot multi-document abstractive summarization. In Proceed-\nings of COLING. International Conference on Computational Linguistics , volume 2020, page\n5640. NIH Public Access.\nHe, X., Tian, Y ., Sun, Y ., Chawla, N. V ., Laurent, T., LeCun, Y ., Bresson, X., and Hooi, B. (2024).\nG-retriever: Retrieval-augmented generation for textual graph understanding and question an-\nswering. arXiv preprint arXiv:2402.07630.\n12",
        "baseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https://aka.ms/graphrag.\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.\narXiv:2404.16130v1  [cs.CL]  24 Apr 2024",
        "and traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n2",
        "The document outlines a Graph RAG pipeline for document summarization using an LLM-derived graph index composed of nodes, edges, and covariates extracted and summarized by domain-specific LLM prompts. The approach employs community detection to organize the graph into parallelizable sections, enabling query-focused summarization through a map-reduce method. The process evaluates the technique with podcasts and news articles, showcasing its efficiency and diversity advantages over traditional methods. The system is robust to entity variations and offers scalability benefits, making it effective for sensemaking through iterative question answering.",
        "Laskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In Advances in\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13–15, 2020, Proceedings 33, pages 342–348. Springer.\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\ners for query-focused abstractive text summarization. Computational Linguistics, 48(2):279–320.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K ¨uttler, H., Lewis, M., Yih,\nW.-t., Rockt¨aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems, 33:9459–9474.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost",
        "i.e., that na¨ıve RAG produces the most direct responses across all comparisons.\n9",
        "The text explores the inherent modularity of graph indexes, emphasizing the potential of community detection algorithms to organize graph elements into related clusters. This process utilizes large language models (LLMs) for extracting entities, relationships, and claims from source texts. For specialized domains, few-shot examples enhance entity extraction. Through multi-stage processing, LLMs ensure thorough identification of entities, producing comprehensive abstractive summaries. The document also acknowledges individuals shaping cultural narratives through entertainment, while a brief mention of an acknowledgments section recognizes contributors to the research.",
        "The text describes the application of the Leiden algorithm for detecting graph communities in the MultiHop-RAG dataset. It explains the hierarchical clustering approach used to identify root and sub-communities, and how these findings aid iterative community-level summarization for efficient data representation and analysis. References highlight relevant methodologies and advancements in leveraging augmented language model capabilities for data management tasks.",
        "Question Which public figures are repeatedly mentioned across various entertainment articles?\nGraph\nRAG\nOverview of Prominent Public Figures in Entertainment\nThe entertainment industry is vast and diverse, encompassing film, television, music, sports, and\ndigital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\nActors and Directors [...] Public Figures in Controversy [...] Musicians and Executives [...]\nAthletes and Coaches [...] Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public’s interest in their work. Their influence spans across various aspects of entertainment, from",
        "该文本主要讨论了一个用于回答全数据集用户问题的方法，称为\"检索增强生成\"（RAG），与\"基于查询的摘要\"（QFS）更为适配。RAG专注于本地文本区域的检索，而QFS需要在检索后进行与查询相关的全局摘要。在整个语料库上进行人工主导的\"解构\"需要一种方式供用户在数据中应用和调整他们的心智模型。\n此方法中引入了大语言模型（LLM）的全局、基于查询的摘要能力，该过程的关键是防止同一实体以不同格式出现重复。\n通过使用富描述性文本建立一个尽可能均匀的向量图结构，与典型知识图谱不同，该方法能够在噪声较大的图结构中提炼出可用的信息。该文本还涉及不同上下文窗口大小对某些任务的影响，表明较小的上下文窗口更有利于综合性的比较，而较大的窗口在多样性和全面性上表现相近。 \n此外，文本还涉及如何将当前事物的健康和技术领域内容整合到教育课程中，如何利用更新的政策和法规推动科技发展，以及语境窗口大小对使用时效和信息获取全面性的影响，强调了8k窗口在全面性上的优势。",
        "LLM\nDecision\nComprehensiveness: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more comprehensive and detailed list of public figures from a\nwider range of entertainment sectors, including film, television, music, sports, gaming, and digital\nmedia. It also includes specific examples of their contributions and the impact they have on their\nrespective fields, as well as mentions of controversies and their implications. Answer 2, while\ndetailed in its coverage of a few individuals, is limited to a smaller number of public figures and\nfocuses primarily on their personal lives and relationships rather than a broad spectrum of their\nprofessional influence across the entertainment industry.\nDiversity: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more varied and rich response by covering a wide range of\npublic figures from different sectors of the entertainment industry, including film, television, music,",
        "compare, and the lack of gold standard answers to our activity-based sensemaking questions, we\ndecided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three\ntarget metrics capturing qualities that are desirable for sensemaking activities, as well as a control\nmetric (directness) used as a indicator of validity. Since directness is effectively in opposition to\ncomprehensiveness and diversity, we would not expect any method to win across all four metrics.\nOur head-to-head measures computed using an LLM evaluator are as follows:\n• Comprehensiveness. How much detail does the answer provide to cover all aspects and\ndetails of the question?\n• Diversity. How varied and rich is the answer in providing different perspectives and insights\non the question?\n• Empowerment. How well does the answer help the reader understand and make informed\njudgements about the topic?\n• Directness. How specifically and clearly does the answer address the question?",
        "The text describes a study evaluating methods for improving human sensemaking over large text corpora. Four metrics—comprehensiveness, diversity, empowerment, and directness—were used to assess the quality of summaries generated by different methods. The study found that community summaries based on a Graph RAG approach yielded improvements in comprehensiveness and diversity compared to na\\\"ive methods. Furthermore, root-level community summaries provided an efficient data index for multiple queries, maintaining competitive performance while reducing token costs. An implementation of these methods is planned to be released as open-source software.",
        "Microsoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669\n× 600-token text chunks, with 100-token overlaps between chunks (∼1 million tokens).\n• News articles. Benchmark dataset comprising news articles published from September\n2013 to December 2023 in a range of categories, including entertainment, business, sports,\ntechnology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 ×\n600-token text chunks, with 100-token overlaps between chunks (∼1.7 million tokens).\n3.2 Queries\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthe associated question sets target explicit fact retrieval rather than summarization for the purpose\nof data sensemaking, i.e., the process though which people inspect, engage with, and contextualize",
        "ing time and query time. The “global answer” to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n“a motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively” (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in",
        "summaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries (C0), it required over 97% fewer tokens. For a modest drop in\nperformance compared with other global methods, root-level Graph RAG offers a highly efficient\nmethod for the iterative question answering that characterizes sensemaking activity, while retaining\nadvantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na¨ıve RAG.\nEmpowerment. Empowerment comparisons showed mixed results for both global approaches versus",
        "We have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna¨ıve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na ¨ıve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https://aka.ms/graphrag.\n11",
        "sity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na ¨ıve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na ¨ıve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents → Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-",
        "This text compares two responses analyzing public figures in the entertainment industry. **Answer 1** provides a detailed overview across diverse sectors like film, music, and digital media, supported by various data sources that help readers form well-rounded opinions. **Answer 2**, focusing mainly on music and sports celebrities, offers a less varied perspective. Although it’s more concise, it lacks the depth found in Answer 1. The text also discusses advanced retrieval-augmented-generation systems, which aim to improve data handling in large language models by overcoming limitations of simpler systems through strategic retrieval methods.",
        "of community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a “tree of clarifications” to answer mul-\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n10",
        "2.2 Text Chunks → Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3",
        "这篇文章探讨了使用GPT-4-Turbo的一种新型实体提取方法，通过在HotPotQA数据集上的实验，研究实体引用数量随块大小和提取次数的变化。文本分析主要基于社区描述的完整覆盖，采用图和Map-Reduce方法实现全局查询总结。此方法被用于评估两个包含播客和新闻文章的数据集，通过生成多样化的问题以测试其效果，并使用LLM生成潜在用户任务的例子。总结中还包括对娱乐业中重复出现的公众人物的总结。\n\n文章强调使用层次化索引和Summarization方法，如Graph RAG等，结合提取-生成策略来应对多文档摘要和多跳问题回答。总的来说，文本提到的方法旨在通过生成全面的回答，改善对大型数据集的理解和总结能力。",
        "covariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related “communities” of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on",
        "这项研究分析了如何利用检索增强生成（RAG）方法以及图形索引技术改进大文本集的摘要能力，尤其在识别全局性问题的主题方面。通过引入图索引和优化上下文窗口的方法，研究能够更好地提取实体、关系和主张，并总结为更广泛的主题。社区检测算法的应用则提高了分析的效率。在实验中证实了这些方法在生成简捷、明确答案方面的有效性，特别是在高层次问题的解答上表现优越。研究还探讨了方法间在特定人物信息比较时的竞争力，注重计算资源与索引益处间的平衡。未来发展的方向包括基于更本地化的嵌入匹配查询和混合RAG方案以实现更细致的总结。",
        "NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\ntion with llm based on knowledge graphs. https://www.nebula-graph.io/posts/graph-RAG.\nNeo4J (2024). Project NaLLM. https://github .com/neo4j/NaLLM.\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\nnational academy of sciences, 103(23):8577–8582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. arXiv preprint arXiv:2310.13848.\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.",
        "decreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.\n• Higher-level communities. If all element summaries fit within the token limit of the con-\ntext window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until fit within the context window is achieved.\n2.6 Community Summaries → Community Answers → Global Answer\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising",
        "The document discusses the development and potential advancements of Graph RAG, a method that combines retrieval-augmented generation with graph-related approaches to enhance the comprehension and diversity of responses generated by language models. It highlights the application of large language models (LLMs) in complex fields like scientific research, emphasizing their role in automating sensemaking. Future enhancements include embedding-based query matching and hybrid RAG models. The text also references several studies and tools related to Graph RAG's evolution and implementation.",
        "The document details a Graph RAG approach that enhances text summarization and query response using large language models (LLMs). It involves partitioning text into nodes in a graph index using algorithms like Leiden, creating community-based summaries that outperform traditional methods in token efficiency and diversity. The study emphasizes LLMs in extracting named entities and relationships across domains, illustrating their effectiveness in generating comprehensive and relevant summaries without predefined answers. This research showcases advancements in knowledge graphs, causal extraction, and narrative generation, providing tools for improved information retrieval and intelligence report production. A multi-step extraction process is introduced, tailored to fields like science and law, using secondary prompts for enhanced data capture. Graph RAG offers a structured, community-oriented method, enabling better data representation and query handling, while upcoming open-source releases promise broader application. Evaluated through metrics like comprehensiveness and empowerment, the approach proves superior in creating summaries, suggesting a significant leap in RAG systems' capabilities.",
        "This text discusses the role of large language models (LLMs) in processing and evaluating textual data, particularly in creating graph representations of text and evaluating natural language generation. It highlights how LLMs can identify entities and relationships within text, allowing for their graphical modeling. The deployment of LLMs in different domains, like podcasts and news, and their capacity to produce metrics for fluency and relevance without gold standard answers, underlines their effectiveness as evaluators. The text also references the Graph RAG system, combining various retrieval and augmentation methods, showcasing the potential of LLMs in enhancing information retrieval and generation.",
        "本文探讨了在文本语料库中促进以人为主导的意义构建的过程，强调了检索增强生成（RAG）和针对查询的抽象性总结（QFS）的重要性。这些方法支持用户提出全局性问题，帮助改进思维模型并生成与查询相关的总结。研究还提到近年来的技术进展如大型语言模型（LLMs）使得不同摘要任务的区分变得不再明显，这些模型能够在上下文窗口内总结任何提供的内容。通过建立图形索引结构，可生成分层的社区总结，最终提出全球答案。图形RAG机制在综合性和多样性上超过了简单RAG方法，但在构建图形索引时需考虑计算预算和查询需求。",
        "Dataset Example activity framing and generation of global sensemaking questions\nPodcast\ntranscripts\nUser: A tech journalist looking for insights and trends in the tech industry\nTask: Understanding how tech leaders view the role of policy and regulation\nQuestions:\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\nNews\narticles\nUser: Educator incorporating current affairs into curricula\nTask: Teaching about health and wellness\nQuestions:\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?",
        "全文讨论了一种基于大语言模型（LLM）生成的图索引来进行文档文本的处理和总结的方法。核心思想是使用图索引来表述文档中检测和抽取的节点（实体）、边（关系）和协变量（论断），并通过社区检测算法（如Leiden模型）将图索引划分为多个小组，以便进行并行处理和总结。这种方法强调图结构的模组化特性，可以通过将文本分块来平衡提取的精准度与召回率。未来的改进方向包括更本地化的基于嵌入的匹配查询方式，以及混合RAG方案以实现更深入细致的总结作业。",
        "Podcast transcripts\n50 17 28 25 22 21\n83 50 50 48 43 44\n72 50 50 53 50 49\n75 52 47 50 52 50\n78 57 50 48 50 52\n79 56 51 50 48 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nComprehensiveness\n50 18 23 25 19 19\n82 50 50 50 43 46\n77 50 50 50 46 44\n75 50 50 50 44 45\n81 57 54 56 50 48\n81 54 56 55 52 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 42 57 52 49 51\n58 50 59 55 52 51\n43 41 50 49 47 48\n48 45 51 50 49 50\n51 48 53 51 50 51\n49 49 52 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 56 65 60 60 60\n44 50 55 52 51 52\n35 45 50 47 48 48\n40 48 53 50 50 50\n40 49 52 50 50 50\n40 48 52 50 50 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDirectness\nNews articles\n50 20 28 25 21 21\n80 50 44 41 38 36\n72 56 50 52 54 52\n75 59 48 50 58 55\n79 62 46 42 50 59\n79 64 48 45 41 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nComprehensiveness\n50 33 38 35 29 31\n67 50 53 45 44 40\n62 47 50 40 41 41\n65 55 60 50 50 50\n71 56 59 50 50 51\n69 60 59 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 47 57 49 50 50",
        "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na ¨ıve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An",
        "This paper explores the combination of retrieval-augmented generation (RAG) with graph-based methods to enhance query-focused summarization (QFS), a process designed to answer global queries over extensive text corpora. Traditional RAG techniques struggle with comprehensive questions about entire datasets, such as identifying overarching themes. The proposed approach improves on this by integrating knowledge graphs to support global sensemaking and enhance the comprehensiveness and diversity of summaries. This method demonstrates significant advancements over RAG baselines and traditional summarization techniques, providing effective global querying capabilities at lower computational costs. Future open-source Python implementations of the methodology are anticipated.",
        "Microsoft (2023). The impact of large language models on scientific discovery: a preliminary study\nusing gpt-4.\n13",
        "该研究探讨了检索增强生成（RAG）在从外部知识源检索相关信息以回答问题中的应用，但RAG在处理全局性问题时面临挑战，如识别数据集的主要主题。这项工作通过引入图形索引技术及优化上下文窗口创建方法，改进了RAG对大文本集的摘要能力。作者采用通用提示进行实体与关系提取，以提升知识索引，并在多种条件实验中评估了方法的有效性，尤其是在生成简洁明了答案方面取得进展。研究还涉及对特定人物信息的直接性比较，强调不同方法在回答问题时的竞争优势。",
        "(a) Root communities at level 0 (b) Sub-communities at level 1\nFigure 3: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\nof hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n• Leaf-level communities. The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add",
        "0 1 2 30\n10000\n20000\n30000\nNumber of gleanings performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo.\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-",
        "Empowerment. Empowerment comparisons showed mixed results for both global approaches versus\nna¨ıve RAG (SS) and Graph RAG approaches versus source text summarization (TS). Ad-hoc LLM\nuse to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the Graph RAG index.\n4 Related Work\n4.1 RAG Approaches and Systems\nWhen using LLMs, RAG involves first retrieving relevant information from external data sources,\nthen adding this information to the context window of the LLM along with the original query (Ram\net al., 2023). Na ¨ıve RAG approaches (Gao et al., 2023) do this by converting documents to text,\nsplitting text into chunks, and embedding these chunks into a vector space in which similar positions",
        "represent similar semantics. Queries are then embedded into the same vector space, with the text\nchunks of the nearest k vectors used as context. More advanced variations exist, but all solve the\nproblem of what to do when an external dataset of interest exceeds the LLM’s context window.\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\ncome the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)",
        "per dataset. Table 1 shows example questions for each of the two evaluation datasets.\n6",
        "该文本介绍了一种通过分层结构对图社群进行总结的方法，这种方法允许对大型数据集进行全局理解。在Leiden层次结构中，每个层级提供了图节点的互斥和集体穷尽的社群划分，生成的社群总结有助于用户在缺乏具体问题时理解数据集的整体结构和语义。用户可以通过浏览上层的社群总结识别感兴趣的主题，再根据这些主题访问较低层级的详尽报告。这些总结支持图形化索引，以响应全局查询。此外，文本还涉及与知识增强语言模型和检索增强生成（RAG）相关的研究工作，重点在于如何有效地理解文本内容并回答复杂查询。",
        "of data sensemaking, i.e., the process though which people inspect, engage with, and contextualize\ndata within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for\nextracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such\nextracted questions can target details that betray prior knowledge of the texts.\nTo evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions\nthat convey only a high-level understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify N potential users and N tasks per user,\nthen for each (user, task) combination, we asked the LLM to generate N questions that require\nunderstanding of the entire corpus. For our evaluation, a value ofN = 5 resulted in 125 test questions",
        "2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n3 Evaluation\n3.1 Datasets\nWe selected two datasets in the one million token range, each equivalent to about 10 novels of text\nand representative of the kind of corpora that users may encounter in their real world activities:\n• Podcast transcripts. Compiled transcripts of podcast conversations between Kevin Scott,\nMicrosoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669",
        "Podcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55 555 1797 2142 3197\nTokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n% Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\nTable 3: Number of context units (community summaries forC0-C3 and text chunks for TS), corre-\nsponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries (C0) require dramatically fewer tokens per query (9x-43x).\nCommunity summaries vs. source texts. When comparing community summaries to source texts\nusing Graph RAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved",
        "本文详细介绍了一种名为Graph RAG的创新方法，该方法基于利用大型语言模型从私有文本语料库中生成基于图结构的索引，从而支持更全面且多样化的问答能力。Graph RAG 涉及从文档中创建实体知识图并生成社区摘要，利用图和Map-Reduce方法处理全局查询任务。研究证明，该方法相比传统方法可在大规模数据集上提升语料综述效果。这种方法还能更多地适用于健康和技术领域创新及其相关政策发展的总结分析。总体而言，文章展示了如何通过综合性索引和表达手段改进数据集的多文档摘要和复杂问答能力。",
        "The document provides an overview of graph communities analyzed using the Leiden algorithm on the MultiHop-RAG dataset. Entities are organized into hierarchical structures, with the arrangement visualized through OpenORD and Force Atlas 2. Two levels of clustering are discussed: \n- **Level 0 (root communities):** Achieves maximal modularity.\n- **Level 1 (sub-communities):** Explores deeper structural nuances.\n\nThe focus includes leaf-level community handling through LLM (Language Learning Model) contexts, which prefers influential nodes and edges based on prominence. The document highlights a comparative evaluation via LLMs across four metrics: \n1. **Comprehensiveness** - detail coverage.\n2. **Diversity** - richness in perspectives.\n3. **Empowerment** - guiding informed decision-making.\n4. **Directness** - clarity and specificity of responses.\n\nThe challenge lies in balancing directness with the other three metrics, as they are often conflicting in nature. Supporting references discuss related algorithms, data sensemaking models, and methodological advancements in NLP, covering statistical, deep learning, and retrieval-augmented approaches.",
        "means that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 3).\nFor a given community level, the global answer to any user query is generated as follows:\n• Prepare community summaries. Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n• Map community answers. Generate intermediate answers in parallel, one for each chunk.\nThe LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\nerated answer is in answering the target question. Answers with score 0 are filtered out.",
        "For example, while our default prompt extracting the broad class of “named entities” like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of “gleanings”, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM",
        "Podcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics\nLLMs have been shown to be good evaluators of natural language generation, achieving state-of-\nthe-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,\n2024). While this approach can generate reference-based metrics when gold standard answers are\nknown, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference-\nfree style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM-\nas-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of\nconventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,\nand answer relevance (RAGAS, Es et al., 2023).\nGiven the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to\ncompare, and the lack of gold standard answers to our activity-based sensemaking questions, we",
        "Instead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.",
        "Yao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\ngraph completion.\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116.\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301.\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems, 36.\n15",
        "conciseness and specificity as Answer 2.\nTable 2: Example question for the News article dataset, with generated answers from Graph RAG\n(C2) and Na¨ıve RAG, as well as LLM-generated assessments.\n8",
        "The text discusses the use of hierarchical community summaries to answer user queries effectively, by distributing relevant information across multiple chunks to avoid data loss. It evaluates different community levels to determine the most effective for summarizing general sensemaking questions.\n\nThe implementation involves preparing community summaries, generating intermediate answers, and evaluating them with a score. Some conditions in the study showed improvements in summary comprehensiveness and diversity.\n\nThe text also explores the integration of graphs with Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) in answering complex questions. Various advanced methods and systems for knowledge graph creation, completion, and text-relationship graph traversal are mentioned, highlighting ongoing research and available software such as LangChain and LlamaIndex. \n\nFinally, it notes the need for further evaluation on a wider range of questions, data types, and graph-based systems to better understand performance variations.",
        "本文提出了一种名为Graph RAG的创新方法，用于在私有文本语料库上进行问答分析，同时兼具用户问题的广泛性和文本来源的规模扩展需求。这种方法通过两个步骤利用大型语言模型创建基于图的文本索引：首先从源文档中派生出实体知识图，然后为所有相关实体群体预生成社区摘要。通过应用于大型Token范围数据集上的全局观点感知问题，实验表明，与传统的RAG基线方法相比，该方法在生成答案的全面性和多样性上均有明显改进。",
        "The text discusses methodologies for enhancing data summarization and sensemaking using graph-based approaches and large language models (LLMs). A major focus is on creating hierarchical summaries from source texts, extracting entities, relationships, and claims, and aggregating these into broader themes using community detection techniques within graph structures. Furthermore, it emphasizes evaluating and validating the performance of these methodologies for answering high-level queries, underscoring their potential applications in extensive data corpus analysis while considering trade-offs between computational resources and the benefits of graph-based indexing.",
        "ous questions with retrieval-augmented large language models.arXiv preprint arXiv:2310.14696.\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\nperspectives. IEEE intelligent systems, 21(4):70–73.\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\ntive model. IEEE Intelligent systems, 21(5):88–92.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets–understanding data\nsensemaking behaviours. International journal of human-computer studies, 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\nLangChain (2024). Langchain graphs. https://python .langchain.com/docs/use cases/graph/.\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via"
    ],
    "RAG": [
        "A fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks → Element Instances",
        "Source Documents\nText Chunks\ntext extraction\nand chunking\nElement Instances\ndomain-tailored\nsummarization\nElement Summaries\ndomain-tailored\nsummarization\nGraph Communities\ncommunity\ndetection\nCommunity Summaries\ndomain-tailored\nsummarization\nCommunity Answers\nquery-focused\nsummarization\nGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query TimePipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The “global answer” to a given query is produced using a final round of",
        "• TS. The same method as in subsection 2.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n• SS. An implementation of na¨ıve RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\nThe graph index supporting conditions C0-C3 was created using our generic prompts for entity and\nrelationship extraction only, with entity types and few-shot examples tailored to the domain of the\ndata. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\nPodcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics",
        "71 56 59 50 50 51\n69 60 59 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 47 57 49 50 50\n53 50 58 50 50 48\n43 42 50 42 45 44\n51 50 58 50 52 51\n50 50 55 48 50 50\n50 52 56 49 50 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 54 59 55 55 54\n46 50 55 53 52 52\n41 45 50 48 48 47\n45 47 52 50 49 49\n45 48 52 51 50 49\n46 48 53 51 51 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDirectness\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na¨ıve RAG\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n3.5 Configuration",
        "From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1† Ha Trinh1† Newman Cheng2 Joshua Bradley2 Alex Chao3\nApurva Mody3 Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n@microsoft.com\n†These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas “What are the main themes in the dataset?”, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical",
        "2.2 Text Chunks → Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3",
        "Podcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics\nLLMs have been shown to be good evaluators of natural language generation, achieving state-of-\nthe-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,\n2024). While this approach can generate reference-based metrics when gold standard answers are\nknown, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference-\nfree style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM-\nas-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of\nconventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,\nand answer relevance (RAGAS, Es et al., 2023).\nGiven the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to\ncompare, and the lack of gold standard answers to our activity-based sensemaking questions, we",
        "Instead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.",
        "decreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.\n• Higher-level communities. If all element summaries fit within the token limit of the con-\ntext window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until fit within the context window is achieved.\n2.6 Community Summaries → Community Answers → Global Answer\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising",
        "in the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.\nWang, Y ., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\nXu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint\narXiv:2106.00104.\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nYao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\nand Information Systems, 53:297–336.\n14",
        "means that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 3).\nFor a given community level, the global answer to any user query is generated as follows:\n• Prepare community summaries. Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n• Map community answers. Generate intermediate answers in parallel, one for each chunk.\nThe LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\nerated answer is in answering the target question. Answers with score 0 are filtered out.",
        "Acknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern´andez, Amber Hoak, Andr ´es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M´onica Carva-\njal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for",
        "ous questions with retrieval-augmented large language models.arXiv preprint arXiv:2310.14696.\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\nperspectives. IEEE intelligent systems, 21(4):70–73.\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\ntive model. IEEE Intelligent systems, 21(5):88–92.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets–understanding data\nsensemaking behaviours. International journal of human-computer studies, 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\nLangChain (2024). Langchain graphs. https://python .langchain.com/docs/use cases/graph/.\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via",
        "3.5 Configuration\nThe effect of context window size on any particular task is unclear, especially for models like\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\nbe “lost in the middle” of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\nplore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse",
        "• Directness. How specifically and clearly does the answer address the question?\nFor our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and\nasked to assess which answer is better according to the metric, as well as why. It returns the winner\nif one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.\nTo account for the stochasticity of LLMs, we run each comparison five times and use mean scores.\nTable 2 shows an example of LLM-generated assessment.\n7",
        "system supports both creation and traversal of text-relationship graphs for multi-hop question an-\nswering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are\nsupported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization.\n5 Discussion\nLimitations of evaluation approach . Our evaluation to date has only examined a certain class of\nsensemaking questions for two corpora in the region of 1 million tokens. More work is needed\nto understand how performance varies across different ranges of question types, data types, and",
        "of community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a “tree of clarifications” to answer mul-\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n10",
        "Microsoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669\n× 600-token text chunks, with 100-token overlaps between chunks (∼1 million tokens).\n• News articles. Benchmark dataset comprising news articles published from September\n2013 to December 2023 in a range of categories, including entertainment, business, sports,\ntechnology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 ×\n600-token text chunks, with 100-token overlaps between chunks (∼1.7 million tokens).\n3.2 Queries\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthe associated question sets target explicit fact retrieval rather than summarization for the purpose\nof data sensemaking, i.e., the process though which people inspect, engage with, and contextualize",
        "Question Which public figures are repeatedly mentioned across various entertainment articles?\nGraph\nRAG\nOverview of Prominent Public Figures in Entertainment\nThe entertainment industry is vast and diverse, encompassing film, television, music, sports, and\ndigital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\nActors and Directors [...] Public Figures in Controversy [...] Musicians and Executives [...]\nAthletes and Coaches [...] Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public’s interest in their work. Their influence spans across various aspects of entertainment, from",
        "Dataset Example activity framing and generation of global sensemaking questions\nPodcast\ntranscripts\nUser: A tech journalist looking for insights and trends in the tech industry\nTask: Understanding how tech leaders view the role of policy and regulation\nQuestions:\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\nNews\narticles\nUser: Educator incorporating current affairs into curricula\nTask: Teaching about health and wellness\nQuestions:\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?",
        "summaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries (C0), it required over 97% fewer tokens. For a modest drop in\nperformance compared with other global methods, root-level Graph RAG offers a highly efficient\nmethod for the iterative question answering that characterizes sensemaking activity, while retaining\nadvantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na¨ıve RAG.\nEmpowerment. Empowerment comparisons showed mixed results for both global approaches versus",
        "P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\nneural information processing systems, 33:1877–1901.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\ntems, 36.\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\nlarge language models. arXiv preprint arXiv:2310.05149.\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.",
        "Yao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\ngraph completion.\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116.\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301.\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems, 36.\n15",
        "3.3 Conditions\nWe compare six different conditions in our analysis, including Graph RAG using four levels of graph\ncommunities (C0, C1, C2, C3), a text summarization method applying our map-reduce approach\ndirectly to source texts (TS), and a na¨ıve “semantic search” RAG approach (SS):\n• CO. Uses root-level community summaries (fewest in number) to answer user queries.\n• C1. Uses high-level community summaries to answer queries. These are sub-communities\nof C0, if present, otherwise C0 communities projected down.\n• C2. Uses intermediate-level community summaries to answer queries. These are sub-\ncommunities of C1, if present, otherwise C1 communities projected down.\n• C3. Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected down.\n• TS. The same method as in subsection 2.6, except source texts (rather than community",
        "to understand how performance varies across different ranges of question types, data types, and\ndataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index. We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).",
        "responds that entities were missed, then a continuation indicating that “MANY entities were missed\nin the last extraction” encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances → Element Summaries\nThe use of an LLM to “extract” descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.",
        "per dataset. Table 1 shows example questions for each of the two evaluation datasets.\n6",
        "the generic community summaries and the use of other graph-related RAG approaches).\nFuture work. The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This “roll-up” operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory “drill down” mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,",
        "erated answer is in answering the target question. Answers with score 0 are filtered out.\n• Reduce to global answer. Intermediate community answers are sorted in descending order\nof helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n5",
        "powerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n3.6 Results\nThe indexing process resulted in a graph consisting of 8564 nodes and 20691 edges for the Podcast\ndataset, and a larger graph of 15754 nodes and 19520 edges for the News dataset. Table 3 shows the\nnumber of community summaries at different levels of each graph community hierarchy.\nGlobal approaches vs. na ¨ıve RAG. As shown in Figure 4, global approaches consistently out-\nperformed the na ¨ıve RAG (SS) approach in both comprehensiveness and diversity metrics across\ndatasets. Specifically, global approaches achieved comprehensiveness win rates between 72-83%\nfor Podcast transcripts and 72-80% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,",
        "Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288.\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. Scientific Reports, 9(1).\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. ArXiv, abs/2305.04676.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509.\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.",
        "We have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna¨ıve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na ¨ıve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https://aka.ms/graphrag.\n11",
        "These qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries → Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).",
        "(a) Root communities at level 0 (b) Sub-communities at level 1\nFigure 3: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\nof hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n• Leaf-level communities. The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add",
        "covariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related “communities” of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on",
        "shaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa¨ıve\nRAG\nPublic figures who are repeatedly mentioned across various entertainment articles include Taylor\nSwift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\nTaylor Swift [...] Travis Kelce [...] Britney Spears [...] Justin Timberlake [...]\nThese figures are frequently covered due to their high-profile status and the public’s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\nLLM\nDecision",
        "Podcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55 555 1797 2142 3197\nTokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n% Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\nTable 3: Number of context units (community summaries forC0-C3 and text chunks for TS), corre-\nsponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries (C0) require dramatically fewer tokens per query (9x-43x).\nCommunity summaries vs. source texts. When comparing community summaries to source texts\nusing Graph RAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved",
        "in the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization.arXiv\npreprint arXiv:1905.13164.\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex.ai/en/stable/\nexamples/index structs/knowledge graph/KnowledgeGraphDemo.html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896.\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA).\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study",
        "Microsoft (2023). The impact of large language models on scientific discovery: a preliminary study\nusing gpt-4.\n13",
        "For example, while our default prompt extracting the broad class of “named entities” like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of “gleanings”, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM",
        "Empowerment. Empowerment comparisons showed mixed results for both global approaches versus\nna¨ıve RAG (SS) and Graph RAG approaches versus source text summarization (TS). Ad-hoc LLM\nuse to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the Graph RAG index.\n4 Related Work\n4.1 RAG Approaches and Systems\nWhen using LLMs, RAG involves first retrieving relevant information from external data sources,\nthen adding this information to the context window of the LLM along with the original query (Ram\net al., 2023). Na ¨ıve RAG approaches (Gao et al., 2023) do this by converting documents to text,\nsplitting text into chunks, and embedding these chunks into a vector space in which similar positions",
        "Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704.\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in",
        "baseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https://aka.ms/graphrag.\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.\narXiv:2404.16130v1  [cs.CL]  24 Apr 2024",
        "compare, and the lack of gold standard answers to our activity-based sensemaking questions, we\ndecided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three\ntarget metrics capturing qualities that are desirable for sensemaking activities, as well as a control\nmetric (directness) used as a indicator of validity. Since directness is effectively in opposition to\ncomprehensiveness and diversity, we would not expect any method to win across all four metrics.\nOur head-to-head measures computed using an LLM evaluator are as follows:\n• Comprehensiveness. How much detail does the answer provide to cover all aspects and\ndetails of the question?\n• Diversity. How varied and rich is the answer in providing different perspectives and insights\non the question?\n• Empowerment. How well does the answer help the reader understand and make informed\njudgements about the topic?\n• Directness. How specifically and clearly does the answer address the question?",
        "for each claim. This approach helps the reader understand the breadth of the topic and make informed\njudgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures\nand primarily discusses their personal lives and relationships, which may not provide as broad an\nunderstanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety\nof Answer 1.\nDirectness: Winner=2 (Na¨ıve RAG)\nAnswer 2 is better because it directly lists specific public figures who are repeatedly mentioned\nacross various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin\nTimberlake, and provides concise explanations for their frequent mentions. Answer 1, while\ncomprehensive, includes a lot of detailed information about various figures in different sectors of\nentertainment, which, while informative, does not directly answer the question with the same level of\nconciseness and specificity as Answer 2.",
        "Laskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In Advances in\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13–15, 2020, Proceedings 33, pages 342–348. Springer.\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\ners for query-focused abstractive text summarization. Computational Linguistics, 48(2):279–320.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K ¨uttler, H., Lewis, M., Yih,\nW.-t., Rockt¨aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems, 33:9459–9474.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost",
        "represent similar semantics. Queries are then embedded into the same vector space, with the text\nchunks of the nearest k vectors used as context. More advanced variations exist, but all solve the\nproblem of what to do when an external dataset of interest exceeds the LLM’s context window.\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\ncome the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)",
        "LLM\nDecision\nComprehensiveness: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more comprehensive and detailed list of public figures from a\nwider range of entertainment sectors, including film, television, music, sports, gaming, and digital\nmedia. It also includes specific examples of their contributions and the impact they have on their\nrespective fields, as well as mentions of controversies and their implications. Answer 2, while\ndetailed in its coverage of a few individuals, is limited to a smaller number of public figures and\nfocuses primarily on their personal lives and relationships rather than a broad spectrum of their\nprofessional influence across the entertainment industry.\nDiversity: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more varied and rich response by covering a wide range of\npublic figures from different sectors of the entertainment industry, including film, television, music,",
        "Each level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities → Community Summaries\nThe next step is to create report-like summaries of each community in the Leiden hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful in\ntheir own right as a way to understand the global structure and semantics of the dataset, and may\nthemselves be used to make sense of a corpus in the absence of a question. For example, a user\nmay scan through community summaries at one level looking for general themes of interest, then\nfollow links to the reports at the lower level that provide more details for each of the subtopics. Here,\nhowever, we focus on their utility as part of a graph-based index used for answering global queries.\nCommunity summaries are generated in the following way:\n4",
        "NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\ntion with llm based on knowledge graphs. https://www.nebula-graph.io/posts/graph-RAG.\nNeo4J (2024). Project NaLLM. https://github .com/neo4j/NaLLM.\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\nnational academy of sciences, 103(23):8577–8582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. arXiv preprint arXiv:2310.13848.\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.",
        "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na ¨ıve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An",
        "and traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n2",
        "2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n3 Evaluation\n3.1 Datasets\nWe selected two datasets in the one million token range, each equivalent to about 10 novels of text\nand representative of the kind of corpora that users may encounter in their real world activities:\n• Podcast transcripts. Compiled transcripts of podcast conversations between Kevin Scott,\nMicrosoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669",
        "conciseness and specificity as Answer 2.\nTable 2: Example question for the News article dataset, with generated answers from Graph RAG\n(C2) and Na¨ıve RAG, as well as LLM-generated assessments.\n8",
        "Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294.\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. arXiv preprint arXiv:2005.03975.\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.",
        "sports, gaming, and digital media. It offers insights into the contributions and influence of these\nfigures, as well as controversies and their impact on public discourse. The answer also cites specific\ndata sources for each mentioned figure, indicating a diverse range of evidence to support the claims.\nIn contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry\nand sports, and relies heavily on a single source for data, which makes it less diverse in perspectives\nand insights.\nEmpowerment: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a comprehensive and structured overview of public figures\nacross various sectors of the entertainment industry, including film, television, music, sports, and\ndigital media. It lists multiple individuals, providing specific examples of their contributions and the\ncontext in which they are mentioned in entertainment articles, along with references to data reports",
        "sity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na ¨ıve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na ¨ıve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents → Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-",
        "Podcast transcripts\n50 17 28 25 22 21\n83 50 50 48 43 44\n72 50 50 53 50 49\n75 52 47 50 52 50\n78 57 50 48 50 52\n79 56 51 50 48 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nComprehensiveness\n50 18 23 25 19 19\n82 50 50 50 43 46\n77 50 50 50 46 44\n75 50 50 50 44 45\n81 57 54 56 50 48\n81 54 56 55 52 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 42 57 52 49 51\n58 50 59 55 52 51\n43 41 50 49 47 48\n48 45 51 50 49 50\n51 48 53 51 50 51\n49 49 52 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 56 65 60 60 60\n44 50 55 52 51 52\n35 45 50 47 48 48\n40 48 53 50 50 50\n40 49 52 50 50 50\n40 48 52 50 50 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDirectness\nNews articles\n50 20 28 25 21 21\n80 50 44 41 38 36\n72 56 50 52 54 52\n75 59 48 50 58 55\n79 62 46 42 50 59\n79 64 48 45 41 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nComprehensiveness\n50 33 38 35 29 31\n67 50 53 45 44 40\n62 47 50 40 41 41\n65 55 60 50 50 50\n71 56 59 50 50 51\n69 60 59 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 47 57 49 50 50",
        "of data sensemaking, i.e., the process though which people inspect, engage with, and contextualize\ndata within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for\nextracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such\nextracted questions can target details that betray prior knowledge of the texts.\nTo evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions\nthat convey only a high-level understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify N potential users and N tasks per user,\nthen for each (user, task) combination, we asked the LLM to generate N questions that require\nunderstanding of the entire corpus. For our evaluation, a value ofN = 5 resulted in 125 test questions",
        "i.e., that na¨ıve RAG produces the most direct responses across all comparisons.\n9",
        "4.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-",
        "Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\n9(6): e98679. https://doi.org/10.1371/journal.pone.0098679.\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\non Knowledge and Data Engineering, 35(2):1149–1170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024.\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-",
        "0 1 2 30\n10000\n20000\n30000\nNumber of gleanings performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo.\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-",
        "all of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be “lost in the middle” of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na¨ıve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present aGraph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored",
        "Fortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., and Wang, H. (2023). Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.\nGoodwin, T. R., Savery, M. E., and Demner-Fushman, D. (2020). Flight of the pegasus? comparing\ntransformers on few-shot and zero-shot multi-document abstractive summarization. In Proceed-\nings of COLING. International Conference on Computational Linguistics , volume 2020, page\n5640. NIH Public Access.\nHe, X., Tian, Y ., Sun, Y ., Chawla, N. V ., Laurent, T., LeCun, Y ., Bresson, X., and Hooi, B. (2024).\nG-retriever: Retrieval-augmented generation for textual graph understanding and question an-\nswering. arXiv preprint arXiv:2402.07630.\n12",
        "ing time and query time. The “global answer” to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n“a motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively” (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in"
    ]
}