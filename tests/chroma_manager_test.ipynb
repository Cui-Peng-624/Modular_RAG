{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "from src.vdb_managers.chroma_manager import ChromaManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_store = ChromaManager() # 自定义的chroma类"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"files/论文 - GraphRAG.pdf\"\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
<<<<<<< HEAD
    "auto_extract_metadata = False\n",
=======
    "auto_extract_metadata = True\n",
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
    "metadata = {\n",
    "    'keywords': \"实习\", # 必须是str，好像bool等也行，不记得了，反正不能是list\n",
    "    \"from\": \"上齐实习\"\n",
    "}\n",
    "collection_name = \"RAG\"\n",
    "discription = \"test vdb function\"\n",
<<<<<<< HEAD
    "summarize_chunks = False\n",
    "similarity_metric = \"cosine\"\n",
    "use_raptor = True"
=======
    "summarize_chunks = True\n",
    "similarity_metric = \"cosine\""
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传PDF文档\n",
<<<<<<< HEAD
    "chroma_store.upload_pdf_file(file_path = file_path, collection_name = collection_name)"
=======
    "chroma_store.upload_pdf_file(file_path = file_path, collection_name = collection_name, summarize_chunks=summarize_chunks)"
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"raptor\"\n",
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"RAG\"\n",
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
    "query = \"What are the general methods of deep reinforcement learning?\"\n",
    "k = 3\n",
    "metadata_filter = {'关键字key': {'$in': ['graphrag', 'node', 'edge']}}\n",
    "fuzzy_filter = True\n",
<<<<<<< HEAD
    "use_summary = True # 是否使用summary进行匹配，如果为false则使用原始文本进行匹配"
=======
    "use_summary = True # 是否使用summary进行匹配，还是使用原始文本进行匹配"
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['1029b4ae-e46d-4035-868b-c6e819e6d627', '1f35ece5-4122-49c7-b810-a8fd48c5ee83', '41c97c2e-e4fb-48dd-b23e-1bcf3198eb51']], 'embeddings': None, 'documents': [['Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\\narXiv:2305.15294.\\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\\ninformation management. arXiv preprint arXiv:2005.03975.\\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.', 'Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\\nlarge language models for advanced causal discovery from data.\\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\\nmodels. arXiv preprint arXiv:1801.07704.\\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\\n2008(10):P10008.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in', 'P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877–1901.\\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\\ntems, 36.\\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\\nlarge language models. arXiv preprint arXiv:2310.05149.\\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.']], 'uris': None, 'data': None, 'metadatas': [[{'doc_id': 'dd8ed097-2d31-49c0-80fc-150cf69a0d68', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 13, 'parent_docs': '[\"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\"]', 'source': 'files/论文 - GraphRAG.pdf'}, {'doc_id': '2bbad329-e427-47ec-afec-f924354e275e', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]', 'source': 'files/论文 - GraphRAG.pdf'}, {'doc_id': '932218db-a4c7-4421-b8c8-5dbde8f1a820', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]', 'source': 'files/论文 - GraphRAG.pdf'}]], 'distances': [[0.6972579868369049, 0.7080599158231401, 0.7081557476798865]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = chroma_store.dense_search(collection_name=collection_name, query=query, k=3)\n",
    "# print(type(results[\"metadatas\"]), results[\"metadatas\"])\n",
    "# for temp in results[\"metadatas\"][0]:\n",
    "#     print(temp, \"\\n\\n\")\n",
    "print(results)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma_store.dense_search(collection_name=collection_name, query=query, k=k)"
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_weight = 1"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results), results.keys(), results['metadatas']"
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\\narXiv:2305.15294.\\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\\ninformation management. arXiv preprint arXiv:2005.03975.\\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.', 'metadata': {'doc_id': 'dd8ed097-2d31-49c0-80fc-150cf69a0d68', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 13, 'parent_docs': '[\"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.3028838410534236}, {'content': 'Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\\nlarge language models for advanced causal discovery from data.\\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\\nmodels. arXiv preprint arXiv:1801.07704.\\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\\n2008(10):P10008.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in', 'metadata': {'doc_id': '2bbad329-e427-47ec-afec-f924354e275e', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.2920942284131274}, {'content': 'P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877–1901.\\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\\ntems, 36.\\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\\nlarge language models. arXiv preprint arXiv:2310.05149.\\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.', 'metadata': {'doc_id': '932218db-a4c7-4421-b8c8-5dbde8f1a820', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.29196770345014844}]\n"
     ]
    }
   ],
   "source": [
    "results = chroma_store.hybrid_search(collection_name=collection_name, query=query, k=k, dense_weight=dense_weight)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\\narXiv:2305.15294.\\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\\ninformation management. arXiv preprint arXiv:2005.03975.\\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.', 'metadata': {'doc_id': 'dd8ed097-2d31-49c0-80fc-150cf69a0d68', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 13, 'parent_docs': '[\"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.3027420131630951}, {'content': 'Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\\nlarge language models for advanced causal discovery from data.\\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\\nmodels. arXiv preprint arXiv:1801.07704.\\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\\n2008(10):P10008.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in', 'metadata': {'doc_id': '2bbad329-e427-47ec-afec-f924354e275e', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.2919400841768599}, {'content': 'P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877–1901.\\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\\ntems, 36.\\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\\nlarge language models. arXiv preprint arXiv:2310.05149.\\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.', 'metadata': {'doc_id': '932218db-a4c7-4421-b8c8-5dbde8f1a820', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.29184425232011346}]\n"
     ]
    }
   ],
   "source": [
    "results = chroma_store.search(collection_name=collection_name, query=query, k=3, dense_weight=1)\n",
    "print(results)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_weight = 0.5"
>>>>>>> e503d4c6a721d0e7d96523baacc992414b9bc723
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {'content': 'Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\\narXiv:2305.15294.\\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\\ninformation management. arXiv preprint arXiv:2005.03975.\\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.', 'metadata': {'doc_id': 'dd8ed097-2d31-49c0-80fc-150cf69a0d68', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 13, 'parent_docs': '[\"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.3027420131630951}, \n",
    "    \n",
    "    {'content': 'Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\\nlarge language models for advanced causal discovery from data.\\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\\nmodels. arXiv preprint arXiv:1801.07704.\\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\\n2008(10):P10008.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in', 'metadata': {'doc_id': '2bbad329-e427-47ec-afec-f924354e275e', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.2919400841768599}, \n",
    "    \n",
    "    {'content': 'P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877–1901.\\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\\ntems, 36.\\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\\nlarge language models. arXiv preprint arXiv:2310.05149.\\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.', 'metadata': {'doc_id': '932218db-a4c7-4421-b8c8-5dbde8f1a820', 'file_path': 'files/论文 - GraphRAG.pdf', 'level': 0, 'page': 11, 'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]', 'source': 'files/论文 - GraphRAG.pdf'}, 'score': 0.29184425232011346}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[文档片段 1]\n",
      "Dataset Example activity framing and generation of global sensemaking questions Podcast transcripts User: A tech journalist looking for insights and trends in the tech industry Task: Understanding how tech leaders view the role of policy and regulation Questions: 1. Which episodes deal primarily with tech policy and government regulation? 2. How do guests perceive the impact of privacy laws on technology development? 3. Do any guests discuss the balance between innovation and ethical considerations? 4. What are the suggested changes to current policies mentioned by the guests? 5. Are collaborations between tech companies and governments discussed and how? News articles User: Educator incorporating current affairs into curricula Task: Teaching about health and wellness Questions: 1. What current topics in health can be integrated into health education curricula? 2. How do news articles address the concepts of preventive medicine and wellness?\n",
      "\n",
      "---\n",
      "\n",
      "[文档片段 2]\n",
      "Each level of this hierarchy provides a community partition that covers the nodes of the graph in a mutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization. 2.5 Graph Communities → Community Summaries The next step is to create report-like summaries of each community in the Leiden hierarchy, using a method designed to scale to very large datasets. These summaries are independently useful in their own right as a way to understand the global structure and semantics of the dataset, and may themselves be used to make sense of a corpus in the absence of a question. For example, a user may scan through community summaries at one level looking for general themes of interest, then follow links to the reports at the lower level that provide more details for each of the subtopics. Here, however, we focus on their utility as part of a graph-based index used for answering global queries. Community summaries are generated in the following way: 4\n",
      "\n",
      "---\n",
      "\n",
      "[文档片段 3]\n",
      "means that questions can be answered using the community summaries from different levels, raising the question of whether a particular level in the hierarchical community structure offers the best balance of summary detail and scope for general sensemaking questions (evaluated in section 3). For a given community level, the global answer to any user query is generated as follows: • Prepare community summaries. Community summaries are randomly shuffled and divided into chunks of pre-specified token size. This ensures relevant information is distributed across chunks, rather than concentrated (and potentially lost) in a single context window. • Map community answers. Generate intermediate answers in parallel, one for each chunk. The LLM is also asked to generate a score between 0-100 indicating how helpful the gen- erated answer is in answering the target question. Answers with score 0 are filtered out.\n"
     ]
    }
   ],
   "source": [
    "formatted_context = chroma_store.get_formatted_context(collection_name=collection_name, query=query, k=3, use_summary=False, dense_weight=0.5)\n",
    "print(formatted_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ChromaManager_RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "from src.vdb_managers.chroma_manager import ChromaManager_RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_manager_raptor = ChromaManager_RAPTOR(n_levels=4, min_cluster_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat请求失败 (尝试 1/3): Error code: 400 - {'error': {'message': 'response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later (request id: 20250214160839972645678vH7Fct7h) (request id: 20250214183839921971764qUngjyD2) (request id: 20250214183839903180852oJ8XxiZR) (request id: 202502141838397683591469hTdci08)', 'type': '', 'param': '', 'code': 'BadRequest'}}\n",
      "Chat请求失败 (尝试 1/3): Error code: 400 - {'error': {'message': 'response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later (request id: 2025021416091728309717g878mVx6) (request id: 20250214183916985634764uppGO3iU) (request id: 20250214183916972041224yBwWEBou) (request id: 2025021418391683772832134HQ5e0j)', 'type': '', 'param': '', 'code': 'BadRequest'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 82 <class 'list'> 82 <class 'list'> 82\n"
     ]
    }
   ],
   "source": [
    "chroma_manager_raptor.upload_pdf_file(file_path=\"files/论文 - GraphRAG.pdf\", collection_name=\"raptor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 set()\n",
      "{'level': {'$eq': 2}}\n",
      "here\n",
      "1 {'f624bc55-8484-4349-9683-b43c2cde509f', 'a4caf940-d538-45c0-802c-52bce043633e', 'dca2cef2-44bd-474d-b93d-3250ed668f9b', '3dd4a466-7d84-4aed-97c3-f90cb4b2d12f', '5fdbc47a-e567-431b-9e98-30ad7c79babd', 'f11ec5ed-2f5b-4b42-9777-5fd8b193841e', '34b93aef-865f-4f07-9b0c-0a6fd05763da', '1a67e54f-62e9-437f-921a-8bba6a7ba248'}\n",
      "{'$and': [{'level': {'$eq': 1}}, {'doc_id': {'$in': ['f624bc55-8484-4349-9683-b43c2cde509f', 'a4caf940-d538-45c0-802c-52bce043633e', 'dca2cef2-44bd-474d-b93d-3250ed668f9b', '3dd4a466-7d84-4aed-97c3-f90cb4b2d12f', '5fdbc47a-e567-431b-9e98-30ad7c79babd', 'f11ec5ed-2f5b-4b42-9777-5fd8b193841e', '34b93aef-865f-4f07-9b0c-0a6fd05763da', '1a67e54f-62e9-437f-921a-8bba6a7ba248']}}]}\n",
      "here\n",
      "0 {'57b35425-0b4f-425a-81d7-4191d486e23d', 'ab9659bc-a080-43d8-bde8-fb6ba5142854', '6b31f18d-df16-4efd-8e8e-23343527ed0b', '3538750e-b0f7-4e54-a485-ec89a4012c98', '843c1e6e-34e6-49cf-a79d-a29a0b135e2b', 'db894cc2-3db5-4b18-8735-dfd4109e42cb', 'c090c00e-7321-4711-a82c-0de0ef9c3577', '0f631e3c-35ad-47cb-b56e-cccccc380952', '2c83f0b1-a586-4866-a3b3-d52d4470736d', '5f8d2ce5-8a45-4240-baab-f8c385a8519b', '016d05f4-42b8-4a33-8b83-54b9937c2640', '2d242f37-ca71-42fa-885c-6602eaa23f4b', '2bbad329-e427-47ec-afec-f924354e275e', 'd4381981-7c6f-4b56-b120-8f585e4dbfdf', '0a6d1c5f-9fe7-4cb7-ae6a-c1ade647eda4'}\n",
      "{'$and': [{'level': {'$eq': 0}}, {'doc_id': {'$in': ['57b35425-0b4f-425a-81d7-4191d486e23d', 'ab9659bc-a080-43d8-bde8-fb6ba5142854', '6b31f18d-df16-4efd-8e8e-23343527ed0b', '3538750e-b0f7-4e54-a485-ec89a4012c98', '843c1e6e-34e6-49cf-a79d-a29a0b135e2b', 'db894cc2-3db5-4b18-8735-dfd4109e42cb', 'c090c00e-7321-4711-a82c-0de0ef9c3577', '0f631e3c-35ad-47cb-b56e-cccccc380952', '2c83f0b1-a586-4866-a3b3-d52d4470736d', '5f8d2ce5-8a45-4240-baab-f8c385a8519b', '016d05f4-42b8-4a33-8b83-54b9937c2640', '2d242f37-ca71-42fa-885c-6602eaa23f4b', '2bbad329-e427-47ec-afec-f924354e275e', 'd4381981-7c6f-4b56-b120-8f585e4dbfdf', '0a6d1c5f-9fe7-4cb7-ae6a-c1ade647eda4']}}]}\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"raptor\"\n",
    "query = \"What are the general methods of deep reinforcement learning?\"\n",
    "k = 20\n",
    "metadata_filter = {'level': {'$eq': 1}}\n",
    "results = chroma_manager_raptor.search(collection_name=collection_name, query=query, k=k, search_type = \"traversal\", d = 3, top_k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " 6,\n",
       " {'content': \"The document details a Graph RAG approach that enhances text summarization and query response using large language models (LLMs). It involves partitioning text into nodes in a graph index using algorithms like Leiden, creating community-based summaries that outperform traditional methods in token efficiency and diversity. The study emphasizes LLMs in extracting named entities and relationships across domains, illustrating their effectiveness in generating comprehensive and relevant summaries without predefined answers. This research showcases advancements in knowledge graphs, causal extraction, and narrative generation, providing tools for improved information retrieval and intelligence report production. A multi-step extraction process is introduced, tailored to fields like science and law, using secondary prompts for enhanced data capture. Graph RAG offers a structured, community-oriented method, enabling better data representation and query handling, while upcoming open-source releases promise broader application. Evaluated through metrics like comprehensiveness and empowerment, the approach proves superior in creating summaries, suggesting a significant leap in RAG systems' capabilities.\",\n",
       "  'metadata': {'child_docs': '[\"dca2cef2-44bd-474d-b93d-3250ed668f9b\", \"1a67e54f-62e9-437f-921a-8bba6a7ba248\", \"5fdbc47a-e567-431b-9e98-30ad7c79babd\", \"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\", \"f624bc55-8484-4349-9683-b43c2cde509f\"]',\n",
       "   'doc_id': 'c7be32b1-5379-408f-9050-646291811a47',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 2,\n",
       "   'parent_docs': '[]'},\n",
       "  'score': 0.21310550567904452},\n",
       " '######################################################################',\n",
       " {'content': '这项研究分析了如何利用检索增强生成（RAG）方法以及图形索引技术改进大文本集的摘要能力，尤其在识别全局性问题的主题方面。通过引入图索引和优化上下文窗口的方法，研究能够更好地提取实体、关系和主张，并总结为更广泛的主题。社区检测算法的应用则提高了分析的效率。在实验中证实了这些方法在生成简捷、明确答案方面的有效性，特别是在高层次问题的解答上表现优越。研究还探讨了方法间在特定人物信息比较时的竞争力，注重计算资源与索引益处间的平衡。未来发展的方向包括基于更本地化的嵌入匹配查询和混合RAG方案以实现更细致的总结。',\n",
       "  'metadata': {'child_docs': '[\"34b93aef-865f-4f07-9b0c-0a6fd05763da\", \"a4caf940-d538-45c0-802c-52bce043633e\", \"f11ec5ed-2f5b-4b42-9777-5fd8b193841e\"]',\n",
       "   'doc_id': 'dc5eddc4-fabc-444c-881f-d602e937a8a9',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 2,\n",
       "   'parent_docs': '[]'},\n",
       "  'score': 0.18069720326069705},\n",
       " '######################################################################',\n",
       " {'content': 'This text discusses advanced methods for extracting named entities from various domains using language models (LLMs). It highlights the importance of customizing extraction techniques for domains like science or law and introduces a secondary prompt to extract additional related data. The text outlines a multi-step extraction process that ensures comprehensive coverage of entities and indicates a structured community approach for answering queries. The document compares several retrieval-augmented generation (RAG) methods across different community levels (C0-C3) and text summarization techniques, emphasizing improvements in comprehensiveness and diversity. This research is situated within the context of enhancing knowledge graphs and leveraging large language models for open-domain question answering and intelligence report generation.',\n",
       "  'metadata': {'child_docs': '[\"2c83f0b1-a586-4866-a3b3-d52d4470736d\", \"ab9659bc-a080-43d8-bde8-fb6ba5142854\", \"c090c00e-7321-4711-a82c-0de0ef9c3577\", \"5f8d2ce5-8a45-4240-baab-f8c385a8519b\", \"0f631e3c-35ad-47cb-b56e-cccccc380952\", \"6b31f18d-df16-4efd-8e8e-23343527ed0b\"]',\n",
       "   'doc_id': '5fdbc47a-e567-431b-9e98-30ad7c79babd',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 1,\n",
       "   'parent_docs': '[\"c7be32b1-5379-408f-9050-646291811a47\"]'},\n",
       "  'score': 0.28486355545007536})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results), len(results), results[0], \"######################################################################\", results[1], \"######################################################################\", results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"The document details a Graph RAG approach that enhances text summarization and query response using large language models (LLMs). It involves partitioning text into nodes in a graph index using algorithms like Leiden, creating community-based summaries that outperform traditional methods in token efficiency and diversity. The study emphasizes LLMs in extracting named entities and relationships across domains, illustrating their effectiveness in generating comprehensive and relevant summaries without predefined answers. This research showcases advancements in knowledge graphs, causal extraction, and narrative generation, providing tools for improved information retrieval and intelligence report production. A multi-step extraction process is introduced, tailored to fields like science and law, using secondary prompts for enhanced data capture. Graph RAG offers a structured, community-oriented method, enabling better data representation and query handling, while upcoming open-source releases promise broader application. Evaluated through metrics like comprehensiveness and empowerment, the approach proves superior in creating summaries, suggesting a significant leap in RAG systems' capabilities.\",\n",
       "  'metadata': {'child_docs': '[\"dca2cef2-44bd-474d-b93d-3250ed668f9b\", \"1a67e54f-62e9-437f-921a-8bba6a7ba248\", \"5fdbc47a-e567-431b-9e98-30ad7c79babd\", \"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\", \"f624bc55-8484-4349-9683-b43c2cde509f\"]',\n",
       "   'doc_id': 'c7be32b1-5379-408f-9050-646291811a47',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 2,\n",
       "   'parent_docs': '[]'},\n",
       "  'score': 0.21310550567904452},\n",
       " {'content': '这项研究分析了如何利用检索增强生成（RAG）方法以及图形索引技术改进大文本集的摘要能力，尤其在识别全局性问题的主题方面。通过引入图索引和优化上下文窗口的方法，研究能够更好地提取实体、关系和主张，并总结为更广泛的主题。社区检测算法的应用则提高了分析的效率。在实验中证实了这些方法在生成简捷、明确答案方面的有效性，特别是在高层次问题的解答上表现优越。研究还探讨了方法间在特定人物信息比较时的竞争力，注重计算资源与索引益处间的平衡。未来发展的方向包括基于更本地化的嵌入匹配查询和混合RAG方案以实现更细致的总结。',\n",
       "  'metadata': {'child_docs': '[\"34b93aef-865f-4f07-9b0c-0a6fd05763da\", \"a4caf940-d538-45c0-802c-52bce043633e\", \"f11ec5ed-2f5b-4b42-9777-5fd8b193841e\"]',\n",
       "   'doc_id': 'dc5eddc4-fabc-444c-881f-d602e937a8a9',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 2,\n",
       "   'parent_docs': '[]'},\n",
       "  'score': 0.18069720326069705},\n",
       " {'content': 'This text discusses advanced methods for extracting named entities from various domains using language models (LLMs). It highlights the importance of customizing extraction techniques for domains like science or law and introduces a secondary prompt to extract additional related data. The text outlines a multi-step extraction process that ensures comprehensive coverage of entities and indicates a structured community approach for answering queries. The document compares several retrieval-augmented generation (RAG) methods across different community levels (C0-C3) and text summarization techniques, emphasizing improvements in comprehensiveness and diversity. This research is situated within the context of enhancing knowledge graphs and leveraging large language models for open-domain question answering and intelligence report generation.',\n",
       "  'metadata': {'child_docs': '[\"2c83f0b1-a586-4866-a3b3-d52d4470736d\", \"ab9659bc-a080-43d8-bde8-fb6ba5142854\", \"c090c00e-7321-4711-a82c-0de0ef9c3577\", \"5f8d2ce5-8a45-4240-baab-f8c385a8519b\", \"0f631e3c-35ad-47cb-b56e-cccccc380952\", \"6b31f18d-df16-4efd-8e8e-23343527ed0b\"]',\n",
       "   'doc_id': '5fdbc47a-e567-431b-9e98-30ad7c79babd',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 1,\n",
       "   'parent_docs': '[\"c7be32b1-5379-408f-9050-646291811a47\"]'},\n",
       "  'score': 0.28486355545007536},\n",
       " {'content': 'This text discusses the role of large language models (LLMs) in processing and evaluating textual data, particularly in creating graph representations of text and evaluating natural language generation. It highlights how LLMs can identify entities and relationships within text, allowing for their graphical modeling. The deployment of LLMs in different domains, like podcasts and news, and their capacity to produce metrics for fluency and relevance without gold standard answers, underlines their effectiveness as evaluators. The text also references the Graph RAG system, combining various retrieval and augmentation methods, showcasing the potential of LLMs in enhancing information retrieval and generation.',\n",
       "  'metadata': {'child_docs': '[\"0a6d1c5f-9fe7-4cb7-ae6a-c1ade647eda4\", \"d4381981-7c6f-4b56-b120-8f585e4dbfdf\", \"843c1e6e-34e6-49cf-a79d-a29a0b135e2b\", \"db894cc2-3db5-4b18-8735-dfd4109e42cb\", \"3538750e-b0f7-4e54-a485-ec89a4012c98\", \"2bbad329-e427-47ec-afec-f924354e275e\", \"57b35425-0b4f-425a-81d7-4191d486e23d\", \"2d242f37-ca71-42fa-885c-6602eaa23f4b\", \"016d05f4-42b8-4a33-8b83-54b9937c2640\"]',\n",
       "   'doc_id': '1a67e54f-62e9-437f-921a-8bba6a7ba248',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 1,\n",
       "   'parent_docs': '[\"c7be32b1-5379-408f-9050-646291811a47\"]'},\n",
       "  'score': 0.22233540760948567},\n",
       " {'content': 'Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\\nlarge language models for advanced causal discovery from data.\\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\\nmodels. arXiv preprint arXiv:1801.07704.\\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\\n2008(10):P10008.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in',\n",
       "  'metadata': {'doc_id': '2bbad329-e427-47ec-afec-f924354e275e',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 11,\n",
       "   'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.2919400841768599},\n",
       " {'content': 'represent similar semantics. Queries are then embedded into the same vector space, with the text\\nchunks of the nearest k vectors used as context. More advanced variations exist, but all solve the\\nproblem of what to do when an external dataset of interest exceeds the LLM’s context window.\\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\\ncome the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and\\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)',\n",
       "  'metadata': {'doc_id': '3538750e-b0f7-4e54-a485-ec89a4012c98',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 9,\n",
       "   'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.2908570553694486}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\\narXiv:2305.15294.\\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\\ninformation management. arXiv preprint arXiv:2005.03975.\\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.',\n",
       "  'metadata': {'doc_id': 'dd8ed097-2d31-49c0-80fc-150cf69a0d68',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 13,\n",
       "   'parent_docs': '[\"3dd4a466-7d84-4aed-97c3-f90cb4b2d12f\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.3027420131630951},\n",
       " {'content': 'Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\\nlarge language models for advanced causal discovery from data.\\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\\nmodels. arXiv preprint arXiv:1801.07704.\\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\\n2008(10):P10008.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in',\n",
       "  'metadata': {'doc_id': '2bbad329-e427-47ec-afec-f924354e275e',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 11,\n",
       "   'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.2919400841768599},\n",
       " {'content': 'P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877–1901.\\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\\ntems, 36.\\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\\nlarge language models. arXiv preprint arXiv:2310.05149.\\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.',\n",
       "  'metadata': {'doc_id': '932218db-a4c7-4421-b8c8-5dbde8f1a820',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 11,\n",
       "   'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.29184425232011346},\n",
       " {'content': 'represent similar semantics. Queries are then embedded into the same vector space, with the text\\nchunks of the nearest k vectors used as context. More advanced variations exist, but all solve the\\nproblem of what to do when an external dataset of interest exceeds the LLM’s context window.\\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\\ncome the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and\\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)',\n",
       "  'metadata': {'doc_id': '3538750e-b0f7-4e54-a485-ec89a4012c98',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 9,\n",
       "   'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.2908570553694486},\n",
       " {'content': 'Laskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\\nincorporating query relevance and transfer learning with transformer models. In Advances in\\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\\nOttawa, ON, Canada, May 13–15, 2020, Proceedings 33, pages 342–348. Springer.\\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\\ners for query-focused abstractive text summarization. Computational Linguistics, 48(2):279–320.\\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K ¨uttler, H., Lewis, M., Yih,\\nW.-t., Rockt¨aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. Advances in Neural Information Processing Systems, 33:9459–9474.\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost',\n",
       "  'metadata': {'doc_id': '31fdaa45-8728-4361-b65d-98258341693d',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 12,\n",
       "   'parent_docs': '[\"b238ea20-2aab-42df-a0de-68bb7d1deef8\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.2866824566021088},\n",
       " {'content': 'This text discusses advanced methods for extracting named entities from various domains using language models (LLMs). It highlights the importance of customizing extraction techniques for domains like science or law and introduces a secondary prompt to extract additional related data. The text outlines a multi-step extraction process that ensures comprehensive coverage of entities and indicates a structured community approach for answering queries. The document compares several retrieval-augmented generation (RAG) methods across different community levels (C0-C3) and text summarization techniques, emphasizing improvements in comprehensiveness and diversity. This research is situated within the context of enhancing knowledge graphs and leveraging large language models for open-domain question answering and intelligence report generation.',\n",
       "  'metadata': {'child_docs': '[\"2c83f0b1-a586-4866-a3b3-d52d4470736d\", \"ab9659bc-a080-43d8-bde8-fb6ba5142854\", \"c090c00e-7321-4711-a82c-0de0ef9c3577\", \"5f8d2ce5-8a45-4240-baab-f8c385a8519b\", \"0f631e3c-35ad-47cb-b56e-cccccc380952\", \"6b31f18d-df16-4efd-8e8e-23343527ed0b\"]',\n",
       "   'doc_id': '5fdbc47a-e567-431b-9e98-30ad7c79babd',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 1,\n",
       "   'parent_docs': '[\"c7be32b1-5379-408f-9050-646291811a47\"]'},\n",
       "  'score': 0.28486355545007536},\n",
       " {'content': 'Instead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\\nparticular, query-focused abstractive summarization that generates natural language summaries and\\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\\nvant. While early applications of the transformer architecture showed substantial improvements on\\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\\nall of which can use in-context learning to summarize any content provided in their context window.',\n",
       "  'metadata': {'doc_id': '985a29a7-7f80-4c98-af18-470c6f962702',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 1,\n",
       "   'parent_docs': '[\"b238ea20-2aab-42df-a0de-68bb7d1deef8\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.27641651361441366},\n",
       " {'content': 'in the middle: How language models use long contexts. arXiv:2307.03172.\\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization.arXiv\\npreprint arXiv:1905.13164.\\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex.ai/en/stable/\\nexamples/index structs/knowledge graph/KnowledgeGraphDemo.html.\\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896.\\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.\\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA).\\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study',\n",
       "  'metadata': {'doc_id': '0f631e3c-35ad-47cb-b56e-cccccc380952',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 12,\n",
       "   'parent_docs': '[\"5fdbc47a-e567-431b-9e98-30ad7c79babd\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.27373587917743203},\n",
       " {'content': 'ous questions with retrieval-augmented large language models.arXiv preprint arXiv:2310.14696.\\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\\nperspectives. IEEE intelligent systems, 21(4):70–73.\\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\\ntive model. IEEE Intelligent systems, 21(5):88–92.\\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets–understanding data\\nsensemaking behaviours. International journal of human-computer studies, 146:102562.\\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\\nLangChain (2024). Langchain graphs. https://python .langchain.com/docs/use cases/graph/.\\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via',\n",
       "  'metadata': {'doc_id': 'e207ef48-ae3a-4d39-8ec8-fecb340bdcad',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 12,\n",
       "   'parent_docs': '[\"3478ef91-5299-4ceb-8b8e-7eb460db2ad8\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.260678258561699},\n",
       " {'content': 'Yao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\\ngraph completion.\\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\\naugmented by chatgpt. arXiv preprint arXiv:2304.11116.\\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\\naugmented generation based large language models. arXiv preprint arXiv:2402.15301.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\\nInformation Processing Systems, 36.\\n15',\n",
       "  'metadata': {'doc_id': '92716123-d272-4843-a858-24ed3d15d4ad',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 14,\n",
       "   'parent_docs': '[\"3478ef91-5299-4ceb-8b8e-7eb460db2ad8\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.2588252128050281},\n",
       " {'content': 'NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\\ntion with llm based on knowledge graphs. https://www.nebula-graph.io/posts/graph-RAG.\\nNeo4J (2024). Project NaLLM. https://github .com/neo4j/NaLLM.\\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\\nnational academy of sciences, 103(23):8577–8582.\\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\\nComputational Linguistics, 11:1316–1331.\\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\\nnarrative construction. arXiv preprint arXiv:2310.13848.\\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.',\n",
       "  'metadata': {'doc_id': '6b31f18d-df16-4efd-8e8e-23343527ed0b',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 13,\n",
       "   'parent_docs': '[\"5fdbc47a-e567-431b-9e98-30ad7c79babd\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.24899074232177165},\n",
       " {'content': 'Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\\n9(6): e98679. https://doi.org/10.1371/journal.pone.0098679.\\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\\non Knowledge and Data Engineering, 35(2):1149–1170.\\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.\\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\\nnlp. arXiv preprint arXiv:2212.14024.\\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-',\n",
       "  'metadata': {'doc_id': '08546fd7-6558-492c-aa69-45e03fb7e1c5',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 12,\n",
       "   'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.24564868443042664},\n",
       " {'content': 'of community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a “tree of clarifications” to answer mul-\\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\\n10',\n",
       "  'metadata': {'doc_id': '059018ad-c8d8-4810-b905-c7f2a3dbd14b',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 9,\n",
       "   'parent_docs': '[\"2b49954f-d0cf-4e33-a57e-d6c7cb316e69\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.24469510834609598},\n",
       " {'content': 'Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288.\\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\\nwell-connected communities. Scientific Reports, 9(1).\\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\\nusing large language models. ArXiv, abs/2305.04676.\\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\\narXiv:2212.10509.\\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.\\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.',\n",
       "  'metadata': {'doc_id': '2d242f37-ca71-42fa-885c-6602eaa23f4b',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 13,\n",
       "   'parent_docs': '[\"1a67e54f-62e9-437f-921a-8bba6a7ba248\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.24148563464702133},\n",
       " {'content': 'all of which can use in-context learning to summarize any content provided in their context window.\\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\\nsuch windows may not be enough given that information can be “lost in the middle” of longer\\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\\nchunks in na¨ıve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\\npre-indexing could support a new RAG approach specifically targeting global summarization.\\nIn this paper, we present aGraph RAG approach based on global summarization of an LLM-derived\\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored',\n",
       "  'metadata': {'doc_id': 'a2d2d929-96cb-4c80-ac70-792fa90f11dd',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 1,\n",
       "   'parent_docs': '[\"3478ef91-5299-4ceb-8b8e-7eb460db2ad8\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.23729436382456315},\n",
       " {'content': '该文本主要讨论了一个用于回答全数据集用户问题的方法，称为\"检索增强生成\"（RAG），与\"基于查询的摘要\"（QFS）更为适配。RAG专注于本地文本区域的检索，而QFS需要在检索后进行与查询相关的全局摘要。在整个语料库上进行人工主导的\"解构\"需要一种方式供用户在数据中应用和调整他们的心智模型。\\n此方法中引入了大语言模型（LLM）的全局、基于查询的摘要能力，该过程的关键是防止同一实体以不同格式出现重复。\\n通过使用富描述性文本建立一个尽可能均匀的向量图结构，与典型知识图谱不同，该方法能够在噪声较大的图结构中提炼出可用的信息。该文本还涉及不同上下文窗口大小对某些任务的影响，表明较小的上下文窗口更有利于综合性的比较，而较大的窗口在多样性和全面性上表现相近。 \\n此外，文本还涉及如何将当前事物的健康和技术领域内容整合到教育课程中，如何利用更新的政策和法规推动科技发展，以及语境窗口大小对使用时效和信息获取全面性的影响，强调了8k窗口在全面性上的优势。',\n",
       "  'metadata': {'child_docs': '[\"8d13be0e-06b6-4d37-9293-ed1a509e6ad5\", \"2f49c607-6c97-47c4-b8f6-70543d247ed1\", \"4f0ffd9a-93ae-4657-89d5-eae896b570c7\", \"57cc8cb4-f619-44a1-bbaa-3eaae5346657\", \"932218db-a4c7-4421-b8c8-5dbde8f1a820\", \"08546fd7-6558-492c-aa69-45e03fb7e1c5\", \"0ba72870-6f76-4c96-9a0a-d8a33395b88e\"]',\n",
       "   'doc_id': '0cf3b74d-66d7-48c2-9578-2fd7445a1ee0',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 1,\n",
       "   'parent_docs': '[\"980f5a0a-0664-4b2a-8b7a-e0d7eb1c7ec7\"]'},\n",
       "  'score': 0.23728099638831823},\n",
       " {'content': '这篇文章探讨了使用GPT-4-Turbo的一种新型实体提取方法，通过在HotPotQA数据集上的实验，研究实体引用数量随块大小和提取次数的变化。文本分析主要基于社区描述的完整覆盖，采用图和Map-Reduce方法实现全局查询总结。此方法被用于评估两个包含播客和新闻文章的数据集，通过生成多样化的问题以测试其效果，并使用LLM生成潜在用户任务的例子。总结中还包括对娱乐业中重复出现的公众人物的总结。\\n\\n文章强调使用层次化索引和Summarization方法，如Graph RAG等，结合提取-生成策略来应对多文档摘要和多跳问题回答。总的来说，文本提到的方法旨在通过生成全面的回答，改善对大型数据集的理解和总结能力。',\n",
       "  'metadata': {'child_docs': '[\"95947bb2-4f04-40e3-a0b8-fc771538310c\", \"9e457335-bb88-474b-a1bb-762a8f66ec1c\", \"0e5528e0-b425-49af-9297-2d7fc4fba736\", \"8006d00c-b722-4c86-8763-45397b3697c6\", \"cda101c1-f82a-4bc7-9146-d7a65226990a\", \"059018ad-c8d8-4810-b905-c7f2a3dbd14b\"]',\n",
       "   'doc_id': '2b49954f-d0cf-4e33-a57e-d6c7cb316e69',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 1,\n",
       "   'parent_docs': '[\"980f5a0a-0664-4b2a-8b7a-e0d7eb1c7ec7\"]'},\n",
       "  'score': 0.23114069205123355},\n",
       " {'content': 'For example, while our default prompt extracting the broad class of “named entities” like people,\\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\\na secondary extraction prompt for any additional covariates we would like to associate with the\\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\\nentities, including the subject, object, type, description, source text span, and start and end dates.\\nTo balance the needs of efficiency and quality, we use multiple rounds of “gleanings”, up to a\\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\\nwhether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM',\n",
       "  'metadata': {'doc_id': '2c83f0b1-a586-4866-a3b3-d52d4470736d',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 3,\n",
       "   'parent_docs': '[\"5fdbc47a-e567-431b-9e98-30ad7c79babd\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.23041542102304113},\n",
       " {'content': '3.5 Configuration\\nThe effect of context window size on any particular task is unclear, especially for models like\\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\\nbe “lost in the middle” of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\\nplore the effects of varying the context window size for our combinations of datasets, questions, and\\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse',\n",
       "  'metadata': {'doc_id': '57cc8cb4-f619-44a1-bbaa-3eaae5346657',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 8,\n",
       "   'parent_docs': '[\"0cf3b74d-66d7-48c2-9578-2fd7445a1ee0\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.22965103635418904},\n",
       " {'content': '4.2 Graphs and LLMs\\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-',\n",
       "  'metadata': {'doc_id': '750c9d2f-673f-4092-9ac1-d8044ee65c1c',\n",
       "   'file_path': 'files/论文 - GraphRAG.pdf',\n",
       "   'level': 0,\n",
       "   'page': 10,\n",
       "   'parent_docs': '[\"dca2cef2-44bd-474d-b93d-3250ed668f9b\"]',\n",
       "   'source': 'files/论文 - GraphRAG.pdf'},\n",
       "  'score': 0.22778363986734762}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = chroma_manager_raptor.search(collection_name=collection_name, query=query, k=k, search_type = \"collapsed\", d = 3, top_k = 2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 set()\n",
      "{'level': {'$eq': 2}}\n",
      "here\n",
      "1 {'f624bc55-8484-4349-9683-b43c2cde509f', 'a4caf940-d538-45c0-802c-52bce043633e', 'dca2cef2-44bd-474d-b93d-3250ed668f9b', '3dd4a466-7d84-4aed-97c3-f90cb4b2d12f', '5fdbc47a-e567-431b-9e98-30ad7c79babd', 'f11ec5ed-2f5b-4b42-9777-5fd8b193841e', '34b93aef-865f-4f07-9b0c-0a6fd05763da', '1a67e54f-62e9-437f-921a-8bba6a7ba248'}\n",
      "{'$and': [{'level': {'$eq': 1}}, {'doc_id': {'$in': ['f624bc55-8484-4349-9683-b43c2cde509f', 'a4caf940-d538-45c0-802c-52bce043633e', 'dca2cef2-44bd-474d-b93d-3250ed668f9b', '3dd4a466-7d84-4aed-97c3-f90cb4b2d12f', '5fdbc47a-e567-431b-9e98-30ad7c79babd', 'f11ec5ed-2f5b-4b42-9777-5fd8b193841e', '34b93aef-865f-4f07-9b0c-0a6fd05763da', '1a67e54f-62e9-437f-921a-8bba6a7ba248']}}]}\n",
      "here\n",
      "0 {'57b35425-0b4f-425a-81d7-4191d486e23d', 'ab9659bc-a080-43d8-bde8-fb6ba5142854', '6b31f18d-df16-4efd-8e8e-23343527ed0b', '3538750e-b0f7-4e54-a485-ec89a4012c98', '843c1e6e-34e6-49cf-a79d-a29a0b135e2b', 'db894cc2-3db5-4b18-8735-dfd4109e42cb', 'c090c00e-7321-4711-a82c-0de0ef9c3577', '0f631e3c-35ad-47cb-b56e-cccccc380952', '2c83f0b1-a586-4866-a3b3-d52d4470736d', '5f8d2ce5-8a45-4240-baab-f8c385a8519b', '016d05f4-42b8-4a33-8b83-54b9937c2640', '2d242f37-ca71-42fa-885c-6602eaa23f4b', '2bbad329-e427-47ec-afec-f924354e275e', 'd4381981-7c6f-4b56-b120-8f585e4dbfdf', '0a6d1c5f-9fe7-4cb7-ae6a-c1ade647eda4'}\n",
      "{'$and': [{'level': {'$eq': 0}}, {'doc_id': {'$in': ['57b35425-0b4f-425a-81d7-4191d486e23d', 'ab9659bc-a080-43d8-bde8-fb6ba5142854', '6b31f18d-df16-4efd-8e8e-23343527ed0b', '3538750e-b0f7-4e54-a485-ec89a4012c98', '843c1e6e-34e6-49cf-a79d-a29a0b135e2b', 'db894cc2-3db5-4b18-8735-dfd4109e42cb', 'c090c00e-7321-4711-a82c-0de0ef9c3577', '0f631e3c-35ad-47cb-b56e-cccccc380952', '2c83f0b1-a586-4866-a3b3-d52d4470736d', '5f8d2ce5-8a45-4240-baab-f8c385a8519b', '016d05f4-42b8-4a33-8b83-54b9937c2640', '2d242f37-ca71-42fa-885c-6602eaa23f4b', '2bbad329-e427-47ec-afec-f924354e275e', 'd4381981-7c6f-4b56-b120-8f585e4dbfdf', '0a6d1c5f-9fe7-4cb7-ae6a-c1ade647eda4']}}]}\n",
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[级别为2的文档片段 1]\\nThe document details a Graph RAG approach that enhances text summarization and query response using large language models (LLMs). It involves partitioning text into nodes in a graph index using algorithms like Leiden, creating community-based summaries that outperform traditional methods in token efficiency and diversity. The study emphasizes LLMs in extracting named entities and relationships across domains, illustrating their effectiveness in generating comprehensive and relevant summaries without predefined answers. This research showcases advancements in knowledge graphs, causal extraction, and narrative generation, providing tools for improved information retrieval and intelligence report production. A multi-step extraction process is introduced, tailored to fields like science and law, using secondary prompts for enhanced data capture. Graph RAG offers a structured, community-oriented method, enabling better data representation and query handling, while upcoming open-source releases promise broader application. Evaluated through metrics like comprehensiveness and empowerment, the approach proves superior in creating summaries, suggesting a significant leap in RAG systems' capabilities.\\n\\n---\\n\\n[级别为2的文档片段 2]\\n这项研究分析了如何利用检索增强生成（RAG）方法以及图形索引技术改进大文本集的摘要能力，尤其在识别全局性问题的主题方面。通过引入图索引和优化上下文窗口的方法，研究能够更好地提取实体、关系和主张，并总结为更广泛的主题。社区检测算法的应用则提高了分析的效率。在实验中证实了这些方法在生成简捷、明确答案方面的有效性，特别是在高层次问题的解答上表现优越。研究还探讨了方法间在特定人物信息比较时的竞争力，注重计算资源与索引益处间的平衡。未来发展的方向包括基于更本地化的嵌入匹配查询和混合RAG方案以实现更细致的总结。\\n\\n---\\n\\n[级别为1的文档片段 3]\\nThis text discusses advanced methods for extracting named entities from various domains using language models (LLMs). It highlights the importance of customizing extraction techniques for domains like science or law and introduces a secondary prompt to extract additional related data. The text outlines a multi-step extraction process that ensures comprehensive coverage of entities and indicates a structured community approach for answering queries. The document compares several retrieval-augmented generation (RAG) methods across different community levels (C0-C3) and text summarization techniques, emphasizing improvements in comprehensiveness and diversity. This research is situated within the context of enhancing knowledge graphs and leveraging large language models for open-domain question answering and intelligence report generation.\\n\\n---\\n\\n[级别为1的文档片段 4]\\nThis text discusses the role of large language models (LLMs) in processing and evaluating textual data, particularly in creating graph representations of text and evaluating natural language generation. It highlights how LLMs can identify entities and relationships within text, allowing for their graphical modeling. The deployment of LLMs in different domains, like podcasts and news, and their capacity to produce metrics for fluency and relevance without gold standard answers, underlines their effectiveness as evaluators. The text also references the Graph RAG system, combining various retrieval and augmentation methods, showcasing the potential of LLMs in enhancing information retrieval and generation.\\n\\n---\\n\\n[级别为0的文档片段 5]\\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136. Ban, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. Baumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo- rating query relevance, multi-document coverage, and summary length constraints into seq2seq models. arXiv preprint arXiv:1801.07704. Blondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment , 2008(10):P10008. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\n\\n---\\n\\n[级别为0的文档片段 6]\\nrepresent similar semantics. Queries are then embedded into the same vector space, with the text chunks of the nearest k vectors used as context. More advanced variations exist, but all solve the problem of what to do when an external dataset of interest exceeds the LLM’s context window. Advanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over- come the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and dynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of Graph RAG incorporates multiple concepts related to other systems. For example, our community summaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re- trieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation of community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_context = chroma_manager_raptor.get_formatted_context(collection_name=collection_name, query=query, k=k, search_type = \"traversal\", d = 3, top_k = 2)\n",
    "formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"[级别为2的文档片段 1]\\nThe document details a Graph RAG approach that enhances text summarization and query response using large language models (LLMs). It involves partitioning text into nodes in a graph index using algorithms like Leiden, creating community-based summaries that outperform traditional methods in token efficiency and diversity. The study emphasizes LLMs in extracting named entities and relationships across domains, illustrating their effectiveness in generating comprehensive and relevant summaries without predefined answers. This research showcases advancements in knowledge graphs, causal extraction, and narrative generation, providing tools for improved information retrieval and intelligence report production. A multi-step extraction process is introduced, tailored to fields like science and law, using secondary prompts for enhanced data capture. Graph RAG offers a structured, community-oriented method, enabling better data representation and query handling, while upcoming open-source releases promise broader application. Evaluated through metrics like comprehensiveness and empowerment, the approach proves superior in creating summaries, suggesting a significant leap in RAG systems' capabilities.\\n\\n---\\n\\n[级别为2的文档片段 2]\\n这项研究分析了如何利用检索增强生成（RAG）方法以及图形索引技术改进大文本集的摘要能力，尤其在识别全局性问题的主题方面。通过引入图索引和优化上下文窗口的方法，研究能够更好地提取实体、关系和主张，并总结为更广泛的主题。社区检测算法的应用则提高了分析的效率。在实验中证实了这些方法在生成简捷、明确答案方面的有效性，特别是在高层次问题的解答上表现优越。研究还探讨了方法间在特定人物信息比较时的竞争力，注重计算资源与索引益处间的平衡。未来发展的方向包括基于更本地化的嵌入匹配查询和混合RAG方案以实现更细致的总结。\\n\\n---\\n\\n[级别为1的文档片段 3]\\nThis text discusses advanced methods for extracting named entities from various domains using language models (LLMs). It highlights the importance of customizing extraction techniques for domains like science or law and introduces a secondary prompt to extract additional related data. The text outlines a multi-step extraction process that ensures comprehensive coverage of entities and indicates a structured community approach for answering queries. The document compares several retrieval-augmented generation (RAG) methods across different community levels (C0-C3) and text summarization techniques, emphasizing improvements in comprehensiveness and diversity. This research is situated within the context of enhancing knowledge graphs and leveraging large language models for open-domain question answering and intelligence report generation.\\n\\n---\\n\\n[级别为1的文档片段 4]\\nThis text discusses the role of large language models (LLMs) in processing and evaluating textual data, particularly in creating graph representations of text and evaluating natural language generation. It highlights how LLMs can identify entities and relationships within text, allowing for their graphical modeling. The deployment of LLMs in different domains, like podcasts and news, and their capacity to produce metrics for fluency and relevance without gold standard answers, underlines their effectiveness as evaluators. The text also references the Graph RAG system, combining various retrieval and augmentation methods, showcasing the potential of LLMs in enhancing information retrieval and generation.\\n\\n---\\n\\n[级别为0的文档片段 5]\\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136. Ban, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. Baumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo- rating query relevance, multi-document coverage, and summary length constraints into seq2seq models. arXiv preprint arXiv:1801.07704. Blondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment , 2008(10):P10008. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\\n\\n---\\n\\n[级别为0的文档片段 6]\\nrepresent similar semantics. Queries are then embedded into the same vector space, with the text chunks of the nearest k vectors used as context. More advanced variations exist, but all solve the problem of what to do when an external dataset of interest exceeds the LLM’s context window. Advanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over- come the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and dynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of Graph RAG incorporates multiple concepts related to other systems. For example, our community summaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re- trieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation of community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('ZETATECHS_API_KEY')\n",
    "base_url = os.getenv('ZETATECHS_API_BASE')\n",
    "\n",
    "import chromadb # type: ignore\n",
    "import chromadb.utils.embedding_functions as embedding_functions # type: ignore\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"ChromaVDB\")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=api_key,\n",
    "    api_base=base_url,\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"test\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(collection.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.get_or_create_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(collection.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.create_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
