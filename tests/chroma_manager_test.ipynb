{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "from src.vdb_managers.chroma_manager import ChromaManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_store = ChromaManager() # 自定义的chroma类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"files/论文 - GraphRAG.pdf\"\n",
    "# file_path = \"files/上奇实习报告.pdf\"\n",
    "auto_extract_metadata = True\n",
    "metadata = {\n",
    "    'keywords': \"实习\", # 必须是str，好像bool等也行，不记得了，反正不能是list\n",
    "    \"from\": \"上齐实习\"\n",
    "}\n",
    "collection_name = \"RAG\"\n",
    "similarity_metric = \"cosine\"\n",
    "dense_weight = 0.5\n",
    "metadata_filter = {'关键字key': {'$in': ['graphrag', 'node', 'edge']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传PDF文档\n",
    "chroma_store.upload_pdf_file(file_path = file_path, collection_name = collection_name)\n",
    "# chroma_store.upload_pdf_file(file_path = file_path, metadata=metadata, collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the general methods of deep reinforcement learning?\"\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模糊元数据过滤后的过滤器： {'keywords': {'$in': ['Graph RAG', 'graph', 'graph nodes', 'graph indexes']}} \n",
      "\n",
      "{'ids': [['d4f37542-1406-4f94-ae39-b545ef0860ca', '5774ee75-5127-4928-9935-ea58aa07e305', 'c94159fd-4a56-459e-9eb5-089ded055c66']], 'embeddings': None, 'documents': [['NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\\ntion with llm based on knowledge graphs. https://www .nebula-graph .io/posts/graph-RAG.\\nNeo4J (2024). Project NaLLM. https://github .com/neo4j/NaLLM.\\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\\nnational academy of sciences , 103(23):8577–8582.\\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\\nComputational Linguistics , 11:1316–1331.\\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\\nnarrative construction. arXiv preprint arXiv:2310.13848 .\\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 .', '414550484847\\n454752504949\\n454852515049\\n464853515150SS\\nTS\\nC0\\nC1\\nC2\\nC3SSTSC0C1C2C3\\nDirectness\\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na ¨ıve RAG\\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\\n3.5 Configuration\\nThe effect of context window size on any particular task is unclear, especially for models like\\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\\nbe “lost in the middle” of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-', 'QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\\nRAG systems. To combine the strengths of these contrasting methods, we propose\\na Graph RAG approach to question answering over private text corpora that scales\\nwith both the generality of user questions and the quantity of source text to be in-\\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\\nfirst to derive an entity knowledge graph from the source documents, then to pre-\\ngenerate community summaries for all groups of closely-related entities. Given a\\nquestion, each community summary is used to generate a partial response, before\\nall partial responses are again summarized in a final response to the user. For a\\nclass of global sensemaking questions over datasets in the 1 million token range,\\nwe show that Graph RAG leads to substantial improvements over a na ¨ıve RAG\\nbaseline for both the comprehensiveness and diversity of generated answers. An']], 'uris': None, 'data': None, 'metadatas': [[{'category': 'artifical_intelligence', 'keywords': 'graph', 'page': 13, 'source': 'files/论文 - GraphRAG.pdf'}, {'category': 'artifical_intelligence', 'keywords': 'graph', 'page': 8, 'source': 'files/论文 - GraphRAG.pdf'}, {'category': 'artifical_intelligence', 'keywords': 'Graph RAG', 'page': 0, 'source': 'files/论文 - GraphRAG.pdf'}]], 'distances': [[0.7467752136883525, 0.7811027019206127, 0.7978898069101084]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = chroma_store.dense_search(collection_name=collection_name, query=query, k=k, metadata_filter = metadata_filter,fuzzy_filter=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma_store.hybrid_search(collection_name=collection_name, query=query, k=k, dense_weight=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma_store.search(collection_name=collection_name, query=query, k=k, dense_weight=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_context = chroma_store.get_formatted_context(collection_name=collection_name, query=query, k=k, dense_weight=dense_weight)\n",
    "formatted_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('ZETATECHS_API_KEY')\n",
    "base_url = os.getenv('ZETATECHS_API_BASE')\n",
    "\n",
    "import chromadb # type: ignore\n",
    "import chromadb.utils.embedding_functions as embedding_functions # type: ignore\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"ChromaVDB\")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=api_key,\n",
    "    api_base=base_url,\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"test\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(collection.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.get_or_create_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(collection.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.create_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
