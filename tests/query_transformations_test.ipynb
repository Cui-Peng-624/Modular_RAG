{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# 直接指定项目根目录\n",
    "project_root = str(Path.cwd().parent)  # 跳到RAG目录下\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"什么是深度强化学习？\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. multi_query.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.query_transformations.multi_query import generate_queries_multi_query, generate_queries_multi_query_with_structured_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = generate_queries_multi_query(query)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = generate_queries_multi_query_with_structured_output(query)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.query_transformations.decomposition import generate_queries_decomposition_with_structured_output, generate_final_prompt_decomposition_recursively, generate_final_prompt_decomposition_individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深度强化学习的基本概念是什么？', '深度强化学习与传统强化学习有什么区别？', '深度强化学习的主要应用领域有哪些？', '深度强化学习的核心算法有哪些？', '如何评价深度强化学习的效果和挑战？']\n"
     ]
    }
   ],
   "source": [
    "result3 = generate_queries_decomposition_with_structured_output(query)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the question you need to answer:\n",
      "\n",
      "\n",
      " --- \n",
      " 什么是深度强化学习？ \n",
      " --- \n",
      "\n",
      "\n",
      "Here is any available background question + answer pairs:\n",
      "\n",
      "\n",
      " --- \n",
      " Question: 深度强化学习的基本概念是什么？\n",
      "Answer: 深度强化学习（Deep Reinforcement Learning, DRL）的基本概念是一个序贯决策框架，其中智能体通过在环境中执行动作来学习，旨在最大化获得的奖励。例如，智能体可以控制视频游戏角色的移动，通过获得分数来实现目标。强化学习涉及几个关键组件，如智能体、环境、动作、奖励和策略。\n",
      "\n",
      "通过学习一个策略，智能体能够将环境的观察转化为动作，目标是最大化与奖励相关的预期回报。在实际应用中，深度强化学习通常结合深度学习技术，使用深度神经网络对环境输入进行编码（如视频游戏画面或机器人传感器数据），并将其映射到所需的动作上。这种结合使得深度强化学习能够处理复杂和高维的输入数据，如图像和时间序列数据，从而提高学习效率和效果（片段 2）。\n",
      "\n",
      "Question: 深度强化学习与传统强化学习有什么区别？\n",
      "Answer: 深度强化学习与传统强化学习的主要区别在于对于策略和环境的处理方式。\n",
      "\n",
      "1. **策略表示**：\n",
      "   - **传统强化学习**通常依赖于表格方法或简单的函数近似技术来表示策略，这些方法在解决简单问题时可能有效。但当状态和行动空间非常大时，传统方法会变得不够高效。\n",
      "   - **深度强化学习**则使用深度神经网络来表示和优化策略。这种方法允许处理高维状态空间（如图像或复杂环境），并能够自动学习状态与行动之间的复杂关系（文档片段 0 和 2）。\n",
      "\n",
      "2. **信息处理**：\n",
      "   - 在**传统强化学习**中，智能体通常直接与环境互动，并通过反馈不断调整策略。\n",
      "   - **深度强化学习**中引入了更先进的数据处理技术，如“决策变换器”，将离线学习视为序列预测问题。这些方法能够处理大量数据，整合信息，从而减少时间信用分配问题（文档片段 1）。\n",
      "\n",
      "3. **探索与利用**：\n",
      "   - 二者都面临探索与利用的平衡问题，但深度强化学习使用深度学习的方法强化这一过程，使得在复杂环境中探索新的策略变得更加可行（文档片段 2）。\n",
      "\n",
      "4. **应用领域**：\n",
      "   - 而在应用方面，深度强化学习已经在视频游戏、机器人控制和金融领域等复杂应用中取得了显著成果，而传统强化学习则主要适用于较为简单的环境（文档片段 2）。\n",
      "\n",
      "综上所述，深度强化学习通过使用深度网络和先进的数据处理技术提高了传统强化学习在复杂环境中学习的效率和能力。\n",
      "\n",
      "Question: 深度强化学习的应用场景有哪些？\n",
      "Answer: 深度强化学习（Deep Reinforcement Learning, DRL）有多个应用场景，以下是一些主要的例子：\n",
      "\n",
      "1. **游戏控制**：在视频游戏中，深度强化学习可以控制角色的移动，以最大化游戏分数。例如，智能体可以通过学习策略来进行有效的游戏决策（文档片段 2）。\n",
      "\n",
      "2. **机器人控制**：在机器人领域，深度强化学习可以用于指导机器人在现实世界中执行任务，通过学习优化其动作，从而获得奖励。例如，机器人可以在特定的环境中学习如何行走或完成其他复杂任务（文档片段 2）。\n",
      "\n",
      "3. **金融交易**：在金融领域，深度强化学习能够控制虚拟交易员在交易平台上进行资产的买卖，以最大化利润。这种应用涉及到复杂的市场环境和策略学习（文档片段 2）。\n",
      "\n",
      "4. **棋类游戏**：深度强化学习也广泛应用于棋类游戏的学习，比如围棋和象棋。智能体在游戏结束时根据胜负获得奖励，通过分析历史游戏数据来学习有效的策略（文档片段 2）。\n",
      "\n",
      "这些应用都展示了深度强化学习在多种复杂环境中进行决策和优化的能力。\n",
      "\n",
      "Question: 深度强化学习的主要算法和模型是什么？\n",
      "Answer: 深度强化学习的主要算法和模型包括以下几种：\n",
      "\n",
      "1. **深度Q学习（Deep Q-Learning）**: 该算法由Mnih等人在2015年提出，利用深度神经网络来估计每个状态的动作价值。它在ATARI游戏的基准测试中能够达到人类水平的表现。这种方法存在一些不稳定性问题，涉及到自举、离策略学习和函数逼近等方面的挑战（[文档片段 1] 和 [文档片段 2]）。\n",
      "\n",
      "2. **决策变换器（Decision Transformer）**: 这一模型将离线强化学习视为序列预测问题，利用状态、动作和剩余回报的序列来进行嵌入并预测下一步的动作。它有效地处理大规模数据，并能够整合广泛时间范围内的信息，这为时间信用分配问题提供了新的解决方案（[文档片段 0]）。\n",
      "\n",
      "3. **策略梯度方法**: 这些方法直接优化策略，而不是为动作赋值。它们生成随机策略，特别适用于部分可观测的环境。这类方法的更新过程通常包含噪声，并引入了多种改进措施以减少方差（[文档片段 0]）。\n",
      "\n",
      "此外，还有一些经典的强化学习方法为深度强化学习奠定了基础：\n",
      "- **SARSA**: 由Rummery & Niranjan在1994年开发。\n",
      "- **拟合Q学习（Fitted Q-Learning）**: 由Gordon在1995年提出，通过机器学习模型预测每个状态-动作对的价值。\n",
      "- **神经拟合Q学习**: 由Riedmiller在2005年引入，使用神经网络从一个状态一次性预测所有动作的价值（[文档片段 2]）。\n",
      "\n",
      "以上这些算法和模型共同推动了深度强化学习的发展，并在诸多应用领域中取得了显著的成就。\n",
      "\n",
      "Question: 如何评估和优化深度强化学习的效果？\n",
      "Answer: 评估和优化深度强化学习的效果可以通过以下几个方面来进行：\n",
      "\n",
      "1. **使用多步估计**：随着样本数量（k）的增加，估计的方差会增加，但偏差会减少。采用如广义优势估计（Schulman等人，2016）等方法，可以将多个估计结果综合，从而优化偏差与方差之间的权衡（文档片段0）。\n",
      "\n",
      "2. **异步演员-评论家模型**：在模型如A3C中，多个代理在并行环境中运行并更新相同的参数。这种方法能有效提高学习效率，因此对强化学习的效果优化有积极作用（文档片段0）。\n",
      "\n",
      "3. **软演员-评论家模型**：该模型在成本函数中增加熵项，这不仅鼓励探索，还能减少过拟合，使策略更加不自信，从而提升模型的泛化能力和表现（文档片段0）。\n",
      "\n",
      "4. **离线强化学习**：通过观察其他代理的行为和获得的奖励来学习策略，而不需要直接干预。这种方式可以帮助减少样本偏差的问题，并提升性能（文档片段0）。\n",
      "\n",
      "5. **强化学习结合人类反馈（RLHF）**：此技术通过用户反馈来优化模型响应，例如通过人类评价者提供的偏好数据进行训练。这种方法可以在生成模型的训练中使用，以确保输出的高质量（文档片段0）。\n",
      "\n",
      "6. **对比基准测试**：可以通过在固定规则和有限动作空间的环境中（如ATARI游戏），与人类水平的表现进行对比，来评估强化学习模型的效果（文档片段1）。\n",
      "\n",
      "7. **应用组合优化问题**：引入组合优化问题的解决方案，例如旅行商问题的强化学习模型，能够展示大规模问题下的表现，从而评估效果（文档片段1）。\n",
      "\n",
      "综合以上方法，可以全面地评估和优化深度强化学习的效果，增强模型的性能和稳定性。\n",
      "\n",
      " \n",
      " --- \n",
      "\n",
      "\n",
      "Here is additional context relevant to the question: \n",
      "\n",
      "\n",
      " --- \n",
      " [文档片段 1]\n",
      "习，并证明了它通过 Banach定理收敛到一个固定点，因为 Bellman 操作是收缩映射。 Watkins (1989) 首次明确地将动态规划和增强学习联系起来。 SARSA是由Rummery & Niranjan (1994) 开发的。 Gordon (1995) 提出了拟合 Q学习，用机器学习模型预测每个 状态-动作对的价值。 Riedmiller (2005) 引入了神经拟合 Q学习，使用神经网络从一个 状态一次性预测所有动作的价值。 Singh & Sutton (1996) 对蒙特卡罗方法进行了早期研 究，探索启动算法则由 Sutton & Barto (1999) 提出。这是对五十多年工作的极简总结， Sutton & Barto (2018) 的著作中有更为全面的论述。 深度 Q网络：Mnih等人在2015年设计的深度 Q学习是神经拟合 Q学习的理论 衍生。它利用了当时卷积网络的进展，开发出一种拟合 Q学习方法，在 ATARI游戏基 准测试中达到人类水平的表现。深度 Q学习存在致命的三重问题，即训练在包含自举、 离策略学习和函数逼近的方案中可能不稳定（ Sutton & Barto, 2018 ） 。很多后续研究致 力于让训练过程更加稳定。 Mnih等人(2015)引入了经验回放机制（ Lin, 1992 ） ，Schaul 等人(2016)对其进行改进，优先考虑更重要的经验，从而加快学习速度，这就是所谓的 优先经验回放。 原始的Q学习论文使用四帧图像串联，以便网络观察到对象的速度，使底层过程更 接近完全可观测。 Hausknecht & Stone (2015) 引入了深度递归 Q学习，使用循环网络 架构，一次只处理一个帧图像，因为它能“记住”之前的状态。 Van Hasselt (2010) 指出\n",
      "\n",
      "---\n",
      "\n",
      "[文档片段 2]\n",
      "350 CHAPTER 19. 深度强化学习 图 19.17:决策变换器。决策变换器把离线强化学习处理为序列预测问题。输入是状态、动作和 剩余回报的序列，每个元素都被转换为固定大小的嵌入。每一步，网络预测下一动作。在测试阶 段，剩余回报未知；实际中，通常从一个初始估计出发，逐渐扣除观测到的奖励。 能够处理大量数据，并在广阔的时间范围内整合信息，使时间信用分配问题变得更易于 处理。这为强化学习开辟了一条新的、令人兴奋的道路。 19.8总结 增强学习 (Reinforcement Learning) 是针对马尔科夫决策过程 (Markov Decision Processes) 及其类似系统的序贯决策框架。本章介绍了增强学习的表格方法，包括动态 规划（环境模型已知） 、蒙特卡罗方法（通过运行多个回合并根据获得的奖励调整动作 值和策略）和时差分方法（在回合进行中更新这些值） 。 深度Q学习(Deep Q-Learning) 是一种时差分方法，使用深度神经网络预测每个状 态的动作价值，能够训练智能体在 Atari 2600 游戏中达到类似人类的水平。策略梯度方 法直接对策略进行优化，而非对动作进行价值赋值。这些方法生成的是随机策略，在部 分可观测的环境中尤其重要。这些更新过程含有噪声，为减少其方差已经引入了多种改 进措施。 当无法直接与环境互动而必须依赖历史数据学习时，就会使用离线增强学习。决策 变换器(Decision Transformer) 利用深度学习的最新进展构建状态 -动作-奖励序列模型， 并预测能够最大化奖励的动作。 19.9笔记 Sutton和Barto在2018年的作品中详细介绍了表格型增强学习方法。 Li (2017) 、 Arulkumaran 等人(2017)、FranCois-Lavet 等人(2018)和Wang等人(2022c)分别提供 了深度增强学习领域的综述。 Graesser 和Keng的2019年作品是一本优秀的入门资源， 其中包含了 Python代码示例。\n",
      "\n",
      "---\n",
      "\n",
      "[文档片段 3]\n",
      "Chapter 19 深度强化学习 强化学习（ RL）是一个序贯决策框架，智能体在此框架中通过在环境内执行动作来 学习，旨在最大化获得的奖励。例如， RL算法可以控制视频游戏中角色（智能体）的 移动（动作） ，以最大化分数（奖励） 。在机器人领域， RL算法能控制机器人（智能体） 在现实世界（环境）内的活动，执行特定任务以赚取奖励。在金融领域， RL算法或许 会控制一个虚拟交易员（智能体） ，在交易平台（环境）上进行资产买卖（动作） ，以最 大化利润（奖励） 。 以学习下棋为例，游戏结束时，根据智能体的胜、负或平，奖励分别为 +1、-1或 0，而在游戏的其他时间步骤中奖励为 0。这体现了 RL的挑战。首先，奖励是稀疏的， 即只有在完成整场游戏后才能获得反馈。其次，奖励与导致其发生的动作之间存在时间 上的延迟，如在获得胜利的三十步之前可能就已获得决定性优势，这要求将奖励与关键 动作关联起来，这种情况称为时间信用分配问题。第三，环境具有随机性，对手的每次 移动不总是相同的，这使得判断一个动作是真正有效还是仅仅依赖运气变得困难。最后， 智能体需要在探索环境（例如，尝试新的开局方式）与利用已有知识（例如，使用之前 成功的开局）之间做出平衡，这种平衡称为探索与利用的权衡。 强化学习是一个广泛的框架，不必须依赖深度学习。然而，在实际应用中，先进的 系统通常会使用深度网络，这些网络对环境（如视频游戏画面、机器人传感器、金融时 间序列或棋盘）进行编码，并将其直接或间接映射到下一步动作（图 1.13） 。 19.1马尔可夫决策过程、回报与策略 强化学习将环境观察转化为动作，其目标是最大化与所获奖励相关的数值。通常， 我们通过学习一个策略来最大化马尔科夫决策过程中的预期回报。本节将解释这些术 语。 331 \n",
      " --- \n",
      "\n",
      "\n",
      "Use the above context and any background question + answer pairs to answer the question: \n",
      " 什么是深度强化学习？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result4 = generate_final_prompt_decomposition_recursively(query, dense_weight=0.7)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a set of Q+A pairs:\n",
      "\n",
      "Question: 深度强化学习的基本概念是什么？\n",
      "Answer: 深度强化学习是结合强化学习（RL）和深度学习的一个领域。在此框架中，智能体通过在环境中执行动作来学习，目标是最大化获得的奖励。强化学习不依赖深度学习，但在实际应用中，深度学习被广泛用于处理复杂的输入数据，例如视频游戏画面、机器人传感器或其他形式的环境数据。\n",
      "\n",
      "深度强化学习的基本概念包括以下几个要素：\n",
      "\n",
      "1. **智能体与环境**：智能体通过在环境中执行动作来影响状态，并从中获得奖励。\n",
      "2. **奖励**：智能体在执行序列动作后，根据环境反馈获得奖励，这些奖励可以是稀疏的，且与动作之间可能存在时间上的延迟。\n",
      "3. **策略**：智能体学习一个策略，该策略将观察到的环境状态映射到动作，以最大化预期的累计奖励。\n",
      "4. **探索与利用**：智能体需要平衡探索新动作与利用已有知识之间的关系，以提高学习效率。\n",
      "\n",
      "深度强化学习利用深度神经网络来逼近策略或价值函数，从而使得学习能够处理高维度和复杂的数据输入。这使得深度强化学习在许多应用领域（如视频游戏、机器人控制、金融交易等）取得了显著的成果。\n",
      "\n",
      "Question: 深度强化学习与传统强化学习有何不同？\n",
      "Answer: 深度强化学习（Deep Reinforcement Learning, DRL）与传统强化学习（Reinforcement Learning, RL）的主要不同点在于使用的功能逼近方法。\n",
      "\n",
      "1. **功能逼近**：\n",
      "   - 在传统强化学习中，通常使用表格方法或线性函数来表示策略或价值函数，这在状态空间相对较小的情况下是可行的。但当状态空间变得非常大或复杂（例如图像、高清视频等），表格方法会变得不实际。\n",
      "   - 深度强化学习则引入了深度学习技术，利用深度神经网络作为功能逼近器，能够处理高维输入（如图像、传感器数据），从而将环境的状态映射到相应的动作或价值估计。\n",
      "\n",
      "2. **处理复杂环境的能力**：\n",
      "   - 深度强化学习能够处理更复杂的环境和时间序列数据，通过在网络中集成大量的数据来识别模式和策略。这使得深度强化学习在许多实际应用（如游戏、机器人控制、金融交易）中表现出更强的性能和灵活性。\n",
      "\n",
      "3. **时间信用分配问题**：\n",
      "   - 在深度强化学习中，使用深度网络能够更好地整合长时间范围内的信息，使得时间信用分配问题（如何将延迟的奖励与具体的动作关联起来）变得更易于处理。这是因为深度网络的结构使得它能够在较大的时间跨度内学习和预测。\n",
      "\n",
      "4. **策略优化**：\n",
      "   - 在深度强化学习中，策略梯度方法允许智能体直接优化其策略，而非仅通过状态-动作值的更新。这种方式支持学习随机策略，并提高了在部分可观测环境中的表现。\n",
      "\n",
      "总之，深度强化学习通过结合深度神经网络并引入更复杂的功能逼近方法，使得其在处理高维状态空间和复杂环境方面相比传统强化学习更为高效和灵活。\n",
      "\n",
      "Question: 深度强化学习的主要应用领域有哪些？\n",
      "Answer: 深度强化学习的主要应用领域包括：\n",
      "\n",
      "1. **视频游戏**：深度强化学习在视频游戏中的应用十分广泛，例如深度Q学习在ATARI游戏中达到了人类水平的表现。此外，AlphaGo战胜了围棋世界冠军、Dota 2中开发的系统击败了世界冠军队等。\n",
      "\n",
      "2. **机器人控制**：深度强化学习能够用于控制机器人在现实世界中执行特定任务，并通过其行为获取奖励。\n",
      "\n",
      "3. **金融交易**：在金融领域，深度强化学习可用于控制虚拟交易员，通过交易平台上的资产买卖来最大化利润。\n",
      "\n",
      "4. **组合优化问题**：例如，研究者利用深度强化学习解决旅行商问题以及矩阵乘法等优化任务，显示出在大型数据集上的有效性。\n",
      "\n",
      "5. **自然语言处理**：最近的应用还扩展到了需要自然语言协商的游戏，如“外交”（Diplomacy），展示了深度强化学习的进一步潜力。\n",
      "\n",
      "总体而言，深度强化学习的应用涵盖了从游戏、机器人、金融到组合优化等多个领域，展示了其广泛的适用性和强大的性能。\n",
      "\n",
      "Question: 深度强化学习的关键技术和算法是什么？\n",
      "Answer: 深度强化学习的关键技术和算法包括以下几个方面：\n",
      "\n",
      "1. **深度 Q 学习 (DQN)**：由 Mnih 等人在 2015 年提出，结合了 Q 学习和深度学习，利用深度神经网络来逼近 Q 值函数。在 ATARI 游戏基准测试中实现了人类水平的表现。\n",
      "\n",
      "2. **经验回放机制**：为了解决训练不稳定的问题，DQN 引入了经验回放（Experience Replay）机制，允许智能体复用之前的经验进行学习。Schaul 等人（2016）对这一机制进行了改进，引入优先经验回放，以加快学习速度。\n",
      "\n",
      "3. **深度递归 Q 学习**：Hausknecht 和 Stone（2015）提出的一种方法，使用循环神经网络（RNN）来处理图像帧，使得智能体能够记住之前的状态，从而更有效地处理时间序列数据。\n",
      "\n",
      "4. **蒙特卡罗方法和SARSA**：早期的强化学习方法之一，包括 SARSA 算法（由 Rummery & Niranjan 提出），以及对蒙特卡罗方法的研究，这些都是深度强化学习发展的基础。\n",
      "\n",
      "5. **拟合 Q 学习**：Gordon（1995）提出的概念，使用机器学习模型来预测每个状态-动作对的价值，成为深度 Q 网络的理论基础。\n",
      "\n",
      "6. **策略迭代与动态规划**：Watkins 等人在 1989 年首次将动态规划和增强学习联系起来，策略迭代及其变种也是强化学习中的关键方法。\n",
      "\n",
      "以上技术和算法组成了深度强化学习的核心，推动了在视频游戏、机器人控制、金融交易等多个领域的应用和发展。\n",
      "\n",
      "Question: 深度强化学习面临的挑战和未来发展方向是什么？\n",
      "Answer: 根据背景信息，深度强化学习面临的主要挑战包括：\n",
      "\n",
      "1. **稀疏奖励问题**：在许多任务中，智能体仅在任务结束时获得奖励，大多数时间没有即时反馈，这使得学习变得困难。\n",
      "\n",
      "2. **时间信用分配问题**：奖励的获得与智能体在执行的动作之间存在延迟，智能体需要将奖励与导致其发生的关键动作关联起来，比如在长时间步骤后获得胜利可能是由于之前的某个关键决策。\n",
      "\n",
      "3. **环境的随机性**：对手的行为通常是不可预测的，这使得评估智能体的动作效果变得更加复杂，不能简单地将动作效果归因于成功或失败。\n",
      "\n",
      "4. **探索与利用的权衡**：智能体需要在尝试新策略（探索）与使用已知成功策略（利用）之间找到平衡，这在很多情况下是一个挑战。\n",
      "\n",
      "未来的发展方向可能包括：\n",
      "\n",
      "1. **改进算法**：开发更有效的算法来处理稀疏奖励和时间信用分配问题，例如采用更复杂的模型来推断动作的长远影响。\n",
      "\n",
      "2. **离线强化学习**：随着对历史数据的依赖增加，研究如何更好地利用这些数据，提升学习效率和效果。\n",
      "\n",
      "3. **更复杂的环境建模**：为了应对环境随机性和复杂性，探索如何将强化学习与更强大的环境模拟或建模方法结合。\n",
      "\n",
      "4. **自适应探索策略**：研究如何使智能体更好地在不同的环境和任务中自适应地调整其探索与利用策略，提高整体学习效率。\n",
      "\n",
      "综上所述，深度强化学习虽然面临诸多挑战，但通过算法创新和有效的利用现有技术，未来仍有广阔的发展空间。\n",
      "\n",
      "\n",
      "\n",
      "Use these to synthesize an answer to the question: 什么是深度强化学习？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result5 = generate_final_prompt_decomposition_individually(query, dense_weight=0.7)\n",
    "print(result5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. step_back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是深度学习和强化学习的结合？\n"
     ]
    }
   ],
   "source": [
    "from src.query_transformations.step_back import step_back_question\n",
    "result6 = step_back_question(query)\n",
    "print(result6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
      "\n",
      "class ChatOpenAI(BaseChatOpenAI)\n",
      " |  ChatOpenAI(*args: Any, name: Optional[str] = None, cache: Union[langchain_core.caches.BaseCache, bool, NoneType] = None, verbose: bool = None, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], list[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, rate_limiter: Optional[langchain_core.rate_limiters.BaseRateLimiter] = None, disable_streaming: Union[bool, Literal['tool_calling']] = False, client: Any = None, async_client: Any = None, root_client: Any = None, root_async_client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, api_key: Optional[pydantic.types.SecretStr] = None, base_url: Optional[str] = None, organization: Optional[str] = None, openai_proxy: Optional[str] = None, timeout: Union[float, Tuple[float, float], Any, NoneType] = None, max_retries: int = 2, presence_penalty: Optional[float] = None, frequency_penalty: Optional[float] = None, seed: Optional[int] = None, logprobs: Optional[bool] = None, top_logprobs: Optional[int] = None, logit_bias: Optional[Dict[int, int]] = None, streaming: bool = False, n: int = 1, top_p: Optional[float] = None, max_tokens: Optional[int] = None, tiktoken_model_name: Optional[str] = None, default_headers: Optional[Mapping[str, str]] = None, default_query: Optional[Mapping[str, object]] = None, http_client: Optional[Any] = None, http_async_client: Optional[Any] = None, stop_sequences: Union[List[str], str, NoneType] = None, extra_body: Optional[Mapping[str, Any]] = None, include_response_headers: bool = False, disabled_params: Optional[Dict[str, Any]] = None, stream_usage: bool = False) -> None\n",
      " |\n",
      " |  OpenAI chat model integration.\n",
      " |\n",
      " |  .. dropdown:: Setup\n",
      " |      :open:\n",
      " |\n",
      " |      Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\n",
      " |\n",
      " |      .. code-block:: bash\n",
      " |\n",
      " |          pip install -U langchain-openai\n",
      " |          export OPENAI_API_KEY=\"your-api-key\"\n",
      " |\n",
      " |  .. dropdown:: Key init args — completion params\n",
      " |\n",
      " |      model: str\n",
      " |          Name of OpenAI model to use.\n",
      " |      temperature: float\n",
      " |          Sampling temperature.\n",
      " |      max_tokens: Optional[int]\n",
      " |          Max number of tokens to generate.\n",
      " |      logprobs: Optional[bool]\n",
      " |          Whether to return logprobs.\n",
      " |      stream_options: Dict\n",
      " |          Configure streaming outputs, like whether to return token usage when\n",
      " |          streaming (``{\"include_usage\": True}``).\n",
      " |\n",
      " |      See full list of supported init args and their descriptions in the params section.\n",
      " |\n",
      " |  .. dropdown:: Key init args — client params\n",
      " |\n",
      " |      timeout: Union[float, Tuple[float, float], Any, None]\n",
      " |          Timeout for requests.\n",
      " |      max_retries: int\n",
      " |          Max number of retries.\n",
      " |      api_key: Optional[str]\n",
      " |          OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\n",
      " |      base_url: Optional[str]\n",
      " |          Base URL for API requests. Only specify if using a proxy or service\n",
      " |          emulator.\n",
      " |      organization: Optional[str]\n",
      " |          OpenAI organization ID. If not passed in will be read from env\n",
      " |          var OPENAI_ORG_ID.\n",
      " |\n",
      " |      See full list of supported init args and their descriptions in the params section.\n",
      " |\n",
      " |  .. dropdown:: Instantiate\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          llm = ChatOpenAI(\n",
      " |              model=\"gpt-4o\",\n",
      " |              temperature=0,\n",
      " |              max_tokens=None,\n",
      " |              timeout=None,\n",
      " |              max_retries=2,\n",
      " |              # api_key=\"...\",\n",
      " |              # base_url=\"...\",\n",
      " |              # organization=\"...\",\n",
      " |              # other params...\n",
      " |          )\n",
      " |\n",
      " |      **NOTE**: Any param which is not explicitly supported will be passed directly to the\n",
      " |      ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\n",
      " |      invoked. For example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |          import openai\n",
      " |\n",
      " |          ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n",
      " |\n",
      " |          # results in underlying API call of:\n",
      " |\n",
      " |          openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n",
      " |\n",
      " |          # which is also equivalent to:\n",
      " |\n",
      " |          ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n",
      " |\n",
      " |  .. dropdown:: Invoke\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          messages = [\n",
      " |              (\n",
      " |                  \"system\",\n",
      " |                  \"You are a helpful translator. Translate the user sentence to French.\",\n",
      " |              ),\n",
      " |              (\"human\", \"I love programming.\"),\n",
      " |          ]\n",
      " |          llm.invoke(messages)\n",
      " |\n",
      " |      .. code-block:: pycon\n",
      " |\n",
      " |          AIMessage(\n",
      " |              content=\"J'adore la programmation.\",\n",
      " |              response_metadata={\n",
      " |                  \"token_usage\": {\n",
      " |                      \"completion_tokens\": 5,\n",
      " |                      \"prompt_tokens\": 31,\n",
      " |                      \"total_tokens\": 36,\n",
      " |                  },\n",
      " |                  \"model_name\": \"gpt-4o\",\n",
      " |                  \"system_fingerprint\": \"fp_43dfabdef1\",\n",
      " |                  \"finish_reason\": \"stop\",\n",
      " |                  \"logprobs\": None,\n",
      " |              },\n",
      " |              id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
      " |              usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: Stream\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          for chunk in llm.stream(messages):\n",
      " |              print(chunk)\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(\n",
      " |              content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n",
      " |          )\n",
      " |          AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(\n",
      " |              content=\"\",\n",
      " |              response_metadata={\"finish_reason\": \"stop\"},\n",
      " |              id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n",
      " |          )\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          stream = llm.stream(messages)\n",
      " |          full = next(stream)\n",
      " |          for chunk in stream:\n",
      " |              full += chunk\n",
      " |          full\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          AIMessageChunk(\n",
      " |              content=\"J'adore la programmation.\",\n",
      " |              response_metadata={\"finish_reason\": \"stop\"},\n",
      " |              id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: Async\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          await llm.ainvoke(messages)\n",
      " |\n",
      " |          # stream:\n",
      " |          # async for chunk in (await llm.astream(messages))\n",
      " |\n",
      " |          # batch:\n",
      " |          # await llm.abatch([messages])\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          AIMessage(\n",
      " |              content=\"J'adore la programmation.\",\n",
      " |              response_metadata={\n",
      " |                  \"token_usage\": {\n",
      " |                      \"completion_tokens\": 5,\n",
      " |                      \"prompt_tokens\": 31,\n",
      " |                      \"total_tokens\": 36,\n",
      " |                  },\n",
      " |                  \"model_name\": \"gpt-4o\",\n",
      " |                  \"system_fingerprint\": \"fp_43dfabdef1\",\n",
      " |                  \"finish_reason\": \"stop\",\n",
      " |                  \"logprobs\": None,\n",
      " |              },\n",
      " |              id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
      " |              usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: Tool calling\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |          class GetWeather(BaseModel):\n",
      " |              '''Get the current weather in a given location'''\n",
      " |\n",
      " |              location: str = Field(\n",
      " |                  ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      " |              )\n",
      " |\n",
      " |\n",
      " |          class GetPopulation(BaseModel):\n",
      " |              '''Get the current population in a given location'''\n",
      " |\n",
      " |              location: str = Field(\n",
      " |                  ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      " |              )\n",
      " |\n",
      " |\n",
      " |          llm_with_tools = llm.bind_tools(\n",
      " |              [GetWeather, GetPopulation]\n",
      " |              # strict = True  # enforce tool args schema is respected\n",
      " |          )\n",
      " |          ai_msg = llm_with_tools.invoke(\n",
      " |              \"Which city is hotter today and which is bigger: LA or NY?\"\n",
      " |          )\n",
      " |          ai_msg.tool_calls\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          [\n",
      " |              {\n",
      " |                  \"name\": \"GetWeather\",\n",
      " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      " |                  \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n",
      " |              },\n",
      " |              {\n",
      " |                  \"name\": \"GetWeather\",\n",
      " |                  \"args\": {\"location\": \"New York, NY\"},\n",
      " |                  \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n",
      " |              },\n",
      " |              {\n",
      " |                  \"name\": \"GetPopulation\",\n",
      " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      " |                  \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n",
      " |              },\n",
      " |              {\n",
      " |                  \"name\": \"GetPopulation\",\n",
      " |                  \"args\": {\"location\": \"New York, NY\"},\n",
      " |                  \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |      Note that ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\n",
      " |      that defaults to ``True``. This parameter can be set to ``False`` to\n",
      " |      disable parallel tool calls:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          ai_msg = llm_with_tools.invoke(\n",
      " |              \"What is the weather in LA and NY?\", parallel_tool_calls=False\n",
      " |          )\n",
      " |          ai_msg.tool_calls\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          [\n",
      " |              {\n",
      " |                  \"name\": \"GetWeather\",\n",
      " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      " |                  \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n",
      " |              }\n",
      " |          ]\n",
      " |\n",
      " |      Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\n",
      " |      using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\n",
      " |      setting ``model_kwargs``.\n",
      " |\n",
      " |      See ``ChatOpenAI.bind_tools()`` method for more.\n",
      " |\n",
      " |  .. dropdown:: Structured output\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Optional\n",
      " |\n",
      " |          from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |          class Joke(BaseModel):\n",
      " |              '''Joke to tell user.'''\n",
      " |\n",
      " |              setup: str = Field(description=\"The setup of the joke\")\n",
      " |              punchline: str = Field(description=\"The punchline to the joke\")\n",
      " |              rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
      " |\n",
      " |\n",
      " |          structured_llm = llm.with_structured_output(Joke)\n",
      " |          structured_llm.invoke(\"Tell me a joke about cats\")\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          Joke(\n",
      " |              setup=\"Why was the cat sitting on the computer?\",\n",
      " |              punchline=\"To keep an eye on the mouse!\",\n",
      " |              rating=None,\n",
      " |          )\n",
      " |\n",
      " |      See ``ChatOpenAI.with_structured_output()`` for more.\n",
      " |\n",
      " |  .. dropdown:: JSON mode\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
      " |          ai_msg = json_llm.invoke(\n",
      " |              \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n",
      " |          )\n",
      " |          ai_msg.content\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          '\\n{\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\n}'\n",
      " |\n",
      " |  .. dropdown:: Image input\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          import base64\n",
      " |          import httpx\n",
      " |          from langchain_core.messages import HumanMessage\n",
      " |\n",
      " |          image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
      " |          image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
      " |          message = HumanMessage(\n",
      " |              content=[\n",
      " |                  {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
      " |                  {\n",
      " |                      \"type\": \"image_url\",\n",
      " |                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
      " |                  },\n",
      " |              ]\n",
      " |          )\n",
      " |          ai_msg = llm.invoke([message])\n",
      " |          ai_msg.content\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n",
      " |\n",
      " |  .. dropdown:: Token usage\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          ai_msg = llm.invoke(messages)\n",
      " |          ai_msg.usage_metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
      " |\n",
      " |      When streaming, set the ``stream_usage`` kwarg:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          stream = llm.stream(messages, stream_usage=True)\n",
      " |          full = next(stream)\n",
      " |          for chunk in stream:\n",
      " |              full += chunk\n",
      " |          full.usage_metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
      " |\n",
      " |      Alternatively, setting ``stream_usage`` when instantiating the model can be\n",
      " |      useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\n",
      " |      methods like ``.with_structured_output``, which generate chains under the\n",
      " |      hood.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\n",
      " |          structured_llm = llm.with_structured_output(...)\n",
      " |\n",
      " |  .. dropdown:: Logprobs\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          logprobs_llm = llm.bind(logprobs=True)\n",
      " |          ai_msg = logprobs_llm.invoke(messages)\n",
      " |          ai_msg.response_metadata[\"logprobs\"]\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\n",
      " |              \"content\": [\n",
      " |                  {\n",
      " |                      \"token\": \"J\",\n",
      " |                      \"bytes\": [74],\n",
      " |                      \"logprob\": -4.9617593e-06,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \"'adore\",\n",
      " |                      \"bytes\": [39, 97, 100, 111, 114, 101],\n",
      " |                      \"logprob\": -0.25202933,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \" la\",\n",
      " |                      \"bytes\": [32, 108, 97],\n",
      " |                      \"logprob\": -0.20141791,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \" programmation\",\n",
      " |                      \"bytes\": [\n",
      " |                          32,\n",
      " |                          112,\n",
      " |                          114,\n",
      " |                          111,\n",
      " |                          103,\n",
      " |                          114,\n",
      " |                          97,\n",
      " |                          109,\n",
      " |                          109,\n",
      " |                          97,\n",
      " |                          116,\n",
      " |                          105,\n",
      " |                          111,\n",
      " |                          110,\n",
      " |                      ],\n",
      " |                      \"logprob\": -1.9361265e-07,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \".\",\n",
      " |                      \"bytes\": [46],\n",
      " |                      \"logprob\": -1.2233183e-05,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |              ]\n",
      " |          }\n",
      " |\n",
      " |  .. dropdown:: Response metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          ai_msg = llm.invoke(messages)\n",
      " |          ai_msg.response_metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\n",
      " |              \"token_usage\": {\n",
      " |                  \"completion_tokens\": 5,\n",
      " |                  \"prompt_tokens\": 28,\n",
      " |                  \"total_tokens\": 33,\n",
      " |              },\n",
      " |              \"model_name\": \"gpt-4o\",\n",
      " |              \"system_fingerprint\": \"fp_319be4768e\",\n",
      " |              \"finish_reason\": \"stop\",\n",
      " |              \"logprobs\": None,\n",
      " |          }\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ChatOpenAI\n",
      " |      BaseChatOpenAI\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel[BaseMessage]\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], TypeVar]\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  get_lc_namespace() -> 'List[str]'\n",
      " |      Get the namespace of the langchain object.\n",
      " |\n",
      " |  is_lc_serializable() -> 'bool'\n",
      " |      Return whether this model can be serialized by Langchain.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |\n",
      " |      These attributes must be accepted by the constructor.\n",
      " |      Default is an empty dictionary.\n",
      " |\n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |\n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'stream_usage': 'bool'}\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'function': {'function': <function BaseCha...\n",
      " |\n",
      " |  __pydantic_custom_init__ = True\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatOpenAI\", validator...\n",
      " |\n",
      " |  __signature__ = <Signature (*args: Any, name: Optional[str] = No...ny]...\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
      " |\n",
      " |  model_fields = {'async_client': FieldInfo(annotation=Any, required=Fal...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseChatOpenAI:\n",
      " |\n",
      " |  bind_functions(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      .. deprecated:: langchain-openai==0.2.1 Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
      " |\n",
      " |      Bind functions (and other objects) to this chat model.\n",
      " |\n",
      " |      Assumes model is compatible with OpenAI function-calling API.\n",
      " |\n",
      " |      NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
      " |          `function_call` request parameters are officially marked as deprecated by\n",
      " |          OpenAI.\n",
      " |\n",
      " |      Args:\n",
      " |          functions: A list of function definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
      " |              models and callables will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          function_call: Which function to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any).\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |\n",
      " |  bind_tools(self, tools: 'Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]', *, tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none', 'required', 'any'], bool]]\" = None, strict: 'Optional[bool]' = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |\n",
      " |      Assumes model is compatible with OpenAI tool-calling API.\n",
      " |\n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |              Supports any tool definition handled by\n",
      " |              :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n",
      " |          tool_choice: Which tool to require the model to call. Options are:\n",
      " |\n",
      " |              - str of the form ``\"<<tool_name>>\"``: calls <<tool_name>> tool.\n",
      " |              - ``\"auto\"``: automatically selects a tool (including no tool).\n",
      " |              - ``\"none\"``: does not call a tool.\n",
      " |              - ``\"any\"`` or ``\"required\"`` or ``True``: force at least one tool to be called.\n",
      " |              - dict of the form ``{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}``: calls <<tool_name>> tool.\n",
      " |              - ``False`` or ``None``: no effect, default OpenAI behavior.\n",
      " |          strict: If True, model output is guaranteed to exactly match the JSON Schema\n",
      " |              provided in the tool definition. If True, the input schema will be\n",
      " |              validated according to\n",
      " |              https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n",
      " |              If False, input schema will not be validated and model output will not\n",
      " |              be validated.\n",
      " |              If None, ``strict`` argument will not be passed to the model.\n",
      " |          kwargs: Any additional parameters are passed directly to\n",
      " |              :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind`.\n",
      " |\n",
      " |      .. versionchanged:: 0.1.21\n",
      " |\n",
      " |          Support for ``strict`` argument added.\n",
      " |\n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
      " |\n",
      " |      **Requirements**: You must have the ``pillow`` installed if you want to count\n",
      " |      image tokens if you are specifying the image as a base64 string, and you must\n",
      " |      have both ``pillow`` and ``httpx`` installed if you are specifying the image\n",
      " |      as a URL. If these aren't installed image inputs will be ignored in token\n",
      " |      counting.\n",
      " |\n",
      " |      OpenAI reference: https://github.com/openai/openai-cookbook/blob/\n",
      " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
      " |\n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Get the tokens present in the text with tiktoken package.\n",
      " |\n",
      " |  validate_environment(self) -> 'Self'\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |\n",
      " |  with_structured_output(self, schema: 'Optional[_DictOrPydanticClass]' = None, *, method: \"Literal['function_calling', 'json_mode', 'json_schema']\" = 'function_calling', include_raw: 'bool' = False, strict: 'Optional[bool]' = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, _DictOrPydantic]'\n",
      " |      Model wrapper that returns outputs formatted to match the given schema.\n",
      " |\n",
      " |      Args:\n",
      " |          schema:\n",
      " |              The output schema. Can be passed in as:\n",
      " |\n",
      " |              - an OpenAI function/tool schema,\n",
      " |              - a JSON Schema,\n",
      " |              - a TypedDict class (support added in 0.1.20),\n",
      " |              - or a Pydantic class.\n",
      " |\n",
      " |              If ``schema`` is a Pydantic class then the model output will be a\n",
      " |              Pydantic instance of that class, and the model-generated fields will be\n",
      " |              validated by the Pydantic class. Otherwise the model output will be a\n",
      " |              dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n",
      " |              for more on how to properly specify types and descriptions of\n",
      " |              schema fields when specifying a Pydantic or TypedDict class.\n",
      " |\n",
      " |          method: The method for steering model generation, one of:\n",
      " |\n",
      " |              - \"function_calling\":\n",
      " |                  Uses OpenAI's tool-calling (formerly called function calling)\n",
      " |                  API: https://platform.openai.com/docs/guides/function-calling\n",
      " |              - \"json_schema\":\n",
      " |                  Uses OpenAI's Structured Output API: https://platform.openai.com/docs/guides/structured-outputs\n",
      " |                  Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", and later\n",
      " |                  models.\n",
      " |              - \"json_mode\":\n",
      " |                  Uses OpenAI's JSON mode. Note that if using JSON mode then you\n",
      " |                  must include instructions for formatting the output into the\n",
      " |                  desired schema into the model call:\n",
      " |                  https://platform.openai.com/docs/guides/structured-outputs/json-mode\n",
      " |\n",
      " |              Learn more about the differences between the methods and which models\n",
      " |              support which methods here:\n",
      " |\n",
      " |              - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n",
      " |              - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n",
      " |\n",
      " |          include_raw:\n",
      " |              If False then only the parsed structured output is returned. If\n",
      " |              an error occurs during model output parsing it will be raised. If True\n",
      " |              then both the raw model response (a BaseMessage) and the parsed model\n",
      " |              response will be returned. If an error occurs during output parsing it\n",
      " |              will be caught and returned as well. The final output is always a dict\n",
      " |              with keys \"raw\", \"parsed\", and \"parsing_error\".\n",
      " |          strict:\n",
      " |\n",
      " |              - True:\n",
      " |                  Model output is guaranteed to exactly match the schema.\n",
      " |                  The input schema will also be validated according to\n",
      " |                  https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n",
      " |              - False:\n",
      " |                  Input schema will not be validated and model output will not be\n",
      " |                  validated.\n",
      " |              - None:\n",
      " |                  ``strict`` argument will not be passed to the model.\n",
      " |\n",
      " |              If ``method`` is \"json_schema\" defaults to True. If ``method`` is\n",
      " |              \"function_calling\" or \"json_mode\" defaults to None. Can only be\n",
      " |              non-null if ``method`` is \"function_calling\" or \"json_schema\".\n",
      " |\n",
      " |          kwargs: Additional keyword args aren't supported.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n",
      " |\n",
      " |          | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n",
      " |\n",
      " |          | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n",
      " |\n",
      " |          - \"raw\": BaseMessage\n",
      " |          - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n",
      " |          - \"parsing_error\": Optional[BaseException]\n",
      " |\n",
      " |      .. versionchanged:: 0.1.20\n",
      " |\n",
      " |          Added support for TypedDict class ``schema``.\n",
      " |\n",
      " |      .. versionchanged:: 0.1.21\n",
      " |\n",
      " |          Support for ``strict`` argument added.\n",
      " |          Support for ``method`` = \"json_schema\" added.\n",
      " |\n",
      " |      .. note:: Planned breaking changes in version `0.3.0`\n",
      " |\n",
      " |          - ``method`` default will be changed to \"json_schema\" from\n",
      " |              \"function_calling\".\n",
      " |          - ``strict`` will default to True when ``method`` is\n",
      " |              \"function_calling\" as of version `0.3.0`.\n",
      " |\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=False, strict=True\n",
      " |\n",
      " |          Note, OpenAI has a number of restrictions on what types of schemas can be\n",
      " |          provided if ``strict`` = True. When using Pydantic, our model cannot\n",
      " |          specify any Field metadata (like min/max constraints) and fields cannot\n",
      " |          have default values.\n",
      " |\n",
      " |          See all constraints here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Optional\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: Optional[str] = Field(\n",
      " |                      default=..., description=\"A justification for the answer.\"\n",
      " |                  )\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification, strict=True\n",
      " |              )\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |\n",
      " |              # -> AnswerWithJustification(\n",
      " |              #     answer='They weigh the same',\n",
      " |              #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |              # )\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=True\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: str\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification, include_raw=True\n",
      " |              )\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
      " |              #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=TypedDict class, method=\"function_calling\", include_raw=False\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n",
      " |              # from typing_extensions, not from typing.\n",
      " |              from typing_extensions import Annotated, TypedDict\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(TypedDict):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: Annotated[\n",
      " |                      Optional[str], None, \"A justification for the answer.\"\n",
      " |                  ]\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'answer': 'They weigh the same',\n",
      " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=OpenAI function schema, method=\"function_calling\", include_raw=False\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |              oai_schema = {\n",
      " |                  'name': 'AnswerWithJustification',\n",
      " |                  'description': 'An answer to the user question along with justification for the answer.',\n",
      " |                  'parameters': {\n",
      " |                      'type': 'object',\n",
      " |                      'properties': {\n",
      " |                          'answer': {'type': 'string'},\n",
      " |                          'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n",
      " |                      },\n",
      " |                     'required': ['answer']\n",
      " |                 }\n",
      " |             }\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(oai_schema)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'answer': 'They weigh the same',\n",
      " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"json_mode\", include_raw=True\n",
      " |\n",
      " |          .. code-block::\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  answer: str\n",
      " |                  justification: str\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification,\n",
      " |                  method=\"json_mode\",\n",
      " |                  include_raw=True\n",
      " |              )\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"Answer the following question. \"\n",
      " |                  \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
      " |                  \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='{\\n    \"answer\": \"They are both the same weight.\",\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\n}'),\n",
      " |              #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=None, method=\"json_mode\", include_raw=True\n",
      " |\n",
      " |          .. code-block::\n",
      " |\n",
      " |              structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"Answer the following question. \"\n",
      " |                  \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
      " |                  \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='{\\n    \"answer\": \"They are both the same weight.\",\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\n}'),\n",
      " |              #     'parsed': {\n",
      " |              #         'answer': 'They are both the same weight.',\n",
      " |              #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n",
      " |              #     },\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseChatOpenAI:\n",
      " |\n",
      " |  build_extra(values: 'Dict[str, Any]') -> 'Any'\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |\n",
      " |  validate_temperature(values: 'Dict[str, Any]') -> 'Any'\n",
      " |      Currently o1 models only allow temperature=1.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  __call__(self, messages: 'list[BaseMessage]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      .. deprecated:: langchain-core==0.1.7 Use :meth:`~invoke` instead.\n",
      " |\n",
      " |  async agenerate(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |\n",
      " |  async agenerate_prompt(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |\n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |\n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the Runnable did not implement a native async version of invoke.\n",
      " |\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |\n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      .. deprecated:: langchain-core==0.1.7 Use :meth:`~ainvoke` instead.\n",
      " |\n",
      " |  async apredict_messages(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      .. deprecated:: langchain-core==0.1.7 Use :meth:`~ainvoke` instead.\n",
      " |\n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  call_as_llm(self, message: 'str', stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      .. deprecated:: langchain-core==0.1.7 Use :meth:`~invoke` instead.\n",
      " |\n",
      " |  dict(self, **kwargs: 'Any') -> 'dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |\n",
      " |  generate(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |\n",
      " |  generate_prompt(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |\n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      .. deprecated:: langchain-core==0.1.7 Use :meth:`~invoke` instead.\n",
      " |\n",
      " |  predict_messages(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      .. deprecated:: langchain-core==0.1.7 Use :meth:`~invoke` instead.\n",
      " |\n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  raise_deprecation(values: 'dict') -> 'Any'\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |\n",
      " |      Args:\n",
      " |          values (Dict): Values to validate.\n",
      " |\n",
      " |      Returns:\n",
      " |          Dict: Validated values.\n",
      " |\n",
      " |      Raises:\n",
      " |          DeprecationWarning: If callback_manager is used.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |\n",
      " |      Useful for checking if an input fits in a model's context window.\n",
      " |\n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |\n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  set_verbose(verbose: 'Optional[bool]') -> 'bool'\n",
      " |      If verbose is None, set it.\n",
      " |\n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |\n",
      " |      Args:\n",
      " |          verbose: The verbosity setting to use.\n",
      " |\n",
      " |      Returns:\n",
      " |          The verbosity setting to use.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for Runnables that can be set at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          which: The ConfigurableField instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |              Defaults to \"default\".\n",
      " |          prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: A dictionary of keys to Runnable instances or callables that\n",
      " |              return Runnable instances.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the alternatives configured.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-sonnet-20240229\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI()\n",
      " |          )\n",
      " |\n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |\n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(\n",
      " |                  configurable={\"llm\": \"openai\"}\n",
      " |              ).invoke(\"which organization created you?\").content\n",
      " |          )\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular Runnable fields at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of ConfigurableField instances to configure.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the fields configured.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \",\n",
      " |              model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 200\n",
      " |          print(\"max_tokens_200: \", model.with_config(\n",
      " |              configurable={\"output_token_number\": 200}\n",
      " |              ).invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |\n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the Runnable to JSON.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |      # Remove default BaseModel init docstring.\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_id() -> list[str]\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```py\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#model_copy\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump_json\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |\n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/json/#json-parsing\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  async abatch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the Runnable.\n",
      " |\n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the Runnable.\n",
      " |\n",
      " |  as_tool(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Create a BaseTool from a Runnable.\n",
      " |\n",
      " |      ``as_tool`` will instantiate a BaseTool with a name, description, and\n",
      " |      ``args_schema`` from a Runnable. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      Runnable takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |\n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A BaseTool instance.\n",
      " |\n",
      " |      Typed dict input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import List\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: List[int]\n",
      " |\n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any, Dict, List\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |\n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: List[int] = Field(..., description=\"List of ints\")\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      String input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |\n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |\n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |\n",
      " |      .. versionadded:: 0.2.14\n",
      " |\n",
      " |  assign(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this Runnable.\n",
      " |      Returns a new Runnable.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |\n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |\n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |\n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |\n",
      " |          print(chain_with_assign.input_schema.model_json_schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.model_json_schema()) #\n",
      " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |\n",
      " |  async astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the Runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the Runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the Runnable that emitted the event.\n",
      " |          A child Runnable that gets invoked as part of the execution of a\n",
      " |          parent Runnable is assigned its own unique ID.\n",
      " |      - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n",
      " |          generated the event. The root Runnable will have an empty list.\n",
      " |          The order of the parent IDs is from the root to the immediate parent.\n",
      " |          Only available for v2 version of the API. The v1 version of the API\n",
      " |          will return an empty list.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |\n",
      " |\n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      **ATTENTION** This reference table is for the V2 version of the schema.\n",
      " |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |\n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |\n",
      " |      Custom events will be only be surfaced with in the `v2` version of the API!\n",
      " |\n",
      " |      A custom event has following format:\n",
      " |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |\n",
      " |      Example: Dispatch Custom Event\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |\n",
      " |\n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |\n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |\n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          version: The version of the schema to use either `v2` or `v1`.\n",
      " |                   Users should use `v2`.\n",
      " |                   `v1` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in `v2`.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |\n",
      " |      Yields:\n",
      " |          An async stream of StreamEvents.\n",
      " |\n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `v1` or `v2`.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a Runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          A RunLogPatch or RunLog object.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  batch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Useful when a Runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous Runnable or included in the user input.\n",
      " |\n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the arguments bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |\n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |\n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |\n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this Runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_config_jsonschema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the Runnable.\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this Runnable.\n",
      " |\n",
      " |  get_input_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the Runnable.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |              print(runnable.get_input_jsonschema())\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the Runnable.\n",
      " |\n",
      " |  get_output_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the Runnable.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |              print(runnable.get_output_jsonschema())\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the Runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this Runnable.\n",
      " |\n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |\n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, list[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this Runnable.\n",
      " |\n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |\n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |\n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Any\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |\n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |\n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |\n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |\n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |\n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind asynchronous lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Asynchronously called before the Runnable starts running.\n",
      " |      on_end: Asynchronously called after the Runnable finishes running.\n",
      " |      on_error: Asynchronously called if the Runnable throws an error.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Asynchronously called before the Runnable starts running.\n",
      " |              Defaults to None.\n",
      " |          on_end: Asynchronously called after the Runnable finishes running.\n",
      " |              Defaults to None.\n",
      " |          on_error: Asynchronously called if the Runnable throws an error.\n",
      " |              Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |\n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |\n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637053+00:00\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637150+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638305+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638383+00:00\n",
      " |          Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n",
      " |          Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n",
      " |          Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:35.640534+00:00\n",
      " |          Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:37.640574+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:37.640654+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:39.641751+00:00\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to bind to the Runnable.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the config bound.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      The new Runnable will try the original Runnable, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to (Exception,).\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Iterator\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |\n",
      " |\n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |\n",
      " |\n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |\n",
      " |\n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Called before the Runnable starts running, with the Run object.\n",
      " |      on_end: Called after the Runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the Runnable throws an error, with the Run object.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called before the Runnable starts running. Defaults to None.\n",
      " |          on_end: Called after the Runnable finishes running. Defaults to None.\n",
      " |          on_error: Called if the Runnable throws an error. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |\n",
      " |          import time\n",
      " |\n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |\n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |\n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |\n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          count = 0\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |\n",
      " |          assert (count == 2)\n",
      " |\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |  with_types(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the Runnable. Defaults to None.\n",
      " |          output_type: The output type to bind to the Runnable. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this Runnable.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this Runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      The type of output this Runnable produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "help(ChatOpenAI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "深度强化学习（Deep Reinforcement Learning, DRL）是一种结合了深度学习和强化学习的机器学习方法。强化学习是一种通过与环境交互来学习最优策略的框架，其中智能体（agent）通过试错法（trial-and-error）来最大化累积奖励。深度学习则利用深度神经网络来处理高维数据，提取复杂特征。\n",
      "\n",
      "在深度强化学习中，深度神经网络被用作函数逼近器，以估计价值函数或策略函数。这使得智能体能够在高维状态空间中进行有效的决策。例如，在图像处理任务中，深度神经网络能够从原始像素数据中提取有用的特征，从而使得智能体能够在复杂环境中进行学习。\n",
      "\n",
      "深度强化学习的一个重要应用是游戏领域，尤其是在围棋、星际争霸等复杂游戏中，智能体通过与环境的反复交互，逐渐学习到高效的策略。此外，DRL还被广泛应用于机器人控制、自动驾驶、金融交易等领域，展现出强大的学习能力和适应性。\n",
      "\n",
      "总之，深度强化学习通过结合深度学习的特征提取能力和强化学习的决策制定能力，为解决复杂的决策问题提供了一种强有力的工具。随着研究的深入和技术的进步，DRL有望在更多实际应用中发挥重要作用。\n"
     ]
    }
   ],
   "source": [
    "from src.query_transformations.HyDE import generate_hyde_document\n",
    "result7 = generate_hyde_document(query)\n",
    "print(result7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
